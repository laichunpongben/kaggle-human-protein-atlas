{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense\n",
    "\n",
    "classifier = Sequential()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20181126-train.csv\n",
      "train_flattened.csv\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Basic data processing\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "# Try visualize pixel values as an image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Split labelled dataset into training and testing data to test & improve our model\n",
    "from sklearn.model_selection import train_test_split \n",
    "# Change label formats between input/human-readable/output-required & better for model training formats\n",
    "from sklearn.preprocessing import LabelEncoder, LabelBinarizer\n",
    "\n",
    "# To build our CNN sequential model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D # CNN\n",
    "from keras import backend as K\n",
    "\n",
    "# List the data files in store\n",
    "from subprocess import check_output\n",
    "print(check_output([\"ls\", \"../../../input/\"]).decode(\"utf8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31072, 35)\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(\"../../../input/train_flattened.csv\")\n",
    "#test  = pd.read_csv(\"../../../input/test.csv\")\n",
    "print(train.shape)\n",
    "#print(test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Ideas\n",
    "- segment into single cells first"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Automated Analysis and Reannotation of Subcellular Locations in Confocal Images from the Human Protein Atlas\n",
    "https://www.researchgate.net/publication/233888909_Automated_Analysis_and_Reannotation_of_Subcellular_Locations_in_Confocal_Images_from_the_Human_Protein_Atlas\n",
    "    https://www.ncbi.nlm.nih.gov/pubmed/23226299"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.analyticsvidhya.com/blog/2018/11/implementation-faster-r-cnn-python-object-detection/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pascal poc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.analyticsvidhya.com/blog/2018/10/a-step-by-step-introduction-to-the-basic-object-detection-algorithms-part-1/\n",
    "    instance segmentation \n",
    "    - no need to \n",
    "    - differentiate between objects of the same type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "We first take a pre-trained convolutional neural network.\n",
    "Then, this model is retrained. We train the last layer of the network based on the number of classes that need to be detected.\n",
    "The third step is to get the Region of Interest for each image. We then reshape all these regions so that they can match the CNN input size.\n",
    "After getting the regions, we train SVM to classify objects and background. For each class, we train one binary SVM.\n",
    "Finally, we train a linear regression model to generate tighter bounding boxes for each identified object in the image.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Ideas\n",
    "\n",
    "- segment into single cells (randomize single cell images from different sources?)\n",
    "- separate color channels\n",
    "\n",
    "\n",
    "### Motivations & Advantages:\n",
    "- single cell: \n",
    "    - train by subcellular localization and texture, instead of cell shapes, densities, positions and orientations etc (Li et al., 2018)\n",
    "    - data augmentation\n",
    "- separate color channels: \n",
    "    - mark bounding boxes of cells with red channel \n",
    "      (usually the whole cell is filled with microtubules outside the nucleus)\n",
    "    - depending on the amount and density of target protein, color may affect\n",
    "    - can validate results by checking if predicted class fall into correct compartment, e.g. nucleus for nucleolus\n",
    "\n",
    "\n",
    "### Proposed procedures\n",
    "\n",
    "1. segment into single cells\n",
    "    - get bounding boxes, e.g. \n",
    "        - retrain from Pascal VOC dataset\n",
    "        - use imageJ\n",
    "        - parse with faster r-cnn `keras_frcnn` (`git clone https://github.com/yhenon/keras-frcnn/`)\n",
    "        - mask R-CNN\n",
    "        - watershed algorithm\n",
    "    \n",
    "2. Randomize and split single cell images into training batches\n",
    "\n",
    "3. Train\n",
    "    - test different models (check references)\n",
    "\n",
    "4. Check if falling into relevant compartments\n",
    "\n",
    "\n",
    "### References\n",
    "\n",
    "- Li, et al., 2018; Deep CNNs for HEp-2 Cells Classification : A Cross-specimen Analysis\n",
    "    - https://arxiv.org/abs/1604.05816 \n",
    "    - differentiate between Speckled, Nucleolar, Centromere, Nuclear Membrane, and Golgi\n",
    "    - dataset: I3A contest (13596 cell images)\n",
    "    - accuracy: 70% (Golgi) - 86.87% (Nuclear membrane)\n",
    "    - segment into single cells\n",
    "    - leave-one-specimen-out (LOSO)\n",
    "    - mean class accuracy (MCA) evaluation for each split\n",
    "    \n",
    "- Li, et al., 2017; A Deep Residual Inception Network for HEp-2 Cell Classification\n",
    "    - https://link.springer.com/chapter/10.1007/978-3-319-67558-9_2\n",
    "    - Auxiliary classifiers were only used as regularizers to improve network convergence during training in original Inception.\n",
    "    - Caffe toolbox\n",
    "    - http://nerone.diem.unisa.it/hep2-benchmarking/dbtools/\n",
    "\n",
    "- R-CNN, Fast R-CNN, Faster R-CNN, Mask R-CNN for object detection\n",
    "    - https://www.pyimagesearch.com/2018/11/19/mask-r-cnn-with-opencv/    \n",
    "    - https://www.analyticsvidhya.com/blog/2018/11/implementation-faster-r-cnn-python-object-detection/\n",
    "    - https://www.analyticsvidhya.com/blog/2018/10/a-step-by-step-introduction-to-the-basic-object-detection-algorithms-part-1/\n",
    "    \n",
    "- Automated Analysis and Reannotation of Subcellular Locations in Confocal Images from the Human Protein Atlas\n",
    "    - https://www.ncbi.nlm.nih.gov/pubmed/23226299\n",
    "    - https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1005746\n",
    "    \n",
    "- A deep convolutional neural network for classification of red blood cells in sickle cell anemia\n",
    "    - https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1005746\n",
    "    - segment into single cell images\n",
    "    - CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
