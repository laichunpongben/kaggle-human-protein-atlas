*** 2018-12-28 05:55:13,382 - code.resnet_fastai - DEBUG ***
Start a new training task
******

*** 2018-12-28 05:55:13,382 - code.resnet_fastai - INFO ***
Device ID: 0
Image size: 512
Network architecture: resnet
Loss function: bce
Sampler: random
Encoder depth: 50
Dropout: 0.5
Threshold: 0.1
Stage 1 #epoch: 15
Stage 2 #epoch: 25
Learning rate #1: 0
Batch size: 32
Dataset: official
Dataset directory: data/official
Output directory: output
******

*** 2018-12-28 05:55:13,382 - code.resnet_fastai - INFO ***
Offical stats: ([0.07986162506177984, 0.05217604947235713, 0.054227752481757215, 0.08201468927464939], [0.1403192215484648, 0.1041239635111223, 0.1532386688507187, 0.14099509309392533])
******

*** 2018-12-28 05:55:13,442 - code.resnet_fastai - DEBUG ***
# Test ids: 11702
******

*** 2018-12-28 05:55:14,239 - code.resnet_fastai - DEBUG ***
Start of fold 0
******

*** 2018-12-28 05:55:14,239 - code.resnet_fastai - DEBUG ***
Size of valid set: 6209
******

*** 2018-12-28 05:55:14,426 - code.resnet_fastai - DEBUG ***
LabelList
y: MultiCategoryList (24863 items)
[MultiCategory 16;0, MultiCategory 1, MultiCategory 18, MultiCategory 0, MultiCategory 25;2]...
Path: data/official
x: ImageItemList (24863 items)
[Image (4, 512, 512), Image (4, 512, 512), Image (4, 512, 512), Image (4, 512, 512), Image (4, 512, 512)]...
Path: data/official
******

*** 2018-12-28 05:55:14,732 - code.resnet_fastai - DEBUG ***
LabelList
y: MultiCategoryList (6209 items)
[MultiCategory 7;1;2;0, MultiCategory 5, MultiCategory 21, MultiCategory 0, MultiCategory 25;4]...
Path: data/official
x: ImageItemList (6209 items)
[Image (4, 512, 512), Image (4, 512, 512), Image (4, 512, 512), Image (4, 512, 512), Image (4, 512, 512)]...
Path: data/official
******

*** 2018-12-28 05:55:17,649 - code.resnet_fastai - DEBUG ***
Databunch created
******

*** 2018-12-28 05:55:17,649 - code.resnet_fastai - INFO ***
Initialising model.
******

*** 2018-12-28 05:55:22,114 - code.resnet_fastai - INFO ***
Complete initialising model.
******

*** 2018-12-28 05:55:22,114 - code.resnet_fastai - INFO ***
No pretrained model.
******

*** 2018-12-28 05:55:22,114 - code.resnet_fastai - DEBUG ***
Start finding LR
******

epoch     train_loss  valid_loss  fbeta   
1         0.412618                
*** 2018-12-28 06:03:18,732 - code.resnet_fastai - DEBUG ***
[(0.005, tensor(0.8226)), (0.005023078951391976, tensor(0.8199)), (0.0050462644303834224, tensor(0.8127)), (0.005069556928683398, tensor(0.8006)), (0.0050929569402705845, tensor(0.7944)), (0.00511646496140377, tensor(0.7811)), (0.0051400814906323675, tensor(0.7659)), (0.0051638070288069875, tensor(0.7524)), (0.005187642079090064, tensor(0.7388)), (0.005211587146966521, tensor(0.7287)), (0.005235642740254498, tensor(0.7129)), (0.005259809369116114, tensor(0.6977)), (0.005284087546068293, tensor(0.6809)), (0.005308477785993622, tensor(0.6634)), (0.00533298060615129, tensor(0.6437)), (0.0053575965261880324, tensor(0.6236)), (0.0053823260681491744, tensor(0.6047)), (0.00540716975648969, tensor(0.5858)), (0.005432128118085328, tensor(0.5675)), (0.005457201682243783, tensor(0.5495)), (0.005482390980715925, tensor(0.5320)), (0.005507696547707075, tensor(0.5146)), (0.005533118919888331, tensor(0.4983)), (0.005558658636407956, tensor(0.4833)), (0.005584316238902805, tensor(0.4694)), (0.005610092271509817, tensor(0.4563)), (0.005635987280877552, tensor(0.4440)), (0.005662001816177785, tensor(0.4322)), (0.005688136429117154, tensor(0.4204)), (0.005714391673948859, tensor(0.4092)), (0.005740768107484414, tensor(0.4001)), (0.005767266289105462, tensor(0.3916)), (0.00579388678077563, tensor(0.3839)), (0.005820630147052457, tensor(0.3762)), (0.005847496955099355, tensor(0.3688)), (0.0058744877746976475, tensor(0.3619)), (0.005901603178258649, tensor(0.3550)), (0.005928843740835801, tensor(0.3482)), (0.005956210040136875, tensor(0.3428)), (0.005983702656536217, tensor(0.3366)), (0.006011322173087065, tensor(0.3302)), (0.006039069175533901, tensor(0.3262)), (0.006066944252324886, tensor(0.3205)), (0.006094947994624332, tensor(0.3156)), (0.006123080996325243, tensor(0.3105)), (0.006151343854061908, tensor(0.3059)), (0.006179737167222552, tensor(0.3018)), (0.006208261537962056, tensor(0.2983)), (0.006236917571214715, tensor(0.2941)), (0.006265705874707079, tensor(0.2911)), (0.006294627058970837, tensor(0.2872)), (0.006323681737355757, tensor(0.2834)), (0.006352870526042708, tensor(0.2802)), (0.006382194044056719, tensor(0.2759)), (0.006411652913280108, tensor(0.2726)), (0.00644124775846567, tensor(0.2697)), (0.006470979207249931, tensor(0.2672)), (0.0065008478901664515, tensor(0.2638)), (0.006530854440659207, tensor(0.2609)), (0.006560999495096016, tensor(0.2580)), (0.006591283692782036, tensor(0.2555)), (0.0066217076759733234, tensor(0.2533)), (0.006652272089890455, tensor(0.2511)), (0.006682977582732211, tensor(0.2498)), (0.006713824805689319, tensor(0.2478)), (0.006744814412958267, tensor(0.2457)), (0.006775947061755181, tensor(0.2437)), (0.0068072234123297505, tensor(0.2411)), (0.006838644127979246, tensor(0.2391)), (0.006870209875062577, tensor(0.2372)), (0.006901921323014424, tensor(0.2353)), (0.006933779144359442, tensor(0.2335)), (0.006965784014726516, tensor(0.2322)), (0.006997936612863091, tensor(0.2305)), (0.00703023762064957, tensor(0.2289)), (0.007062687723113772, tensor(0.2273)), (0.007095287608445461, tensor(0.2256)), (0.007128037968010941, tensor(0.2242)), (0.007160939496367718, tensor(0.2223)), (0.007193992891279227, tensor(0.2211)), (0.007227198853729637, tensor(0.2200)), (0.007260558087938711, tensor(0.2187)), (0.007294071301376742, tensor(0.2176)), (0.007327739204779558, tensor(0.2163)), (0.007361562512163594, tensor(0.2157)), (0.007395541940841037, tensor(0.2149)), (0.007429678211435035, tensor(0.2143)), (0.00746397204789498, tensor(0.2127)), (0.007498424177511868, tensor(0.2119)), (0.00753303533093371, tensor(0.2110)), (0.0075678062421810416, tensor(0.2102)), (0.007602737648662478, tensor(0.2089)), (0.007637830291190363, tensor(0.2078)), (0.007673084913996472, tensor(0.2071)), (0.007708502264747797, tensor(0.2059)), (0.007744083094562407, tensor(0.2045)), (0.007779828158025373, tensor(0.2032)), (0.00781573821320477, tensor(0.2024)), (0.007851814021667762, tensor(0.2018)), (0.007888056348496745, tensor(0.2001)), (0.007924465962305569, tensor(0.1992)), (0.00796104363525585, tensor(0.1983)), (0.007997790143073344, tensor(0.1979)), (0.008034706265064386, tensor(0.1972)), (0.00807179278413243, tensor(0.1969)), (0.00810905048679465, tensor(0.1956)), (0.008146480163198612, tensor(0.1947)), (0.008184082607139043, tensor(0.1935)), (0.008221858616074659, tensor(0.1922)), (0.008259808991145074, tensor(0.1916)), (0.008297934537187803, tensor(0.1910)), (0.008336236062755315, tensor(0.1906)), (0.008374714380132186, tensor(0.1899)), (0.008413370305352337, tensor(0.1893)), (0.008452204658216321, tensor(0.1885)), (0.008491218262308722, tensor(0.1876)), (0.008530411945015618, tensor(0.1870)), (0.008569786537542127, tensor(0.1862)), (0.008609342874930035, tensor(0.1856)), (0.008649081796075507, tensor(0.1849)), (0.008689004143746877, tensor(0.1845)), (0.008729110764602518, tensor(0.1840)), (0.008769402509208806, tensor(0.1837)), (0.008809880232058146, tensor(0.1836)), (0.008850544791587105, tensor(0.1830)), (0.008891397050194615, tensor(0.1825)), (0.008932437874260254, tensor(0.1819)), (0.008973668134162633, tensor(0.1812)), (0.009015088704297845, tensor(0.1807)), (0.009056700463098014, tensor(0.1803)), (0.009098504293049918, tensor(0.1795)), (0.009140501080713714, tensor(0.1788)), (0.009182691716741732, tensor(0.1786)), (0.009225077095897368, tensor(0.1776)), (0.009267658117074057, tensor(0.1772)), (0.009310435683314338, tensor(0.1765)), (0.009353410701829004, tensor(0.1759)), (0.009396584084016344, tensor(0.1749)), (0.009439956745481468, tensor(0.1744)), (0.009483529606055733, tensor(0.1735)), (0.009527303589816237, tensor(0.1728)), (0.009571279625105428, tensor(0.1724)), (0.009615458644550792, tensor(0.1725)), (0.00965984158508462, tensor(0.1723)), (0.00970442938796389, tensor(0.1717)), (0.009749222998790226, tensor(0.1707)), (0.009794223367529949, tensor(0.1710)), (0.009839431448534225, tensor(0.1711)), (0.009884848200559303, tensor(0.1709)), (0.009930474586786857, tensor(0.1700)), (0.009976311574844398, tensor(0.1701)), (0.010022360136825805, tensor(0.1702)), (0.010068621249311942, tensor(0.1694)), (0.010115095893391357, tensor(0.1693)), (0.01016178505468111, tensor(0.1693)), (0.010208689723347646, tensor(0.1691)), (0.010255810894127828, tensor(0.1690)), (0.010303149566350001, tensor(0.1686)), (0.010350706743955211, tensor(0.1684)), (0.010398483435518477, tensor(0.1677)), (0.010446480654270198, tensor(0.1673)), (0.010494699418117622, tensor(0.1671)), (0.010543140749666446, tensor(0.1672)), (0.01059180567624251, tensor(0.1668)), (0.010640695229913559, tensor(0.1671)), (0.010689810447511161, tensor(0.1671)), (0.01073915237065267, tensor(0.1668)), (0.010788722045763333, tensor(0.1666)), (0.010838520524098474, tensor(0.1658)), (0.010888548861765794, tensor(0.1658)), (0.010938808119747763, tensor(0.1656)), (0.010989299363924126, tensor(0.1652)), (0.011040023665094498, tensor(0.1649)), (0.011090982099001095, tensor(0.1644)), (0.011142175746351518, tensor(0.1641)), (0.011193605692841698, tensor(0.1640)), (0.011245273029178904, tensor(0.1641)), (0.011297178851104888, tensor(0.1639)), (0.011349324259419109, tensor(0.1635)), (0.011401710360002091, tensor(0.1637)), (0.011454338263838865, tensor(0.1636)), (0.011507209087042542, tensor(0.1634)), (0.011560323950877973, tensor(0.1629)), (0.011613683981785534, tensor(0.1626)), (0.011667290311405015, tensor(0.1626)), (0.01172114407659961, tensor(0.1622)), (0.011775246419480048, tensor(0.1627)), (0.011829598487428792, tensor(0.1625)), (0.011884201433124382, tensor(0.1620)), (0.011939056414565887, tensor(0.1616)), (0.011994164595097452, tensor(0.1619)), (0.012049527143432975, tensor(0.1617)), (0.012105145233680892, tensor(0.1612)), (0.012161020045369078, tensor(0.1606)), (0.01221715276346986, tensor(0.1603)), (0.012273544578425152, tensor(0.1597)), (0.012330196686171697, tensor(0.1600)), (0.012387110288166428, tensor(0.1599)), (0.012444286591411955, tensor(0.1595)), (0.012501726808482157, tensor(0.1599)), (0.0125594321575479, tensor(0.1592)), (0.012617403862402874, tensor(0.1587)), (0.01267564315248954, tensor(0.1583)), (0.012734151262925207, tensor(0.1580)), (0.012792929434528229, tensor(0.1581)), (0.012851978913844318, tensor(0.1576)), (0.01291130095317298, tensor(0.1574)), (0.01297089681059407, tensor(0.1573)), (0.013030767749994477, tensor(0.1574)), (0.013090915041094926, tensor(0.1575)), (0.013151339959476909, tensor(0.1575)), (0.013212043786609734, tensor(0.1572)), (0.013273027809877696, tensor(0.1567)), (0.013334293322607398, tensor(0.1564)), (0.01339584162409516, tensor(0.1568)), (0.013457674019634578, tensor(0.1570)), (0.013519791820544219, tensor(0.1572)), (0.013582196344195413, tensor(0.1567)), (0.013644888914040205, tensor(0.1569)), (0.013707870859639412, tensor(0.1574)), (0.013771143516690832, tensor(0.1575)), (0.013834708227057558, tensor(0.1574)), (0.013898566338796443, tensor(0.1571)), (0.013962719206186691, tensor(0.1573)), (0.014027168189758568, tensor(0.1572)), (0.01409191465632227, tensor(0.1572)), (0.014156959978996898, tensor(0.1567)), (0.014222305537239579, tensor(0.1564)), (0.014287952716874735, tensor(0.1568)), (0.014353902910123456, tensor(0.1571)), (0.01442015751563303, tensor(0.1571)), (0.014486717938506618, tensor(0.1566)), (0.014553585590333026, tensor(0.1564)), (0.014620761889216679, tensor(0.1566)), (0.014688248259807656, tensor(0.1564)), (0.014756046133331929, tensor(0.1566)), (0.014824156947621712, tensor(0.1564)), (0.014892582147145948, tensor(0.1568)), (0.014961323183040944, tensor(0.1577)), (0.015030381513141153, tensor(0.1577)), (0.01509975860201008, tensor(0.1574)), (0.015169455920971353, tensor(0.1574)), (0.015239474948139915, tensor(0.1573)), (0.015309817168453387, tensor(0.1573)), (0.01538048407370354, tensor(0.1571)), (0.015451477162567952, tensor(0.1570)), (0.015522797940641778, tensor(0.1568)), (0.015594447920469686, tensor(0.1565)), (0.015666428621577928, tensor(0.1563)), (0.01573874157050658, tensor(0.1561)), (0.0158113883008419, tensor(0.1561)), (0.015884370353248852, tensor(0.1556)), (0.015957689275503808, tensor(0.1558)), (0.01603134662252733, tensor(0.1557)), (0.016105343956417172, tensor(0.1556)), (0.016179682846481414, tensor(0.1557)), (0.016254364869271717, tensor(0.1554)), (0.01632939160861679, tensor(0.1551)), (0.016404764655655952, tensor(0.1555)), (0.01648048560887289, tensor(0.1552)), (0.016556556074129557, tensor(0.1554)), (0.016632977664700226, tensor(0.1555)), (0.016709752001305714, tensor(0.1556)), (0.016786880712147734, tensor(0.1555)), (0.016864365432943444, tensor(0.1551)), (0.016942207806960128, tensor(0.1556)), (0.017020409485050046, tensor(0.1549)), (0.017098972125685445, tensor(0.1545)), (0.017177897394993733, tensor(0.1542)), (0.017257186966792817, tensor(0.1542)), (0.017336842522626583, tensor(0.1545)), (0.01741686575180059, tensor(0.1542)), (0.017497258351417865, tensor(0.1539)), (0.017578022026414907, tensor(0.1539)), (0.01765915848959785, tensor(0.1541)), (0.017740669461678776, tensor(0.1537)), (0.017822556671312214, tensor(0.1541)), (0.017904821855131803, tensor(0.1537)), (0.017987466757787115, tensor(0.1535)), (0.018070493131980666, tensor(0.1533)), (0.01815390273850507, tensor(0.1534)), (0.018237697346280394, tensor(0.1535)), (0.01832187873239166, tensor(0.1533)), (0.01840644868212657, tensor(0.1535)), (0.01849140898901331, tensor(0.1528)), (0.018576761454858627, tensor(0.1532)), (0.018662507889786028, tensor(0.1535)), (0.018748650112274175, tensor(0.1538)), (0.018835189949195443, tensor(0.1546)), (0.018922129235854665, tensor(0.1543)), (0.01900946981602806, tensor(0.1542)), (0.019097213542002327, tensor(0.1545)), (0.019185362274613937, tensor(0.1544)), (0.01927391788328859, tensor(0.1547)), (0.01936288224608086, tensor(0.1547)), (0.019452257249714028, tensor(0.1545)), (0.0195420447896201, tensor(0.1544)), (0.019632246769979992, tensor(0.1543)), (0.01972286510376392, tensor(0.1539)), (0.019813901712771972, tensor(0.1543)), (0.01990535852767486, tensor(0.1540)), (0.019997237488054872, tensor(0.1544)), (0.020089540542446997, tensor(0.1548)), (0.020182269648380248, tensor(0.1546)), (0.02027542677241919, tensor(0.1542)), (0.020369013890205637, tensor(0.1542)), (0.020463032986500544, tensor(0.1540)), (0.02055748605522611, tensor(0.1538)), (0.020652375099508066, tensor(0.1535)), (0.02074770213171815, tensor(0.1533)), (0.02084346917351677, tensor(0.1532)), (0.02093967825589592, tensor(0.1531)), (0.021036331419222204, tensor(0.1527)), (0.021133430713280152, tensor(0.1526)), (0.021230978197315644, tensor(0.1524)), (0.021328975940079636, tensor(0.1523)), (0.021427426019871975, tensor(0.1520)), (0.021526330524585535, tensor(0.1520)), (0.021625691551750436, tensor(0.1520)), (0.021725511208578577, tensor(0.1521)), (0.021825791612008302, tensor(0.1525)), (0.021926534888749287, tensor(0.1524)), (0.022027743175327665, tensor(0.1526)), (0.02212941861813133, tensor(0.1529)), (0.022231563373455436, tensor(0.1528)), (0.02233417960754816, tensor(0.1526)), (0.02243726949665661, tensor(0.1523)), (0.022540835227073012, tensor(0.1522)), (0.02264487899518104, tensor(0.1526)), (0.02274940300750243, tensor(0.1526)), (0.022854409480743753, tensor(0.1525)), (0.022959900641843434, tensor(0.1527)), (0.02306587872801897, tensor(0.1526)), (0.023172345986814405, tensor(0.1526)), (0.02327930467614795, tensor(0.1524)), (0.023386757064359914, tensor(0.1527)), (0.023494705430260773, tensor(0.1528)), (0.02360315206317953, tensor(0.1527)), (0.023712099263012237, tensor(0.1529)), (0.023821549340270788, tensor(0.1526)), (0.02393150461613192, tensor(0.1529)), (0.024041967422486432, tensor(0.1528)), (0.024152940101988638, tensor(0.1527)), (0.024264425008106057, tensor(0.1523)), (0.024376424505169313, tensor(0.1524)), (0.02448894096842231, tensor(0.1527)), (0.024601976784072544, tensor(0.1528)), (0.024715534349341768, tensor(0.1527)), (0.0248296160725168, tensor(0.1532)), (0.024944224373000607, tensor(0.1531)), (0.02505936168136361, tensor(0.1527)), (0.025175030439395236, tensor(0.1528)), (0.0252912331001557, tensor(0.1530)), (0.025407972128028027, tensor(0.1527)), (0.02552524999877031, tensor(0.1523)), (0.02564306919956824, tensor(0.1523)), (0.025761432229087822, tensor(0.1521)), (0.025880341597528382, tensor(0.1529)), (0.025999799826675798, tensor(0.1529)), (0.026119809449955986, tensor(0.1536)), (0.02624037301248863, tensor(0.1537)), (0.026361493071141138, tensor(0.1537)), (0.02648317219458289, tensor(0.1536)), (0.02660541296333971, tensor(0.1537)), (0.026728217969848582, tensor(0.1541)), (0.026851589818512635, tensor(0.1539)), (0.02697553112575638, tensor(0.1540)), (0.02710004452008119, tensor(0.1543)), (0.027225132642121057, tensor(0.1543)), (0.027350798144698582, tensor(0.1537)), (0.02747704369288123, tensor(0.1540)), (0.02760387196403787, tensor(0.1541)), (0.027731285647895537, tensor(0.1539)), (0.027859287446596492, tensor(0.1540)), (0.02798788007475551, tensor(0.1540)), (0.028117066259517456, tensor(0.1539)), (0.028246848740615126, tensor(0.1534)), (0.028377230270427357, tensor(0.1536)), (0.028508213614037374, tensor(0.1541)), (0.028639801549291462, tensor(0.1545)), (0.028771996866857847, tensor(0.1552)), (0.028904802370285906, tensor(0.1553)), (0.029038220876065605, tensor(0.1555)), (0.02917225521368724, tensor(0.1559)), (0.02930690822570144, tensor(0.1555)), (0.02944218276777945, tensor(0.1550)), (0.029578081708773702, tensor(0.1550)), (0.029714607930778635, tensor(0.1553)), (0.029851764329191847, tensor(0.1555)), (0.029989553812775474, tensor(0.1556)), (0.03012797930371789, tensor(0.1558)), (0.030267043737695683, tensor(0.1559)), (0.030406750063935898, tensor(0.1560)), (0.030547101245278607, tensor(0.1557)), (0.030688100258239714, tensor(0.1558)), (0.03082975009307411, tensor(0.1564)), (0.030972053753839074, tensor(0.1570)), (0.031115014258457977, tensor(0.1574)), (0.0312586346387843, tensor(0.1577)), (0.031402917940665895, tensor(0.1577)), (0.031547867224009665, tensor(0.1585)), (0.03169348556284635, tensor(0.1589)), (0.03183977604539579, tensor(0.1592)), (0.03198674177413241, tensor(0.1589)), (0.03213438586585099, tensor(0.1594)), (0.03228271145173278, tensor(0.1596)), (0.03243172167741192, tensor(0.1599)), (0.03258141970304213, tensor(0.1604)), (0.03273180870336374, tensor(0.1604)), (0.03288289186777102, tensor(0.1606)), (0.0330346724003798, tensor(0.1608)), (0.033187153520095436, tensor(0.1615)), (0.0333403384606811, tensor(0.1617)), (0.03349423047082632, tensor(0.1617)), (0.033648832814215886, tensor(0.1615)), (0.033804148769599085, tensor(0.1615)), (0.033960181630859225, tensor(0.1613)), (0.034116934707083484, tensor(0.1621)), (0.03427441132263308, tensor(0.1623)), (0.034432614817213804, tensor(0.1624)), (0.03459154854594682, tensor(0.1623)), (0.03475121587943984, tensor(0.1624)), (0.034911620203858566, tensor(0.1626)), (0.03507276492099856, tensor(0.1626)), (0.03523465344835735, tensor(0.1625)), (0.035397289219206894, tensor(0.1627)), (0.03556067568266645, tensor(0.1629)), (0.035724816303775664, tensor(0.1632)), (0.03588971456356809, tensor(0.1632)), (0.03605537395914498, tensor(0.1631)), (0.03622179800374951, tensor(0.1630)), (0.0363889902268412, tensor(0.1633)), (0.036556954174170875, tensor(0.1640)), (0.036725693407855746, tensor(0.1643)), (0.03689521150645505, tensor(0.1643)), (0.03706551206504587, tensor(0.1647)), (0.03723659869529945, tensor(0.1653)), (0.03740847502555772, tensor(0.1657)), (0.03758114470091028, tensor(0.1665)), (0.0377546113832717, tensor(0.1666)), (0.037928878751459186, tensor(0.1669)), (0.0381039505012706, tensor(0.1664)), (0.038279830345562815, tensor(0.1669)), (0.03845652201433048, tensor(0.1672)), (0.03863402925478512, tensor(0.1677)), (0.03881235583143459, tensor(0.1675)), (0.038991505526162935, tensor(0.1682)), (0.03917148213831059, tensor(0.1682)), (0.039352289484754936, tensor(0.1686)), (0.03953393139999126, tensor(0.1687)), (0.03971641173621408, tensor(0.1686)), (0.03989973436339883, tensor(0.1686)), (0.04008390316938396, tensor(0.1684)), (0.04026892205995333, tensor(0.1686)), (0.04045479495891912, tensor(0.1688)), (0.040641525808204966, tensor(0.1688)), (0.040829118567929625, tensor(0.1694)), (0.041017577216490915, tensor(0.1697)), (0.041206905750650114, tensor(0.1695)), (0.04139710818561671, tensor(0.1699)), (0.041588188555133555, tensor(0.1698)), (0.041780150911562404, tensor(0.1704)), (0.04197299932596987, tensor(0.1704)), (0.042166737888213776, tensor(0.1710)), (0.042361370707029826, tensor(0.1707)), (0.042556901910118836, tensor(0.1707)), (0.04275333564423417, tensor(0.1712)), (0.0429506760752698, tensor(0.1714)), (0.043148927388348524, tensor(0.1713)), (0.04334809378791083, tensor(0.1711)), (0.043548179497804027, tensor(0.1709)), (0.0437491887613718, tensor(0.1703)), (0.04395112584154421, tensor(0.1702)), (0.044153995020928136, tensor(0.1704)), (0.044357800601898034, tensor(0.1706)), (0.04456254690668727, tensor(0.1712)), (0.04476823827747969, tensor(0.1710)), (0.04497487907650176, tensor(0.1712)), (0.045182473686115075, tensor(0.1711)), (0.045391026508909284, tensor(0.1714)), (0.045600541967795484, tensor(0.1722)), (0.04581102450609999, tensor(0.1725)), (0.04602247858765856, tensor(0.1725)), (0.046234908696911124, tensor(0.1732)), (0.04644831933899681, tensor(0.1733)), (0.046662715039849555, tensor(0.1733)), (0.04687810034629401, tensor(0.1739)), (0.047094479826142066, tensor(0.1745)), (0.04731185806828964, tensor(0.1750)), (0.04753023968281407, tensor(0.1755)), (0.047749629301071794, tensor(0.1754)), (0.04797003157579666, tensor(0.1763)), (0.048191451181198525, tensor(0.1764)), (0.04841389281306246, tensor(0.1769)), (0.04863736118884826, tensor(0.1775)), (0.048861861047790535, tensor(0.1776)), (0.049087397150999225, tensor(0.1779)), (0.04931397428156053, tensor(0.1775)), (0.04954159724463838, tensor(0.1784)), (0.04977027086757635, tensor(0.1787)), (0.05, tensor(0.1796)), (0.05023078951391976, tensor(0.1799)), (0.05046264430383422, tensor(0.1799)), (0.050695569286833976, tensor(0.1803)), (0.05092956940270585, tensor(0.1806)), (0.05116464961403771, tensor(0.1808)), (0.05140081490632368, tensor(0.1806)), (0.051638070288069875, tensor(0.1807)), (0.05187642079090063, tensor(0.1808)), (0.052115871469665204, tensor(0.1815)), (0.05235642740254498, tensor(0.1815)), (0.05259809369116114, tensor(0.1814)), (0.052840875460682925, tensor(0.1818)), (0.05308477785993624, tensor(0.1824)), (0.05332980606151289, tensor(0.1826)), (0.053575965261880326, tensor(0.1830)), (0.05382326068149175, tensor(0.1832)), (0.0540716975648969, tensor(0.1831)), (0.054321281180853276, tensor(0.1832)), (0.05457201682243783, tensor(0.1835)), (0.05482390980715926, tensor(0.1841)), (0.055076965477070756, tensor(0.1839)), (0.055331189198883315, tensor(0.1849)), (0.05558658636407957, tensor(0.1851)), (0.05584316238902806, tensor(0.1858)), (0.05610092271509818, tensor(0.1865)), (0.056359872808775524, tensor(0.1880)), (0.05662001816177786, tensor(0.1886)), (0.056881364291171556, tensor(0.1888)), (0.0571439167394886, tensor(0.1889)), (0.05740768107484415, tensor(0.1899)), (0.05767266289105462, tensor(0.1901)), (0.05793886780775631, tensor(0.1906)), (0.05820630147052457, tensor(0.1911)), (0.05847496955099356, tensor(0.1910)), (0.05874487774697649, tensor(0.1918)), (0.05901603178258649, tensor(0.1916)), (0.05928843740835801, tensor(0.1916)), (0.05956210040136875, tensor(0.1916)), (0.05983702656536218, tensor(0.1920)), (0.06011322173087066, tensor(0.1927)), (0.06039069175533902, tensor(0.1929)), (0.06066944252324888, tensor(0.1923)), (0.060949479946243335, tensor(0.1925)), (0.06123080996325245, tensor(0.1928)), (0.06151343854061909, tensor(0.1933)), (0.061797371672225536, tensor(0.1937)), (0.062082615379620565, tensor(0.1937)), (0.06236917571214717, tensor(0.1936)), (0.06265705874707081, tensor(0.1945)), (0.06294627058970838, tensor(0.1941)), (0.06323681737355759, tensor(0.1951)), (0.06352870526042709, tensor(0.1950)), (0.0638219404405672, tensor(0.1952)), (0.0641165291328011, tensor(0.1953)), (0.06441247758465671, tensor(0.1952)), (0.06470979207249931, tensor(0.1952)), (0.06500847890166453, tensor(0.1957)), (0.06530854440659209, tensor(0.1967)), (0.06560999495096018, tensor(0.1972)), (0.06591283692782036, tensor(0.1976)), (0.06621707675973325, tensor(0.1975)), (0.06652272089890457, tensor(0.1978)), (0.0668297758273221, tensor(0.1978)), (0.06713824805689318, tensor(0.1984)), (0.06744814412958267, tensor(0.1980)), (0.06775947061755179, tensor(0.1977)), (0.06807223412329749, tensor(0.1979)), (0.06838644127979243, tensor(0.1982)), (0.06870209875062575, tensor(0.1982)), (0.06901921323014423, tensor(0.1991)), (0.06933779144359441, tensor(0.1999)), (0.06965784014726514, tensor(0.2005)), (0.0699793661286309, tensor(0.2003)), (0.07030237620649568, tensor(0.2003)), (0.0706268772311377, tensor(0.2002)), (0.0709528760844546, tensor(0.2002)), (0.0712803796801094, tensor(0.2007)), (0.07160939496367716, tensor(0.2010)), (0.07193992891279226, tensor(0.2012)), (0.07227198853729636, tensor(0.2012)), (0.07260558087938711, tensor(0.2012)), (0.07294071301376741, tensor(0.2009)), (0.07327739204779557, tensor(0.2007)), (0.07361562512163594, tensor(0.2004)), (0.07395541940841036, tensor(0.2004)), (0.07429678211435034, tensor(0.2004)), (0.0746397204789498, tensor(0.2012)), (0.07498424177511867, tensor(0.2012)), (0.07533035330933709, tensor(0.2013)), (0.0756780624218104, tensor(0.2015)), (0.07602737648662478, tensor(0.2016)), (0.07637830291190362, tensor(0.2019)), (0.07673084913996471, tensor(0.2020)), (0.07708502264747796, tensor(0.2021)), (0.07744083094562405, tensor(0.2026)), (0.0777982815802537, tensor(0.2024)), (0.07815738213204769, tensor(0.2032)), (0.07851814021667762, tensor(0.2027)), (0.07888056348496743, tensor(0.2035)), (0.07924465962305567, tensor(0.2042)), (0.0796104363525585, tensor(0.2044)), (0.07997790143073344, tensor(0.2049)), (0.08034706265064387, tensor(0.2050)), (0.08071792784132431, tensor(0.2051)), (0.08109050486794649, tensor(0.2065)), (0.0814648016319861, tensor(0.2072)), (0.08184082607139043, tensor(0.2065)), (0.08221858616074658, tensor(0.2075)), (0.08259808991145075, tensor(0.2075)), (0.08297934537187804, tensor(0.2076)), (0.08336236062755313, tensor(0.2075)), (0.08374714380132187, tensor(0.2080)), (0.08413370305352336, tensor(0.2088)), (0.08452204658216321, tensor(0.2091)), (0.08491218262308722, tensor(0.2096)), (0.08530411945015617, tensor(0.2098)), (0.08569786537542128, tensor(0.2104)), (0.08609342874930034, tensor(0.2107)), (0.08649081796075507, tensor(0.2111)), (0.08689004143746877, tensor(0.2138)), (0.08729110764602518, tensor(0.2140)), (0.08769402509208807, tensor(0.2142)), (0.08809880232058147, tensor(0.2154)), (0.08850544791587106, tensor(0.2166)), (0.08891397050194615, tensor(0.2168)), (0.08932437874260254, tensor(0.2174)), (0.08973668134162634, tensor(0.2173)), (0.09015088704297845, tensor(0.2173)), (0.09056700463098012, tensor(0.2185)), (0.09098504293049917, tensor(0.2183)), (0.09140501080713713, tensor(0.2187)), (0.09182691716741732, tensor(0.2183)), (0.09225077095897369, tensor(0.2187)), (0.09267658117074058, tensor(0.2187)), (0.09310435683314339, tensor(0.2194)), (0.09353410701829004, tensor(0.2201)), (0.09396584084016343, tensor(0.2203)), (0.09439956745481468, tensor(0.2213)), (0.09483529606055732, tensor(0.2209)), (0.09527303589816237, tensor(0.2210)), (0.09571279625105429, tensor(0.2212)), (0.09615458644550794, tensor(0.2220)), (0.0965984158508462, tensor(0.2225)), (0.09704429387963892, tensor(0.2227)), (0.09749222998790227, tensor(0.2217)), (0.0979422336752995, tensor(0.2215)), (0.09839431448534225, tensor(0.2219)), (0.09884848200559304, tensor(0.2221)), (0.09930474586786858, tensor(0.2224)), (0.09976311574844399, tensor(0.2231)), (0.10022360136825807, tensor(0.2245)), (0.10068621249311942, tensor(0.2250)), (0.1011509589339136, tensor(0.2257)), (0.10161785054681109, tensor(0.2270)), (0.10208689723347648, tensor(0.2272)), (0.10255810894127829, tensor(0.2269)), (0.10303149566350003, tensor(0.2271)), (0.10350706743955213, tensor(0.2274)), (0.10398483435518481, tensor(0.2282)), (0.10446480654270199, tensor(0.2288)), (0.10494699418117623, tensor(0.2292)), (0.10543140749666449, tensor(0.2299)), (0.10591805676242512, tensor(0.2309)), (0.10640695229913562, tensor(0.2321)), (0.10689810447511162, tensor(0.2341)), (0.10739152370652671, tensor(0.2349)), (0.10788722045763334, tensor(0.2353)), (0.10838520524098476, tensor(0.2362)), (0.10888548861765796, tensor(0.2373)), (0.10938808119747766, tensor(0.2374)), (0.10989299363924128, tensor(0.2365)), (0.11040023665094502, tensor(0.2377)), (0.11090982099001098, tensor(0.2386)), (0.11142175746351521, tensor(0.2390)), (0.111936056928417, tensor(0.2394)), (0.11245273029178908, tensor(0.2392)), (0.1129717885110489, tensor(0.2403)), (0.11349324259419112, tensor(0.2400)), (0.11401710360002093, tensor(0.2409)), (0.11454338263838867, tensor(0.2413)), (0.11507209087042546, tensor(0.2419)), (0.11560323950877976, tensor(0.2418)), (0.11613683981785537, tensor(0.2425)), (0.11667290311405017, tensor(0.2452)), (0.11721144076599613, tensor(0.2457)), (0.11775246419480052, tensor(0.2465)), (0.11829598487428795, tensor(0.2468)), (0.1188420143312438, tensor(0.2476)), (0.11939056414565885, tensor(0.2490)), (0.11994164595097451, tensor(0.2511)), (0.12049527143432971, tensor(0.2515)), (0.12105145233680889, tensor(0.2526)), (0.12161020045369073, tensor(0.2529)), (0.12217152763469857, tensor(0.2546)), (0.1227354457842515, tensor(0.2563)), (0.12330196686171693, tensor(0.2583)), (0.12387110288166424, tensor(0.2587)), (0.12444286591411953, tensor(0.2601)), (0.12501726808482155, tensor(0.2622)), (0.12559432157547898, tensor(0.2630)), (0.1261740386240287, tensor(0.2656)), (0.12675643152489535, tensor(0.2665)), (0.12734151262925203, tensor(0.2680)), (0.12792929434528227, tensor(0.2686)), (0.12851978913844317, tensor(0.2682)), (0.1291130095317298, tensor(0.2697)), (0.12970896810594068, tensor(0.2705)), (0.13030767749994474, tensor(0.2715)), (0.13090915041094925, tensor(0.2743)), (0.13151339959476907, tensor(0.2750)), (0.1321204378660973, tensor(0.2754)), (0.13273027809877694, tensor(0.2758)), (0.133342933226074, tensor(0.2787)), (0.13395841624095156, tensor(0.2797)), (0.13457674019634577, tensor(0.2830)), (0.13519791820544216, tensor(0.2852)), (0.13582196344195413, tensor(0.2859)), (0.136448889140402, tensor(0.2865)), (0.1370787085963941, tensor(0.2886)), (0.1377114351669083, tensor(0.2891)), (0.13834708227057557, tensor(0.2912)), (0.13898566338796442, tensor(0.2938)), (0.13962719206186688, tensor(0.2947)), (0.14027168189758565, tensor(0.2961)), (0.14091914656322266, tensor(0.2995)), (0.14156959978996894, tensor(0.3008)), (0.14222305537239577, tensor(0.3024)), (0.14287952716874733, tensor(0.3027)), (0.14353902910123453, tensor(0.3034)), (0.1442015751563303, tensor(0.3049)), (0.14486717938506613, tensor(0.3050)), (0.14553585590333026, tensor(0.3080)), (0.14620761889216677, tensor(0.3092)), (0.14688248259807654, tensor(0.3133)), (0.1475604613333193, tensor(0.3170)), (0.1482415694762171, tensor(0.3190)), (0.14892582147145947, tensor(0.3204)), (0.14961323183040945, tensor(0.3221)), (0.15030381513141153, tensor(0.3228)), (0.1509975860201008, tensor(0.3245)), (0.15169455920971353, tensor(0.3290)), (0.15239474948139914, tensor(0.3324)), (0.15309817168453385, tensor(0.3357)), (0.1538048407370354, tensor(0.3364)), (0.15451477162567953, tensor(0.3420)), (0.15522797940641778, tensor(0.3443)), (0.15594447920469687, tensor(0.3473)), (0.1566642862157793, tensor(0.3511)), (0.1573874157050658, tensor(0.3548)), (0.15811388300841897, tensor(0.3586)), (0.15884370353248853, tensor(0.3618)), (0.15957689275503809, tensor(0.3654)), (0.16031346622527332, tensor(0.3662)), (0.16105343956417173, tensor(0.3686)), (0.16179682846481414, tensor(0.3696)), (0.16254364869271717, tensor(0.3696)), (0.16329391608616792, tensor(0.3744)), (0.16404764655655954, tensor(0.3779)), (0.1648048560887289, tensor(0.3802)), (0.16556556074129555, tensor(0.3800)), (0.16632977664700227, tensor(0.3827)), (0.16709752001305714, tensor(0.3853)), (0.16786880712147734, tensor(0.3880)), (0.16864365432943446, tensor(0.3898)), (0.16942207806960127, tensor(0.3910)), (0.17020409485050045, tensor(0.3950)), (0.17098972125685444, tensor(0.3975)), (0.17177897394993732, tensor(0.3974)), (0.17257186966792812, tensor(0.4008)), (0.17336842522626583, tensor(0.4022)), (0.1741686575180059, tensor(0.4021)), (0.17497258351417866, tensor(0.4040)), (0.17578022026414908, tensor(0.4079)), (0.1765915848959785, tensor(0.4085)), (0.17740669461678774, tensor(0.4126)), (0.17822556671312215, tensor(0.4180)), (0.17904821855131803, tensor(0.4244)), (0.17987466757787118, tensor(0.4270)), (0.18070493131980667, tensor(0.4289)), (0.1815390273850507, tensor(0.4314)), (0.18237697346280393, tensor(0.4331)), (0.18321878732391667, tensor(0.4362)), (0.18406448682126572, tensor(0.4364)), (0.1849140898901331, tensor(0.4371)), (0.1857676145485863, tensor(0.4376)), (0.1866250788978603, tensor(0.4381)), (0.1874865011227418, tensor(0.4422)), (0.18835189949195447, tensor(0.4445)), (0.1892212923585467, tensor(0.4482)), (0.19009469816028063, tensor(0.4488)), (0.19097213542002334, tensor(0.4537)), (0.1918536227461394, tensor(0.4540)), (0.19273917883288594, tensor(0.4544)), (0.19362882246080865, tensor(0.4562)), (0.19452257249714033, tensor(0.4615)), (0.19542044789620108, tensor(0.4655)), (0.19632246769979997, tensor(0.4675)), (0.19722865103763926, tensor(0.4719)), (0.19813901712771975, tensor(0.4713)), (0.19905358527674868, tensor(0.4751)), (0.19997237488054878, tensor(0.4764)), (0.20089540542447, tensor(0.4771)), (0.20182269648380252, tensor(0.4816)), (0.20275426772419194, tensor(0.4851)), (0.2036901389020564, tensor(0.4861)), (0.20463032986500548, tensor(0.4907)), (0.20557486055226118, tensor(0.4909)), (0.20652375099508075, tensor(0.4978)), (0.20747702131718154, tensor(0.5001)), (0.20843469173516777, tensor(0.5082)), (0.20939678255895924, tensor(0.5126)), (0.2103633141922221, tensor(0.5171)), (0.2113343071328014, tensor(0.5176)), (0.2123097819731564, tensor(0.5194)), (0.21328975940079628, tensor(0.5221)), (0.2142742601987197, tensor(0.5247)), (0.2152633052458553, tensor(0.5285)), (0.2162569155175043, tensor(0.5319)), (0.2172551120857857, tensor(0.5383)), (0.2182579161200829, tensor(0.5424)), (0.2192653488874928, tensor(0.5483)), (0.2202774317532766, tensor(0.5508)), (0.22129418618131322, tensor(0.5536)), (0.2223156337345543, tensor(0.5588)), (0.22334179607548152, tensor(0.5674)), (0.22437269496656606, tensor(0.5742)), (0.22540835227073003, tensor(0.5804)), (0.22644878995181036, tensor(0.5889)), (0.22749403007502422, tensor(0.5901)), (0.22854409480743748, tensor(0.5958)), (0.22959900641843425, tensor(0.5964)), (0.23065878728018965, tensor(0.6012)), (0.23172345986814397, tensor(0.6051)), (0.23279304676147944, tensor(0.6108))]
******

*** 2018-12-28 06:03:19,107 - code.resnet_fastai - DEBUG ***
0.021625691551750436
******

*** 2018-12-28 06:03:19,146 - matplotlib.axes._base - DEBUG ***
update_title_pos
******

*** 2018-12-28 06:03:19,150 - matplotlib.ticker - DEBUG ***
vmin 0.0043358076204591525 vmax 0.27470717598107075
******

*** 2018-12-28 06:03:19,150 - matplotlib.ticker - DEBUG ***
ticklocs array([1.e-04, 1.e-03, 1.e-02, 1.e-01, 1.e+00, 1.e+01])
******

*** 2018-12-28 06:03:19,160 - matplotlib.ticker - DEBUG ***
vmin 0.0043358076204591525 vmax 0.27470717598107075
******

*** 2018-12-28 06:03:19,161 - matplotlib.ticker - DEBUG ***
ticklocs [0.0002, 0.00030000000000000003, 0.0004, 0.0005, 0.0006000000000000001, 0.0007, 0.0008, 0.0009000000000000001, 0.002, 0.003, 0.004, 0.005, 0.006, 0.007, 0.008, 0.009000000000000001, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.2, 0.30000000000000004, 0.4, 0.5, 0.6000000000000001, 0.7000000000000001, 0.8, 0.9, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 20.0, 30.0, 40.0, 50.0, 60.0, 70.0, 80.0, 90.0]
******

*** 2018-12-28 06:03:19,248 - matplotlib.font_manager - DEBUG ***
findfont: Matching :family=sans-serif:style=normal:variant=normal:weight=normal:stretch=normal:size=10.0 to DejaVu Sans ('/home/ubuntu/.local/lib/python3.6/site-packages/matplotlib/mpl-data/fonts/ttf/DejaVuSans.ttf') with score of 0.050000.
******

*** 2018-12-28 06:03:19,311 - matplotlib.ticker - DEBUG ***
vmin 0.0043358076204591525 vmax 0.27470717598107075
******

*** 2018-12-28 06:03:19,311 - matplotlib.ticker - DEBUG ***
ticklocs array([1.e-04, 1.e-03, 1.e-02, 1.e-01, 1.e+00, 1.e+01])
******

*** 2018-12-28 06:03:19,312 - matplotlib.ticker - DEBUG ***
vmin 0.0043358076204591525 vmax 0.27470717598107075
******

*** 2018-12-28 06:03:19,313 - matplotlib.ticker - DEBUG ***
ticklocs [0.0002, 0.00030000000000000003, 0.0004, 0.0005, 0.0006000000000000001, 0.0007, 0.0008, 0.0009000000000000001, 0.002, 0.003, 0.004, 0.005, 0.006, 0.007, 0.008, 0.009000000000000001, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.2, 0.30000000000000004, 0.4, 0.5, 0.6000000000000001, 0.7000000000000001, 0.8, 0.9, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 20.0, 30.0, 40.0, 50.0, 60.0, 70.0, 80.0, 90.0]
******

*** 2018-12-28 06:03:19,433 - matplotlib.axes._base - DEBUG ***
update_title_pos
******

*** 2018-12-28 06:03:19,436 - matplotlib.ticker - DEBUG ***
vmin 0.0043358076204591525 vmax 0.27470717598107075
******

*** 2018-12-28 06:03:19,436 - matplotlib.ticker - DEBUG ***
ticklocs array([1.e-04, 1.e-03, 1.e-02, 1.e-01, 1.e+00, 1.e+01])
******

*** 2018-12-28 06:03:19,438 - matplotlib.ticker - DEBUG ***
vmin 0.0043358076204591525 vmax 0.27470717598107075
******

*** 2018-12-28 06:03:19,438 - matplotlib.ticker - DEBUG ***
ticklocs [0.0002, 0.00030000000000000003, 0.0004, 0.0005, 0.0006000000000000001, 0.0007, 0.0008, 0.0009000000000000001, 0.002, 0.003, 0.004, 0.005, 0.006, 0.007, 0.008, 0.009000000000000001, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.2, 0.30000000000000004, 0.4, 0.5, 0.6000000000000001, 0.7000000000000001, 0.8, 0.9, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 20.0, 30.0, 40.0, 50.0, 60.0, 70.0, 80.0, 90.0]
******

*** 2018-12-28 06:03:19,477 - matplotlib.ticker - DEBUG ***
vmin 0.0043358076204591525 vmax 0.27470717598107075
******

*** 2018-12-28 06:03:19,477 - matplotlib.ticker - DEBUG ***
ticklocs array([1.e-04, 1.e-03, 1.e-02, 1.e-01, 1.e+00, 1.e+01])
******

*** 2018-12-28 06:03:19,479 - matplotlib.ticker - DEBUG ***
vmin 0.0043358076204591525 vmax 0.27470717598107075
******

*** 2018-12-28 06:03:19,488 - matplotlib.ticker - DEBUG ***
ticklocs [0.0002, 0.00030000000000000003, 0.0004, 0.0005, 0.0006000000000000001, 0.0007, 0.0008, 0.0009000000000000001, 0.002, 0.003, 0.004, 0.005, 0.006, 0.007, 0.008, 0.009000000000000001, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.2, 0.30000000000000004, 0.4, 0.5, 0.6000000000000001, 0.7000000000000001, 0.8, 0.9, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 20.0, 30.0, 40.0, 50.0, 60.0, 70.0, 80.0, 90.0]
******

*** 2018-12-28 06:03:19,537 - code.resnet_fastai - INFO ***
Start model fitting: Stage 1
******

*** 2018-12-28 06:03:19,537 - code.resnet_fastai - DEBUG ***
Use best LR: 0.021625691551750436
******

2         0.610776                
LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.
epoch     train_loss  valid_loss  fbeta   
1         0.145743    0.157631    0.408888  
2         0.130957    0.130537    0.518171  
3         0.136015    0.129543    0.530745  
4         0.139605    0.607895    0.488719  
5         0.137799    1.414590    0.353296  
Epoch 5: reducing lr to 0.004300984296272203
6         0.135134    0.317594    0.293639  
Traceback (most recent call last):
  File "/usr/lib/python3.6/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/usr/lib/python3.6/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/ubuntu/atlas/code/resnet_fastai.py", line 475, in <module>
    learn = fit_model(learn, stage=1, fold=index)
  File "/home/ubuntu/atlas/code/resnet_fastai.py", line 413, in fit_model
    learn.fit_one_cycle(cyc_len, max_lr)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/fastai/train.py", line 21, in fit_one_cycle
    learn.fit(cyc_len, max_lr, wd=wd, callbacks=callbacks)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/fastai/basic_train.py", line 166, in fit
    callbacks=self.callbacks+callbacks)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/fastai/basic_train.py", line 94, in fit
    raise e
  File "/home/ubuntu/.local/lib/python3.6/site-packages/fastai/basic_train.py", line 82, in fit
    for xb,yb in progress_bar(data.train_dl, parent=pbar):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/fastprogress/fastprogress.py", line 65, in __iter__
    for i,o in enumerate(self._gen):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/fastai/basic_data.py", line 70, in __iter__
    for b in self.dl:
  File "/home/ubuntu/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 330, in __next__
    idx, batch = self._get_batch()
  File "/home/ubuntu/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 309, in _get_batch
    return self.data_queue.get()
  File "/usr/lib/python3.6/multiprocessing/queues.py", line 335, in get
    res = self._reader.recv_bytes()
  File "/usr/lib/python3.6/multiprocessing/connection.py", line 216, in recv_bytes
    buf = self._recv_bytes(maxlength)
  File "/usr/lib/python3.6/multiprocessing/connection.py", line 407, in _recv_bytes
    buf = self._recv(4)
  File "/usr/lib/python3.6/multiprocessing/connection.py", line 379, in _recv
    chunk = read(handle, remaining)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 227, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 5049) is killed by signal: Terminated. Details are lost due to multiprocessing. Rerunning with num_workers=0 may give better error trace.
