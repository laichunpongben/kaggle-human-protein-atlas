*** 2018-12-29 13:26:36,582 - code.resnet_fastai - DEBUG ***
Start a new training task
******

*** 2018-12-29 13:26:36,582 - code.resnet_fastai - INFO ***
Device ID: 0
Image size: 512
Network architecture: resnet
Loss function: bce
Sampler: random
Encoder depth: 50
Dropout: 0.5
Threshold: 0.2
Stage 1 #epoch: 3
Stage 2 #epoch: 25
Learning rate #1: 0
Batch size: 32
Dataset: official_hpav18
Dataset directory: data/hpav18
Output directory: output
******

*** 2018-12-29 13:26:36,582 - code.resnet_fastai - INFO ***
official_hpav18 stats: ([0.1066346176636245, 0.06331175038503216, 0.06268440253388274, 0.08384531792200918], [0.167137549903456, 0.11800686037410352, 0.169741759561414, 0.12320841516204066])
******

*** 2018-12-29 13:26:36,643 - code.resnet_fastai - DEBUG ***
# Test ids: 11702
******

*** 2018-12-29 13:26:39,035 - code.resnet_fastai - DEBUG ***
Start of fold 0
******

*** 2018-12-29 13:26:39,035 - code.resnet_fastai - DEBUG ***
Size of valid set: 20528
******

*** 2018-12-29 13:26:39,760 - code.resnet_fastai - DEBUG ***
LabelList
y: MultiCategoryList (81981 items)
[MultiCategory 16;0, MultiCategory 7;1;2;0, MultiCategory 18, MultiCategory 0, MultiCategory 25;2]...
Path: data/hpav18
x: ImageItemList (81981 items)
[Image (4, 512, 512), Image (4, 512, 512), Image (4, 512, 512), Image (4, 512, 512), Image (4, 512, 512)]...
Path: data/hpav18
******

*** 2018-12-29 13:26:39,893 - code.resnet_fastai - DEBUG ***
LabelList
y: MultiCategoryList (20528 items)
[MultiCategory 5, MultiCategory 1, MultiCategory 7, MultiCategory 23, MultiCategory 25]...
Path: data/hpav18
x: ImageItemList (20528 items)
[Image (4, 512, 512), Image (4, 512, 512), Image (4, 512, 512), Image (4, 512, 512), Image (4, 512, 512)]...
Path: data/hpav18
******

*** 2018-12-29 13:26:42,075 - code.resnet_fastai - DEBUG ***
Databunch created
******

*** 2018-12-29 13:26:42,076 - code.resnet_fastai - INFO ***
Initialising model.
******

*** 2018-12-29 13:26:46,506 - code.resnet_fastai - INFO ***
Complete initialising model.
******

*** 2018-12-29 13:26:46,506 - code.resnet_fastai - INFO ***
No pretrained model.
******

*** 2018-12-29 13:26:46,506 - code.resnet_fastai - DEBUG ***
Start finding LR
******

epoch     train_loss  valid_loss  fbeta   
*** 2018-12-29 13:35:27,356 - code.resnet_fastai - DEBUG ***
[(0.01, tensor(0.8479)), (0.010013872557113347, tensor(0.8459)), (0.010027764359010777, tensor(0.8286)), (0.010041675432389732, tensor(0.8040)), (0.010055605803984681, tensor(0.7752)), (0.01006955550056719, tensor(0.7466)), (0.010083524548945951, tensor(0.7155)), (0.010097512975966859, tensor(0.6844)), (0.010111520808513042, tensor(0.6548)), (0.01012554807350493, tensor(0.6225)), (0.010139594797900291, tensor(0.5918)), (0.010153661008694297, tensor(0.5624)), (0.010167746732919562, tensor(0.5352)), (0.010181851997646207, tensor(0.5083)), (0.010195976829981905, tensor(0.4835)), (0.010210121257071934, tensor(0.4622)), (0.010224285306099224, tensor(0.4411)), (0.010238469004284424, tensor(0.4244)), (0.010252672378885939, tensor(0.4103)), (0.01026689545719999, tensor(0.3965)), (0.010281138266560666, tensor(0.3814)), (0.010295400834339972, tensor(0.3705)), (0.010309683187947886, tensor(0.3614)), (0.01032398535483242, tensor(0.3526)), (0.010338307362479645, tensor(0.3435)), (0.010352649238413777, tensor(0.3361)), (0.010367011010197207, tensor(0.3288)), (0.010381392705430573, tensor(0.3228)), (0.010395794351752788, tensor(0.3166)), (0.010410215976841115, tensor(0.3108)), (0.010424657608411214, tensor(0.3054)), (0.01043911927421719, tensor(0.3013)), (0.010453601002051649, tensor(0.2972)), (0.010468102819745757, tensor(0.2923)), (0.010482624755169288, tensor(0.2885)), (0.010497166836230674, tensor(0.2833)), (0.010511729090877065, tensor(0.2786)), (0.010526311547094385, tensor(0.2760)), (0.01054091423290738, tensor(0.2728)), (0.010555537176379668, tensor(0.2697)), (0.010570180405613803, tensor(0.2659)), (0.010584843948751328, tensor(0.2650)), (0.010599527833972817, tensor(0.2620)), (0.010614232089497947, tensor(0.2586)), (0.010628956743585532, tensor(0.2555)), (0.010643701824533598, tensor(0.2531)), (0.010658467360679425, tensor(0.2500)), (0.010673253380399601, tensor(0.2483)), (0.01068805991211008, tensor(0.2453)), (0.01070288698426624, tensor(0.2428)), (0.010717734625362931, tensor(0.2401)), (0.010732602863934536, tensor(0.2376)), (0.010747491728555013, tensor(0.2349)), (0.010762401247837972, tensor(0.2334)), (0.01077733145043671, tensor(0.2305)), (0.010792282365044273, tensor(0.2287)), (0.010807254020393516, tensor(0.2261)), (0.01082224644525715, tensor(0.2239)), (0.0108372596684478, tensor(0.2212)), (0.010852293718818072, tensor(0.2195)), (0.01086734862526058, tensor(0.2173)), (0.010882424416708036, tensor(0.2155)), (0.010897521122133283, tensor(0.2133)), (0.01091263877054935, tensor(0.2117)), (0.010927777391009526, tensor(0.2107)), (0.010942937012607394, tensor(0.2086)), (0.010958117664476909, tensor(0.2081)), (0.01097331937579243, tensor(0.2070)), (0.010988542175768796, tensor(0.2059)), (0.011003786093661372, tensor(0.2038)), (0.011019051158766106, tensor(0.2030)), (0.011034337400419592, tensor(0.2010)), (0.011049644847999116, tensor(0.2001)), (0.011064973530922721, tensor(0.1990)), (0.011080323478649259, tensor(0.1987)), (0.01109569472067845, tensor(0.1976)), (0.011111087286550936, tensor(0.1967)), (0.01112650120584834, tensor(0.1958)), (0.011141936508193324, tensor(0.1942)), (0.01115739322324964, tensor(0.1934)), (0.011172871380722201, tensor(0.1922)), (0.011188371010357112, tensor(0.1914)), (0.01120389214194176, tensor(0.1907)), (0.011219434805304844, tensor(0.1894)), (0.011234999030316451, tensor(0.1888)), (0.011250584846888094, tensor(0.1876)), (0.011266192284972793, tensor(0.1866)), (0.011281821374565116, tensor(0.1854)), (0.011297472145701235, tensor(0.1848)), (0.011313144628459003, tensor(0.1841)), (0.011328838852957986, tensor(0.1840)), (0.01134455484935954, tensor(0.1835)), (0.011360292647866864, tensor(0.1823)), (0.01137605227872505, tensor(0.1815)), (0.01139183377222115, tensor(0.1808)), (0.011407637158684236, tensor(0.1796)), (0.011423462468485452, tensor(0.1797)), (0.011439309732038074, tensor(0.1793)), (0.01145517897979757, tensor(0.1783)), (0.011471070242261653, tensor(0.1777)), (0.011486983549970351, tensor(0.1778)), (0.011502918933506053, tensor(0.1773)), (0.011518876423493576, tensor(0.1772)), (0.011534856050600225, tensor(0.1759)), (0.011550857845535842, tensor(0.1757)), (0.011566881839052873, tensor(0.1761)), (0.011582928061946432, tensor(0.1753)), (0.011598996545054346, tensor(0.1747)), (0.011615087319257221, tensor(0.1740)), (0.011631200415478509, tensor(0.1732)), (0.011647335864684558, tensor(0.1732)), (0.011663493697884672, tensor(0.1725)), (0.01167967394613118, tensor(0.1729)), (0.011695876640519473, tensor(0.1717)), (0.011712101812188099, tensor(0.1710)), (0.011728349492318789, tensor(0.1700)), (0.011744619712136535, tensor(0.1693)), (0.011760912502909648, tensor(0.1687)), (0.011777227895949816, tensor(0.1682)), (0.01179356592261216, tensor(0.1676)), (0.011809926614295304, tensor(0.1670)), (0.011826310002441427, tensor(0.1661)), (0.011842716118536328, tensor(0.1658)), (0.011859144994109479, tensor(0.1651)), (0.011875596660734103, tensor(0.1647)), (0.01189207115002721, tensor(0.1644)), (0.011908568493649683, tensor(0.1639)), (0.011925088723306316, tensor(0.1635)), (0.011941631870745895, tensor(0.1630)), (0.011958197967761241, tensor(0.1624)), (0.011974787046189286, tensor(0.1621)), (0.011991399137911127, tensor(0.1617)), (0.012008034274852086, tensor(0.1609)), (0.012024692488981777, tensor(0.1610)), (0.012041373812314158, tensor(0.1601)), (0.012058078276907604, tensor(0.1599)), (0.012074805914864964, tensor(0.1594)), (0.012091556758333614, tensor(0.1587)), (0.012108330839505538, tensor(0.1587)), (0.012125128190617371, tensor(0.1586)), (0.01214194884395047, tensor(0.1583)), (0.012158792831830972, tensor(0.1579)), (0.012175660186629862, tensor(0.1576)), (0.012192550940763032, tensor(0.1573)), (0.012209465126691344, tensor(0.1571)), (0.012226402776920685, tensor(0.1564)), (0.012243363924002045, tensor(0.1556)), (0.012260348600531563, tensor(0.1549)), (0.012277356839150605, tensor(0.1540)), (0.012294388672545807, tensor(0.1541)), (0.012311444133449163, tensor(0.1538)), (0.012328523254638067, tensor(0.1537)), (0.012345626068935384, tensor(0.1531)), (0.012362752609209516, tensor(0.1527)), (0.012379902908374457, tensor(0.1531)), (0.012397076999389868, tensor(0.1529)), (0.012414274915261123, tensor(0.1529)), (0.012431496689039397, tensor(0.1526)), (0.012448742353821703, tensor(0.1524)), (0.012466011942750974, tensor(0.1518)), (0.01248330548901612, tensor(0.1516)), (0.012500623025852092, tensor(0.1515)), (0.012517964586539945, tensor(0.1510)), (0.012535330204406905, tensor(0.1516)), (0.012552719912826433, tensor(0.1513)), (0.012570133745218284, tensor(0.1507)), (0.012587571735048578, tensor(0.1508)), (0.012605033915829856, tensor(0.1503)), (0.012622520321121156, tensor(0.1506)), (0.012640030984528068, tensor(0.1502)), (0.012657565939702799, tensor(0.1503)), (0.012675125220344245, tensor(0.1499)), (0.012692708860198049, tensor(0.1499)), (0.012710316893056664, tensor(0.1492)), (0.01272794935275943, tensor(0.1487)), (0.012745606273192623, tensor(0.1484)), (0.01276328768828953, tensor(0.1485)), (0.012780993632030516, tensor(0.1480)), (0.012798724138443079, tensor(0.1472)), (0.01281647924160193, tensor(0.1476)), (0.012834258975629042, tensor(0.1477)), (0.01285206337469373, tensor(0.1470)), (0.012869892473012708, tensor(0.1464)), (0.012887746304850156, tensor(0.1465)), (0.01290562490451779, tensor(0.1459)), (0.012923528306374924, tensor(0.1461)), (0.012941456544828533, tensor(0.1461)), (0.012959409654333336, tensor(0.1458)), (0.012977387669391834, tensor(0.1457)), (0.0129953906245544, tensor(0.1452)), (0.013013418554419336, tensor(0.1451)), (0.013031471493632941, tensor(0.1452)), (0.013049549476889577, tensor(0.1455)), (0.013067652538931733, tensor(0.1461)), (0.013085780714550101, tensor(0.1457)), (0.013103934038583634, tensor(0.1459)), (0.013122112545919608, tensor(0.1454)), (0.01314031627149371, tensor(0.1449)), (0.013158545250290081, tensor(0.1443)), (0.0131767995173414, tensor(0.1446)), (0.013195079107728942, tensor(0.1449)), (0.013213384056582652, tensor(0.1448)), (0.013231714399081202, tensor(0.1446)), (0.013250070170452075, tensor(0.1443)), (0.013268451405971618, tensor(0.1443)), (0.013286858140965117, tensor(0.1447)), (0.01330529041080686, tensor(0.1446)), (0.013323748250920218, tensor(0.1446)), (0.01334223169677769, tensor(0.1446)), (0.013360740783900994, tensor(0.1444)), (0.01337927554786112, tensor(0.1439)), (0.013397836024278409, tensor(0.1431)), (0.013416422248822613, tensor(0.1430)), (0.013435034257212968, tensor(0.1428)), (0.013453672085218263, tensor(0.1422)), (0.013472335768656902, tensor(0.1421)), (0.013491025343396988, tensor(0.1416)), (0.013509740845356374, tensor(0.1427)), (0.013528482310502745, tensor(0.1425)), (0.013547249774853679, tensor(0.1427)), (0.013566043274476719, tensor(0.1424)), (0.013584862845489447, tensor(0.1419)), (0.01360370852405955, tensor(0.1416)), (0.013622580346404883, tensor(0.1410)), (0.013641478348793546, tensor(0.1421)), (0.013660402567543955, tensor(0.1416)), (0.013679353039024908, tensor(0.1412)), (0.013698329799655658, tensor(0.1414)), (0.013717332885905976, tensor(0.1409)), (0.013736362334296225, tensor(0.1405)), (0.013755418181397439, tensor(0.1403)), (0.013774500463831376, tensor(0.1407)), (0.013793609218270607, tensor(0.1412)), (0.013812744481438568, tensor(0.1414)), (0.013831906290109648, tensor(0.1416)), (0.013851094681109247, tensor(0.1419)), (0.01387030969131385, tensor(0.1424)), (0.013889551357651105, tensor(0.1419)), (0.01390881971709988, tensor(0.1418)), (0.01392811480669035, tensor(0.1414)), (0.013947436663504054, tensor(0.1414)), (0.013966785324673976, tensor(0.1418)), (0.013986160827384615, tensor(0.1415)), (0.014005563208872047, tensor(0.1416)), (0.01402499250642401, tensor(0.1420)), (0.014044448757379972, tensor(0.1416)), (0.014063931999131193, tensor(0.1419)), (0.014083442269120807, tensor(0.1414)), (0.014102979604843895, tensor(0.1411)), (0.01412254404384755, tensor(0.1412)), (0.014142135623730952, tensor(0.1412)), (0.014161754382145439, tensor(0.1408)), (0.014181400356794587, tensor(0.1415)), (0.014201073585434272, tensor(0.1416)), (0.014220774105872747, tensor(0.1411)), (0.014240501955970717, tensor(0.1411)), (0.014260257173641409, tensor(0.1411)), (0.01428003979685064, tensor(0.1410)), (0.014299849863616907, tensor(0.1407)), (0.014319687412011436, tensor(0.1407)), (0.014339552480158273, tensor(0.1408)), (0.014359445106234355, tensor(0.1406)), (0.014379365328469574, tensor(0.1407)), (0.014399313185146858, tensor(0.1405)), (0.014419288714602248, tensor(0.1407)), (0.014439291955224962, tensor(0.1410)), (0.014459322945457473, tensor(0.1404)), (0.01447938172379559, tensor(0.1402)), (0.014499468328788519, tensor(0.1403)), (0.014519582799038944, tensor(0.1403)), (0.014539725173203106, tensor(0.1404)), (0.014559895489990867, tensor(0.1404)), (0.014580093788165792, tensor(0.1401)), (0.014600320106545217, tensor(0.1400)), (0.014620574484000334, tensor(0.1400)), (0.014640856959456255, tensor(0.1397)), (0.014661167571892094, tensor(0.1399)), (0.014681506360341032, tensor(0.1400)), (0.01470187336389041, tensor(0.1401)), (0.014722268621681784, tensor(0.1397)), (0.014742692172911012, tensor(0.1397)), (0.01476314405682833, tensor(0.1396)), (0.014783624312738419, tensor(0.1391)), (0.014804132980000488, tensor(0.1390)), (0.01482467009802835, tensor(0.1390)), (0.014845235706290491, tensor(0.1392)), (0.01486582984431015, tensor(0.1387)), (0.014886452551665397, tensor(0.1384)), (0.014907103867989204, tensor(0.1382)), (0.01492778383296953, tensor(0.1384)), (0.014948492486349383, tensor(0.1380)), (0.014969229867926915, tensor(0.1379)), (0.014989996017555475, tensor(0.1383)), (0.015010790975143712, tensor(0.1383)), (0.015031614780655627, tensor(0.1385)), (0.01505246747411067, tensor(0.1386)), (0.01507334909558381, tensor(0.1384)), (0.015094259685205598, tensor(0.1386)), (0.015115199283162267, tensor(0.1396)), (0.015136167929695792, tensor(0.1392)), (0.01515716566510398, tensor(0.1393)), (0.01517819252974054, tensor(0.1392)), (0.015199248564015158, tensor(0.1389)), (0.01522033380839358, tensor(0.1385)), (0.015241448303397694, tensor(0.1393)), (0.015262592089605592, tensor(0.1393)), (0.015283765207651666, tensor(0.1392)), (0.015304967698226677, tensor(0.1387)), (0.015326199602077832, tensor(0.1387)), (0.015347460960008868, tensor(0.1383)), (0.015368751812880124, tensor(0.1383)), (0.015390072201608625, tensor(0.1387)), (0.015411422167168157, tensor(0.1383)), (0.015432801750589349, tensor(0.1386)), (0.015454210992959747, tensor(0.1384)), (0.015475649935423899, tensor(0.1386)), (0.015497118619183431, tensor(0.1385)), (0.015518617085497122, tensor(0.1387)), (0.01554014537568099, tensor(0.1386)), (0.015561703531108374, tensor(0.1390)), (0.015583291593209998, tensor(0.1389)), (0.01560490960347407, tensor(0.1386)), (0.015626557603446348, tensor(0.1383)), (0.015648235634730227, tensor(0.1384)), (0.015669943738986815, tensor(0.1390)), (0.015691681957935015, tensor(0.1394)), (0.015713450333351607, tensor(0.1395)), (0.01573524890707132, tensor(0.1393)), (0.015757077720986924, tensor(0.1391)), (0.015778936817049304, tensor(0.1389)), (0.015800826237267546, tensor(0.1385)), (0.015822746023709, tensor(0.1383)), (0.015844696218499384, tensor(0.1382)), (0.015866676863822857, tensor(0.1384)), (0.015888688001922096, tensor(0.1388)), (0.015910729675098375, tensor(0.1389)), (0.015932801925711657, tensor(0.1391)), (0.015954904796180662, tensor(0.1392)), (0.01597703832898296, tensor(0.1390)), (0.01599920256665505, tensor(0.1392)), (0.01602139755179244, tensor(0.1389)), (0.016043623327049727, tensor(0.1391)), (0.01606587993514068, tensor(0.1394)), (0.016088167418838315, tensor(0.1394)), (0.016110485820975004, tensor(0.1394)), (0.016132835184442525, tensor(0.1398)), (0.01615521555219216, tensor(0.1394)), (0.016177626967234782, tensor(0.1391)), (0.016200069472640917, tensor(0.1388)), (0.016222543111540855, tensor(0.1386)), (0.01624504792712471, tensor(0.1385)), (0.016267583962642516, tensor(0.1384)), (0.016290151261404307, tensor(0.1379)), (0.016312749866780194, tensor(0.1376)), (0.016335379822200454, tensor(0.1374)), (0.01635804117115562, tensor(0.1370)), (0.016380733957196553, tensor(0.1373)), (0.016403458223934526, tensor(0.1368)), (0.016426214015041317, tensor(0.1366)), (0.016449001374249286, tensor(0.1366)), (0.01647182034535146, tensor(0.1366)), (0.016494670972201628, tensor(0.1366)), (0.016517553298714398, tensor(0.1362)), (0.016540467368865313, tensor(0.1352)), (0.01656341322669091, tensor(0.1349)), (0.016586390916288832, tensor(0.1354)), (0.01660940048181788, tensor(0.1349)), (0.016632441967498128, tensor(0.1356)), (0.01665551541761098, tensor(0.1354)), (0.01667862087649928, tensor(0.1353)), (0.016701758388567387, tensor(0.1357)), (0.016724927998281257, tensor(0.1357)), (0.016748129750168532, tensor(0.1356)), (0.016771363688818625, tensor(0.1356)), (0.016794629858882807, tensor(0.1357)), (0.01681792830507429, tensor(0.1354)), (0.01684125907216832, tensor(0.1355)), (0.01686462220500225, tensor(0.1356)), (0.01688801774847564, tensor(0.1357)), (0.01691144574755033, tensor(0.1354)), (0.016934906247250543, tensor(0.1356)), (0.016958399292662955, tensor(0.1358)), (0.016981924928936794, tensor(0.1361)), (0.01700548320128392, tensor(0.1359)), (0.0170290741549789, tensor(0.1356)), (0.017052697835359135, tensor(0.1358)), (0.017076354287824898, tensor(0.1359)), (0.017100043557839454, tensor(0.1356)), (0.01712376569092914, tensor(0.1355)), (0.017147520732683434, tensor(0.1355)), (0.017171308728755077, tensor(0.1354)), (0.01719512972486013, tensor(0.1355)), (0.01721898376677808, tensor(0.1353)), (0.01724287090035192, tensor(0.1348)), (0.017266791171488237, tensor(0.1345)), (0.017290744626157303, tensor(0.1344)), (0.01731473131039317, tensor(0.1343)), (0.017338751270293735, tensor(0.1337)), (0.01736280455202086, tensor(0.1340)), (0.017386891201800436, tensor(0.1339)), (0.017411011265922482, tensor(0.1346)), (0.017435164790741246, tensor(0.1343)), (0.01745935182267526, tensor(0.1341)), (0.017483572408207464, tensor(0.1340)), (0.017507826593885282, tensor(0.1341)), (0.017532114426320702, tensor(0.1339)), (0.017556435952190388, tensor(0.1339)), (0.01758079121823574, tensor(0.1341)), (0.017605180271263017, tensor(0.1343)), (0.017629603158143402, tensor(0.1341)), (0.017654059925813096, tensor(0.1344)), (0.017678550621273423, tensor(0.1342)), (0.017703075291590903, tensor(0.1346)), (0.017727633983897345, tensor(0.1342)), (0.017752226745389958, tensor(0.1339)), (0.017776853623331403, tensor(0.1339)), (0.017801514665049926, tensor(0.1341)), (0.017826209917939425, tensor(0.1340)), (0.017850939429459534, tensor(0.1342)), (0.01787570324713574, tensor(0.1344)), (0.01790050141855945, tensor(0.1344)), (0.017925333991388098, tensor(0.1348)), (0.01795020101334523, tensor(0.1346)), (0.017975102532220594, tensor(0.1347)), (0.01800003859587024, tensor(0.1346)), (0.018025009252216603, tensor(0.1345)), (0.0180500145492486, tensor(0.1340)), (0.018075054535021718, tensor(0.1343)), (0.01810012925765811, tensor(0.1344)), (0.018125238765346687, tensor(0.1348)), (0.018150383106343218, tensor(0.1346)), (0.018175562328970402, tensor(0.1343)), (0.018200776481617983, tensor(0.1345)), (0.018226025612742832, tensor(0.1347)), (0.01825130977086904, tensor(0.1346)), (0.01827662900458801, tensor(0.1350)), (0.018301983362558567, tensor(0.1348)), (0.018327372893507027, tensor(0.1347)), (0.0183527976462273, tensor(0.1340)), (0.018378257669580997, tensor(0.1344)), (0.0184037530124975, tensor(0.1343)), (0.01842928372397408, tensor(0.1349)), (0.018454849853075966, tensor(0.1345)), (0.01848045144893647, tensor(0.1348)), (0.018506088560757045, tensor(0.1352)), (0.018531761237807417, tensor(0.1353)), (0.018557469529425656, tensor(0.1352)), (0.018583213485018266, tensor(0.1360)), (0.018608993154060307, tensor(0.1361)), (0.018634808586095463, tensor(0.1360)), (0.01866065983073615, tensor(0.1360)), (0.01868654693766361, tensor(0.1361)), (0.018712469956628, tensor(0.1359)), (0.01873842893744851, tensor(0.1360)), (0.018764423930013423, tensor(0.1364)), (0.018790454984280235, tensor(0.1366)), (0.018816522150275756, tensor(0.1367)), (0.018842625478096175, tensor(0.1367)), (0.018868765017907203, tensor(0.1365)), (0.018894940819944125, tensor(0.1364)), (0.01892115293451192, tensor(0.1370)), (0.018947401411985355, tensor(0.1367)), (0.01897368630280908, tensor(0.1371)), (0.019000007657497722, tensor(0.1373)), (0.019026365526635985, tensor(0.1376)), (0.01905275996087875, tensor(0.1381)), (0.019079191010951166, tensor(0.1379)), (0.019105658727648748, tensor(0.1375)), (0.01913216316183749, tensor(0.1373)), (0.01915870436445393, tensor(0.1372)), (0.019185282386505288, tensor(0.1370)), (0.019211897279069533, tensor(0.1371)), (0.019238549093295493, tensor(0.1369)), (0.019265237880402956, tensor(0.1377)), (0.019291963691682765, tensor(0.1376)), (0.01931872657849691, tensor(0.1372)), (0.01934552659227864, tensor(0.1372)), (0.019372363784532554, tensor(0.1369)), (0.019399238206834698, tensor(0.1369)), (0.019426149910832666, tensor(0.1366)), (0.01945309894824571, tensor(0.1368)), (0.01948008537086482, tensor(0.1362)), (0.019507109230552835, tensor(0.1363)), (0.019534170579244548, tensor(0.1372)), (0.019561269468946787, tensor(0.1370)), (0.01958840595173854, tensor(0.1369)), (0.019615580079771027, tensor(0.1368)), (0.019642791905267826, tensor(0.1365)), (0.019670041480524966, tensor(0.1372)), (0.019697328857911013, tensor(0.1377)), (0.019724654089867184, tensor(0.1381)), (0.019752017228907452, tensor(0.1379)), (0.01977941832761863, tensor(0.1379)), (0.019806857438660494, tensor(0.1379)), (0.019834334614765862, tensor(0.1382)), (0.01986184990874072, tensor(0.1378)), (0.019889403373464287, tensor(0.1379)), (0.019916995061889164, tensor(0.1383)), (0.01994462502704139, tensor(0.1382)), (0.01997229332202058, tensor(0.1377)), (0.02, tensor(0.1379)), (0.020027745114226694, tensor(0.1381)), (0.020055528718021555, tensor(0.1384)), (0.020083350864779463, tensor(0.1378)), (0.020111211607969363, tensor(0.1377)), (0.02013911100113438, tensor(0.1371)), (0.020167049097891902, tensor(0.1370)), (0.020195025951933718, tensor(0.1373)), (0.020223041617026084, tensor(0.1375)), (0.02025109614700986, tensor(0.1371)), (0.020279189595800582, tensor(0.1370)), (0.020307322017388593, tensor(0.1366)), (0.020335493465839124, tensor(0.1364)), (0.020363703995292415, tensor(0.1366)), (0.02039195365996381, tensor(0.1371)), (0.020420242514143868, tensor(0.1369)), (0.020448570612198447, tensor(0.1370)), (0.020476938008568847, tensor(0.1375)), (0.020505344757771878, tensor(0.1379)), (0.02053379091439998, tensor(0.1384)), (0.020562276533121333, tensor(0.1383)), (0.020590801668679944, tensor(0.1383)), (0.02061936637589578, tensor(0.1381)), (0.02064797070966484, tensor(0.1383)), (0.02067661472495929, tensor(0.1381)), (0.020705298476827554, tensor(0.1379)), (0.02073402202039442, tensor(0.1382)), (0.020762785410861146, tensor(0.1384)), (0.020791588703505576, tensor(0.1386)), (0.02082043195368223, tensor(0.1382)), (0.02084931521682243, tensor(0.1388)), (0.02087823854843438, tensor(0.1384)), (0.020907202004103297, tensor(0.1386)), (0.020936205639491518, tensor(0.1386)), (0.020965249510338575, tensor(0.1387)), (0.020994333672461347, tensor(0.1393)), (0.021023458181754134, tensor(0.1399)), (0.021052623094188774, tensor(0.1395)), (0.02108182846581476, tensor(0.1395)), (0.021111074352759336, tensor(0.1392)), (0.02114036081122761, tensor(0.1399)), (0.021169687897502655, tensor(0.1402)), (0.021199055667945638, tensor(0.1403)), (0.021228464178995893, tensor(0.1406)), (0.021257913487171067, tensor(0.1403)), (0.0212874036490672, tensor(0.1406)), (0.02131693472135885, tensor(0.1407)), (0.021346506760799203, tensor(0.1404)), (0.021376119824220163, tensor(0.1408)), (0.02140577396853248, tensor(0.1408)), (0.021435469250725862, tensor(0.1411)), (0.02146520572786907, tensor(0.1413)), (0.02149498345711003, tensor(0.1419)), (0.021524802495675944, tensor(0.1420)), (0.02155466290087342, tensor(0.1415)), (0.021584564730088546, tensor(0.1413)), (0.02161450804078703, tensor(0.1411)), (0.0216444928905143, tensor(0.1411)), (0.021674519336895605, tensor(0.1405)), (0.021704587437636143, tensor(0.1407)), (0.021734697250521164, tensor(0.1413)), (0.02176484883341608, tensor(0.1414)), (0.021795042244266566, tensor(0.1407)), (0.0218252775410987, tensor(0.1411)), (0.021855554782019046, tensor(0.1410)), (0.021885874025214788, tensor(0.1410)), (0.021916235328953815, tensor(0.1413)), (0.02194663875158486, tensor(0.1411)), (0.02197708435153759, tensor(0.1408)), (0.022007572187322744, tensor(0.1408)), (0.022038102317532213, tensor(0.1408)), (0.022068674800839183, tensor(0.1410)), (0.022099289695998232, tensor(0.1414)), (0.022129947061845442, tensor(0.1412)), (0.022160646957298517, tensor(0.1409)), (0.022191389441356898, tensor(0.1410)), (0.022222174573101872, tensor(0.1413)), (0.02225300241169668, tensor(0.1413)), (0.022283873016386645, tensor(0.1413)), (0.02231478644649928, tensor(0.1413)), (0.022345742761444395, tensor(0.1417)), (0.022376742020714224, tensor(0.1423)), (0.02240778428388352, tensor(0.1427)), (0.022438869610609688, tensor(0.1424)), (0.022469998060632896, tensor(0.1428)), (0.022501169693776187, tensor(0.1429)), (0.022532384569945586, tensor(0.1427)), (0.022563642749130225, tensor(0.1423)), (0.02259494429140247, tensor(0.1426)), (0.022626289256918005, tensor(0.1432)), (0.022657677705915973, tensor(0.1433)), (0.02268910969871908, tensor(0.1436)), (0.022720585295733727, tensor(0.1440)), (0.02275210455745009, tensor(0.1439)), (0.0227836675444423, tensor(0.1439)), (0.022815274317368472, tensor(0.1437)), (0.022846924936970905, tensor(0.1441)), (0.02287861946407615, tensor(0.1442)), (0.02291035795959514, tensor(0.1450)), (0.022942140484523303, tensor(0.1455)), (0.0229739670999407, tensor(0.1459)), (0.023005837867012106, tensor(0.1455)), (0.023037752846987152, tensor(0.1458)), (0.02306971210120045, tensor(0.1459)), (0.02310171569107168, tensor(0.1460)), (0.023133763678105747, tensor(0.1467)), (0.023165856123892863, tensor(0.1465)), (0.023197993090108688, tensor(0.1461)), (0.023230174638514442, tensor(0.1465)), (0.023262400830957018, tensor(0.1471)), (0.023294671729369117, tensor(0.1463)), (0.023326987395769345, tensor(0.1465)), (0.023359347892262353, tensor(0.1460)), (0.023391753281038947, tensor(0.1461)), (0.023424203624376198, tensor(0.1462)), (0.023456698984637578, tensor(0.1468)), (0.02348923942427307, tensor(0.1471)), (0.023521825005819296, tensor(0.1468)), (0.02355445579189963, tensor(0.1464)), (0.02358713184522432, tensor(0.1463)), (0.023619853228590608, tensor(0.1466)), (0.023652620004882854, tensor(0.1465)), (0.023685432237072656, tensor(0.1465)), (0.023718289988218958, tensor(0.1471)), (0.023751193321468207, tensor(0.1471)), (0.02378414230005442, tensor(0.1475)), (0.023817136987299366, tensor(0.1474)), (0.023850177446612632, tensor(0.1474)), (0.02388326374149179, tensor(0.1473)), (0.023916395935522482, tensor(0.1474)), (0.02394957409237857, tensor(0.1475)), (0.023982798275822254, tensor(0.1470)), (0.024016068549704173, tensor(0.1469)), (0.024049384977963554, tensor(0.1466)), (0.024082747624628316, tensor(0.1469)), (0.02411615655381521, tensor(0.1467)), (0.02414961182972993, tensor(0.1476)), (0.02418311351666723, tensor(0.1480)), (0.024216661679011077, tensor(0.1491)), (0.024250256381234743, tensor(0.1485)), (0.02428389768790094, tensor(0.1490)), (0.024317585663661944, tensor(0.1489)), (0.024351320373259724, tensor(0.1492)), (0.024385101881526063, tensor(0.1490)), (0.024418930253382688, tensor(0.1489)), (0.024452805553841373, tensor(0.1487)), (0.02448672784800409, tensor(0.1487)), (0.024520697201063126, tensor(0.1485)), (0.02455471367830121, tensor(0.1488)), (0.024588777345091618, tensor(0.1486)), (0.024622888266898325, tensor(0.1487)), (0.024657046509276134, tensor(0.1490)), (0.02469125213787077, tensor(0.1492)), (0.02472550521841903, tensor(0.1494)), (0.024759805816748914, tensor(0.1493)), (0.024794153998779735, tensor(0.1496)), (0.024828549830522247, tensor(0.1492)), (0.024862993378078794, tensor(0.1491)), (0.024897484707643407, tensor(0.1489)), (0.024932023885501947, tensor(0.1488)), (0.02496661097803224, tensor(0.1487)), (0.025001246051704184, tensor(0.1487)), (0.02503592917307989, tensor(0.1484)), (0.02507066040881381, tensor(0.1491)), (0.025105439825652866, tensor(0.1487)), (0.025140267490436567, tensor(0.1486)), (0.025175143470097156, tensor(0.1484)), (0.025210067831659716, tensor(0.1487)), (0.025245040642242315, tensor(0.1495)), (0.025280061969056137, tensor(0.1498)), (0.025315131879405598, tensor(0.1493)), (0.02535025044068849, tensor(0.1495)), (0.0253854177203961, tensor(0.1493)), (0.025420633786113332, tensor(0.1487)), (0.02545589870551886, tensor(0.1483)), (0.025491212546385245, tensor(0.1483)), (0.025526575376579062, tensor(0.1485)), (0.02556198726406103, tensor(0.1491)), (0.02559744827688616, tensor(0.1485)), (0.02563295848320386, tensor(0.1489)), (0.025668517951258085, tensor(0.1487)), (0.02570412674938746, tensor(0.1488)), (0.025739784946025416, tensor(0.1488)), (0.02577549260970031, tensor(0.1487)), (0.02581124980903558, tensor(0.1488)), (0.025847056612749848, tensor(0.1485)), (0.02588291308965707, tensor(0.1479)), (0.02591881930866667, tensor(0.1476)), (0.025954775338783667, tensor(0.1475)), (0.0259907812491088, tensor(0.1474)), (0.026026837108838668, tensor(0.1476)), (0.026062942987265882, tensor(0.1470)), (0.02609909895377915, tensor(0.1464)), (0.026135305077863467, tensor(0.1467)), (0.026171561429100203, tensor(0.1468)), (0.026207868077167264, tensor(0.1466)), (0.026244225091839216, tensor(0.1467)), (0.026280632542987417, tensor(0.1466)), (0.026317090500580162, tensor(0.1469)), (0.0263535990346828, tensor(0.1468)), (0.026390158215457885, tensor(0.1473)), (0.0264267681131653, tensor(0.1478)), (0.0264634287981624, tensor(0.1475)), (0.026500140340904147, tensor(0.1474)), (0.026536902811943232, tensor(0.1478)), (0.02657371628193023, tensor(0.1480)), (0.02661058082161372, tensor(0.1481)), (0.026647496501840437, tensor(0.1486)), (0.026684463393555378, tensor(0.1481)), (0.026721481567801988, tensor(0.1483)), (0.02675855109572224, tensor(0.1490)), (0.026795672048556818, tensor(0.1490)), (0.026832844497645225, tensor(0.1489)), (0.026870068514425936, tensor(0.1488)), (0.026907344170436522, tensor(0.1485)), (0.026944671537313804, tensor(0.1476)), (0.026982050686793976, tensor(0.1478)), (0.02701948169071275, tensor(0.1475)), (0.027056964621005486, tensor(0.1472)), (0.027094499549707357, tensor(0.1469)), (0.027132086548953438, tensor(0.1473)), (0.027169725690978894, tensor(0.1474)), (0.0272074170481191, tensor(0.1470)), (0.027245160692809762, tensor(0.1467)), (0.027282956697587093, tensor(0.1466)), (0.02732080513508791, tensor(0.1470)), (0.027358706078049817, tensor(0.1471)), (0.027396659599311316, tensor(0.1473)), (0.027434665771811945, tensor(0.1472)), (0.02747272466859245, tensor(0.1491)), (0.027510836362794874, tensor(0.1495)), (0.027549000927662753, tensor(0.1504)), (0.027587218436541213, tensor(0.1511)), (0.027625488962877136, tensor(0.1515)), (0.027663812580219296, tensor(0.1518)), (0.027702189362218493, tensor(0.1522)), (0.0277406193826277, tensor(0.1530)), (0.02777910271530221, tensor(0.1531)), (0.02781763943419976, tensor(0.1536)), (0.0278562296133807, tensor(0.1540)), (0.02789487332700811, tensor(0.1545)), (0.02793357064934795, tensor(0.1541)), (0.02797232165476923, tensor(0.1541)), (0.028011126417744094, tensor(0.1535)), (0.02804998501284802, tensor(0.1535)), (0.028088897514759945, tensor(0.1533)), (0.028127863998262385, tensor(0.1533)), (0.028166884538241614, tensor(0.1532)), (0.02820595920968779, tensor(0.1534)), (0.0282450880876951, tensor(0.1537)), (0.028284271247461905, tensor(0.1537)), (0.028323508764290878, tensor(0.1540)), (0.028362800713589174, tensor(0.1543)), (0.028402147170868544, tensor(0.1547)), (0.028441548211745493, tensor(0.1555)), (0.028481003911941433, tensor(0.1557)), (0.028520514347282817, tensor(0.1564)), (0.02856007959370128, tensor(0.1564)), (0.028599699727233814, tensor(0.1567)), (0.028639374824022873, tensor(0.1566)), (0.028679104960316545, tensor(0.1566)), (0.02871889021246871, tensor(0.1575)), (0.02875873065693915, tensor(0.1574)), (0.028798626370293717, tensor(0.1575)), (0.028838577429204496, tensor(0.1573)), (0.028878583910449923, tensor(0.1578)), (0.028918645890914946, tensor(0.1581)), (0.02895876344759118, tensor(0.1582)), (0.028998936657577037, tensor(0.1579)), (0.029039165598077888, tensor(0.1579)), (0.029079450346406212, tensor(0.1584)), (0.029119790979981734, tensor(0.1587)), (0.029160187576331584, tensor(0.1587)), (0.029200640213090434, tensor(0.1587)), (0.029241148968000667, tensor(0.1588)), (0.02928171391891251, tensor(0.1592)), (0.029322335143784187, tensor(0.1588)), (0.029363012720682063, tensor(0.1591)), (0.02940374672778082, tensor(0.1601)), (0.02944453724336357, tensor(0.1595)), (0.029485384345822024, tensor(0.1592)), (0.02952628811365666, tensor(0.1593)), (0.029567248625476838, tensor(0.1593)), (0.029608265960000983, tensor(0.1590)), (0.029649340196056705, tensor(0.1589)), (0.029690471412580983, tensor(0.1592)), (0.0297316596886203, tensor(0.1595)), (0.029772905103330794, tensor(0.1599)), (0.029814207735978412, tensor(0.1598)), (0.02985556766593906, tensor(0.1594)), (0.02989698497269877, tensor(0.1595)), (0.02993845973585383, tensor(0.1593)), (0.029979992035110953, tensor(0.1596)), (0.030021581950287424, tensor(0.1588)), (0.030063229561311258, tensor(0.1589)), (0.03010493494822135, tensor(0.1592)), (0.03014669819116762, tensor(0.1598)), (0.030188519370411195, tensor(0.1597)), (0.030230398566324534, tensor(0.1596)), (0.030272335859391583, tensor(0.1596)), (0.030314331330207965, tensor(0.1592)), (0.030356385059481083, tensor(0.1595)), (0.030398497128030316, tensor(0.1590)), (0.030440667616787164, tensor(0.1589)), (0.030482896606795387, tensor(0.1587)), (0.030525184179211188, tensor(0.1582)), (0.030567530415303336, tensor(0.1581)), (0.030609935396453354, tensor(0.1588)), (0.030652399204155665, tensor(0.1585)), (0.03069492192001774, tensor(0.1581)), (0.03073750362576025, tensor(0.1580)), (0.03078014440321725, tensor(0.1578)), (0.030822844334336318, tensor(0.1576)), (0.030865603501178694, tensor(0.1572)), (0.03090842198591949, tensor(0.1569)), (0.030951299870847798, tensor(0.1580)), (0.030994237238366855, tensor(0.1576)), (0.03103723417099424, tensor(0.1581)), (0.03108029075136198, tensor(0.1580)), (0.03112340706221674, tensor(0.1580)), (0.03116658318641999, tensor(0.1588)), (0.03120981920694814, tensor(0.1590)), (0.031253115206892695, tensor(0.1592)), (0.03129647126946045, tensor(0.1587)), (0.03133988747797363, tensor(0.1588)), (0.03138336391587003, tensor(0.1584)), (0.03142690066670321, tensor(0.1588)), (0.031470497814142635, tensor(0.1583)), (0.03151415544197385, tensor(0.1585)), (0.03155787363409861, tensor(0.1591)), (0.031601652474535086, tensor(0.1587)), (0.03164549204741799, tensor(0.1587)), (0.03168939243699876, tensor(0.1596)), (0.03173335372764571, tensor(0.1592)), (0.031777376003844185, tensor(0.1589)), (0.03182145935019674, tensor(0.1589)), (0.031865603851423306, tensor(0.1590)), (0.03190980959236132, tensor(0.1591)), (0.031954076657965916, tensor(0.1600)), (0.0319984051333101, tensor(0.1612)), (0.03204279510358488, tensor(0.1612)), (0.03208724665409945, tensor(0.1615)), (0.03213175987028136, tensor(0.1614)), (0.03217633483767663, tensor(0.1611)), (0.03222097164195001, tensor(0.1611)), (0.03226567036888505, tensor(0.1621)), (0.03231043110438432, tensor(0.1620)), (0.032355253934469565, tensor(0.1629)), (0.032400138945281834, tensor(0.1632)), (0.03244508622308171, tensor(0.1636)), (0.03249009585424942, tensor(0.1642)), (0.03253516792528503, tensor(0.1641)), (0.032580302522808614, tensor(0.1641)), (0.03262549973356039, tensor(0.1644)), (0.03267075964440091, tensor(0.1645)), (0.03271608234231124, tensor(0.1651)), (0.032761467914393105, tensor(0.1652)), (0.03280691644786905, tensor(0.1650)), (0.032852428030082634, tensor(0.1655)), (0.03289800274849857, tensor(0.1660)), (0.03294364069070292, tensor(0.1658)), (0.032989341944403255, tensor(0.1655)), (0.033035106597428796, tensor(0.1654)), (0.033080934737730626, tensor(0.1658)), (0.03312682645338182, tensor(0.1661)), (0.033172781832577665, tensor(0.1662)), (0.03321880096363576, tensor(0.1658)), (0.033264883934996256, tensor(0.1662)), (0.03331103083522196, tensor(0.1665)), (0.03335724175299856, tensor(0.1663)), (0.033403516777134774, tensor(0.1662)), (0.033449855996562514, tensor(0.1663)), (0.033496259500337064, tensor(0.1662)), (0.03354272737763725, tensor(0.1663)), (0.033589259717765614, tensor(0.1658)), (0.03363585661014858, tensor(0.1660)), (0.03368251814433664, tensor(0.1657)), (0.0337292444100045, tensor(0.1656)), (0.03377603549695128, tensor(0.1657)), (0.03382289149510066, tensor(0.1661)), (0.033869812494501085, tensor(0.1658)), (0.03391679858532591, tensor(0.1664)), (0.03396384985787359, tensor(0.1667)), (0.03401096640256784, tensor(0.1667)), (0.0340581483099578, tensor(0.1669)), (0.03410539567071827, tensor(0.1668)), (0.034152708575649796, tensor(0.1669)), (0.03420008711567891, tensor(0.1671)), (0.03424753138185828, tensor(0.1674)), (0.03429504146536687, tensor(0.1676)), (0.034342617457510154, tensor(0.1683)), (0.03439025944972026, tensor(0.1685)), (0.03443796753355616, tensor(0.1686)), (0.03448574180070384, tensor(0.1689)), (0.034533582342976474, tensor(0.1689)), (0.03458148925231461, tensor(0.1689)), (0.03462946262078634, tensor(0.1685)), (0.03467750254058747, tensor(0.1686)), (0.03472560910404172, tensor(0.1681)), (0.03477378240360087, tensor(0.1679)), (0.034822022531844965, tensor(0.1680)), (0.03487032958148249, tensor(0.1679)), (0.03491870364535052, tensor(0.1687)), (0.03496714481641493, tensor(0.1689)), (0.035015653187770564, tensor(0.1685)), (0.035064228852641405, tensor(0.1679)), (0.035112871904380775, tensor(0.1679)), (0.03516158243647148, tensor(0.1680)), (0.03521036054252604, tensor(0.1685)), (0.03525920631628681, tensor(0.1686)), (0.03530811985162619, tensor(0.1691)), (0.035357101242546846, tensor(0.1689)), (0.035406150583181806, tensor(0.1693)), (0.03545526796779469, tensor(0.1696)), (0.035504453490779915, tensor(0.1695)), (0.03555370724666281, tensor(0.1698)), (0.03560302933009986, tensor(0.1693)), (0.03565241983587885, tensor(0.1687)), (0.035701878858919074, tensor(0.1689)), (0.03575140649427148, tensor(0.1690)), (0.0358010028371189, tensor(0.1693)), (0.035850667982776196, tensor(0.1700)), (0.03590040202669046, tensor(0.1700)), (0.035950205064441194, tensor(0.1702)), (0.03600007719174048, tensor(0.1700)), (0.036050018504433214, tensor(0.1703)), (0.0361000290984972, tensor(0.1701)), (0.036150109070043436, tensor(0.1700)), (0.03620025851531622, tensor(0.1701)), (0.03625047753069338, tensor(0.1699)), (0.036300766212686436, tensor(0.1700)), (0.036351124657940805, tensor(0.1704)), (0.03640155296323597, tensor(0.1703)), (0.036452051225485664, tensor(0.1702)), (0.03650261954173808, tensor(0.1703)), (0.036553258009176026, tensor(0.1698)), (0.03660396672511714, tensor(0.1692)), (0.036654745787014054, tensor(0.1690)), (0.0367055952924546, tensor(0.1693)), (0.036756515339161994, tensor(0.1692)), (0.036807506024995, tensor(0.1697)), (0.03685856744794815, tensor(0.1691)), (0.03690969970615193, tensor(0.1695)), (0.03696090289787293, tensor(0.1703)), (0.03701217712151409, tensor(0.1707)), (0.037063522475614834, tensor(0.1711)), (0.037114939058851305, tensor(0.1709)), (0.037166426970036526, tensor(0.1714)), (0.037217986308120614, tensor(0.1709)), (0.037269617172190926, tensor(0.1710)), (0.0373213196614723, tensor(0.1706)), (0.03737309387532722, tensor(0.1703)), (0.037424939913256, tensor(0.1701)), (0.03747685787489702, tensor(0.1708)), (0.037528847860026845, tensor(0.1712)), (0.03758090996856047, tensor(0.1720)), (0.037633044300551505, tensor(0.1724)), (0.037685250956192344, tensor(0.1724)), (0.037737530035814405, tensor(0.1722)), (0.037789881639888244, tensor(0.1717)), (0.03784230586902384, tensor(0.1720)), (0.03789480282397071, tensor(0.1725)), (0.03794737260561816, tensor(0.1724)), (0.03800001531499544, tensor(0.1723)), (0.03805273105327196, tensor(0.1722)), (0.038105519921757494, tensor(0.1722)), (0.038158382021902325, tensor(0.1725)), (0.038211317455297496, tensor(0.1726)), (0.03826432632367497, tensor(0.1730)), (0.03831740872890786, tensor(0.1728)), (0.038370564773010575, tensor(0.1722)), (0.03842379455813907, tensor(0.1719)), (0.03847709818659099, tensor(0.1718)), (0.03853047576080591, tensor(0.1717)), (0.03858392738336553, tensor(0.1717)), (0.03863745315699382, tensor(0.1720)), (0.03869105318455728, tensor(0.1720)), (0.03874472756906511, tensor(0.1720)), (0.038798476413669396, tensor(0.1718)), (0.03885229982166533, tensor(0.1718)), (0.03890619789649142, tensor(0.1720)), (0.03896017074172964, tensor(0.1719)), (0.03901421846110567, tensor(0.1725)), (0.039068341158489096, tensor(0.1723)), (0.039122538937893574, tensor(0.1722)), (0.03917681190347708, tensor(0.1726)), (0.039231160159542054, tensor(0.1723)), (0.03928558381053565, tensor(0.1725)), (0.03934008296104993, tensor(0.1724)), (0.039394657715822026, tensor(0.1722)), (0.03944930817973437, tensor(0.1726)), (0.039504034457814904, tensor(0.1718)), (0.03955883665523726, tensor(0.1714)), (0.03961371487732099, tensor(0.1719)), (0.039668669229531724, tensor(0.1727)), (0.03972369981748144, tensor(0.1732)), (0.039778806746928574, tensor(0.1731)), (0.03983399012377833, tensor(0.1731)), (0.03988925005408278, tensor(0.1735)), (0.03994458664404116, tensor(0.1739))]
******

*** 2018-12-29 13:35:27,733 - code.resnet_fastai - DEBUG ***
Best LR: 0.017338751270293735
******

*** 2018-12-29 13:35:27,767 - matplotlib.axes._base - DEBUG ***
update_title_pos
******

*** 2018-12-29 13:35:27,770 - matplotlib.ticker - DEBUG ***
vmin 0.009471074402518941 vmax 0.042468701550101975
******

*** 2018-12-29 13:35:27,770 - matplotlib.ticker - DEBUG ***
ticklocs array([1.e-04, 1.e-03, 1.e-02, 1.e-01, 1.e+00])
******

*** 2018-12-29 13:35:27,777 - matplotlib.ticker - DEBUG ***
vmin 0.009471074402518941 vmax 0.042468701550101975
******

*** 2018-12-29 13:35:27,777 - matplotlib.ticker - DEBUG ***
ticklocs [0.0002, 0.00030000000000000003, 0.0004, 0.0005, 0.0006000000000000001, 0.0007, 0.0008, 0.0009000000000000001, 0.002, 0.003, 0.004, 0.005, 0.006, 0.007, 0.008, 0.009000000000000001, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.2, 0.30000000000000004, 0.4, 0.5, 0.6000000000000001, 0.7000000000000001, 0.8, 0.9, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0]
******

*** 2018-12-29 13:35:27,861 - matplotlib.font_manager - DEBUG ***
findfont: Matching :family=sans-serif:style=normal:variant=normal:weight=normal:stretch=normal:size=10.0 to DejaVu Sans ('/home/ubuntu/.local/lib/python3.6/site-packages/matplotlib/mpl-data/fonts/ttf/DejaVuSans.ttf') with score of 0.050000.
******

*** 2018-12-29 13:35:27,871 - matplotlib.font_manager - DEBUG ***
findfont: Matching :family=STIXGeneral:style=normal:variant=normal:weight=normal:stretch=normal:size=10.0 to STIXGeneral ('/home/ubuntu/.local/lib/python3.6/site-packages/matplotlib/mpl-data/fonts/ttf/STIXGeneral.ttf') with score of 0.050000.
******

*** 2018-12-29 13:35:27,877 - matplotlib.font_manager - DEBUG ***
findfont: Matching :family=STIXGeneral:style=italic:variant=normal:weight=normal:stretch=normal:size=10.0 to STIXGeneral ('/home/ubuntu/.local/lib/python3.6/site-packages/matplotlib/mpl-data/fonts/ttf/STIXGeneralItalic.ttf') with score of 0.050000.
******

*** 2018-12-29 13:35:27,878 - matplotlib.font_manager - DEBUG ***
findfont: Matching :family=STIXGeneral:style=normal:variant=normal:weight=bold:stretch=normal:size=10.0 to STIXGeneral ('/home/ubuntu/.local/lib/python3.6/site-packages/matplotlib/mpl-data/fonts/ttf/STIXGeneralBol.ttf') with score of 0.000000.
******

*** 2018-12-29 13:35:27,883 - matplotlib.font_manager - DEBUG ***
findfont: Matching :family=STIXNonUnicode:style=normal:variant=normal:weight=normal:stretch=normal:size=10.0 to STIXNonUnicode ('/home/ubuntu/.local/lib/python3.6/site-packages/matplotlib/mpl-data/fonts/ttf/STIXNonUni.ttf') with score of 0.050000.
******

*** 2018-12-29 13:35:27,888 - matplotlib.font_manager - DEBUG ***
findfont: Matching :family=STIXNonUnicode:style=italic:variant=normal:weight=normal:stretch=normal:size=10.0 to STIXNonUnicode ('/home/ubuntu/.local/lib/python3.6/site-packages/matplotlib/mpl-data/fonts/ttf/STIXNonUniIta.ttf') with score of 0.050000.
******

*** 2018-12-29 13:35:27,889 - matplotlib.font_manager - DEBUG ***
findfont: Matching :family=STIXNonUnicode:style=normal:variant=normal:weight=bold:stretch=normal:size=10.0 to STIXNonUnicode ('/home/ubuntu/.local/lib/python3.6/site-packages/matplotlib/mpl-data/fonts/ttf/STIXNonUniBol.ttf') with score of 0.000000.
******

*** 2018-12-29 13:35:27,894 - matplotlib.font_manager - DEBUG ***
findfont: Matching :family=STIXSizeOneSym:style=normal:variant=normal:weight=normal:stretch=normal:size=10.0 to STIXSizeOneSym ('/home/ubuntu/.local/lib/python3.6/site-packages/matplotlib/mpl-data/fonts/ttf/STIXSizOneSymReg.ttf') with score of 0.050000.
******

*** 2018-12-29 13:35:27,915 - matplotlib.font_manager - DEBUG ***
findfont: Matching :family=STIXSizeTwoSym:style=normal:variant=normal:weight=normal:stretch=normal:size=10.0 to STIXSizeTwoSym ('/home/ubuntu/.local/lib/python3.6/site-packages/matplotlib/mpl-data/fonts/ttf/STIXSizTwoSymReg.ttf') with score of 0.050000.
******

*** 2018-12-29 13:35:27,920 - matplotlib.font_manager - DEBUG ***
findfont: Matching :family=STIXSizeThreeSym:style=normal:variant=normal:weight=normal:stretch=normal:size=10.0 to STIXSizeThreeSym ('/home/ubuntu/.local/lib/python3.6/site-packages/matplotlib/mpl-data/fonts/ttf/STIXSizThreeSymReg.ttf') with score of 0.050000.
******

*** 2018-12-29 13:35:27,925 - matplotlib.font_manager - DEBUG ***
findfont: Matching :family=STIXSizeFourSym:style=normal:variant=normal:weight=normal:stretch=normal:size=10.0 to STIXSizeFourSym ('/home/ubuntu/.local/lib/python3.6/site-packages/matplotlib/mpl-data/fonts/ttf/STIXSizFourSymReg.ttf') with score of 0.050000.
******

*** 2018-12-29 13:35:27,931 - matplotlib.font_manager - DEBUG ***
findfont: Matching :family=STIXSizeFiveSym:style=normal:variant=normal:weight=normal:stretch=normal:size=10.0 to STIXSizeFiveSym ('/home/ubuntu/.local/lib/python3.6/site-packages/matplotlib/mpl-data/fonts/ttf/STIXSizFiveSymReg.ttf') with score of 0.050000.
******

*** 2018-12-29 13:35:27,961 - matplotlib.font_manager - DEBUG ***
findfont: Matching :family=cmsy10:style=normal:variant=normal:weight=normal:stretch=normal:size=10.0 to cmsy10 ('/home/ubuntu/.local/lib/python3.6/site-packages/matplotlib/mpl-data/fonts/ttf/cmsy10.ttf') with score of 0.050000.
******

*** 2018-12-29 13:35:27,975 - matplotlib.font_manager - DEBUG ***
findfont: Matching :family=cmr10:style=normal:variant=normal:weight=normal:stretch=normal:size=10.0 to cmr10 ('/home/ubuntu/.local/lib/python3.6/site-packages/matplotlib/mpl-data/fonts/ttf/cmr10.ttf') with score of 0.050000.
******

*** 2018-12-29 13:35:27,981 - matplotlib.font_manager - DEBUG ***
findfont: Matching :family=cmtt10:style=normal:variant=normal:weight=normal:stretch=normal:size=10.0 to cmtt10 ('/home/ubuntu/.local/lib/python3.6/site-packages/matplotlib/mpl-data/fonts/ttf/cmtt10.ttf') with score of 0.050000.
******

*** 2018-12-29 13:35:27,994 - matplotlib.font_manager - DEBUG ***
findfont: Matching :family=cmmi10:style=normal:variant=normal:weight=normal:stretch=normal:size=10.0 to cmmi10 ('/home/ubuntu/.local/lib/python3.6/site-packages/matplotlib/mpl-data/fonts/ttf/cmmi10.ttf') with score of 0.050000.
******

*** 2018-12-29 13:35:27,999 - matplotlib.font_manager - DEBUG ***
findfont: Matching :family=cmb10:style=normal:variant=normal:weight=normal:stretch=normal:size=10.0 to cmb10 ('/home/ubuntu/.local/lib/python3.6/site-packages/matplotlib/mpl-data/fonts/ttf/cmb10.ttf') with score of 0.050000.
******

*** 2018-12-29 13:35:28,020 - matplotlib.font_manager - DEBUG ***
findfont: Matching :family=cmss10:style=normal:variant=normal:weight=normal:stretch=normal:size=10.0 to cmss10 ('/home/ubuntu/.local/lib/python3.6/site-packages/matplotlib/mpl-data/fonts/ttf/cmss10.ttf') with score of 0.050000.
******

*** 2018-12-29 13:35:28,026 - matplotlib.font_manager - DEBUG ***
findfont: Matching :family=cmex10:style=normal:variant=normal:weight=normal:stretch=normal:size=10.0 to cmex10 ('/home/ubuntu/.local/lib/python3.6/site-packages/matplotlib/mpl-data/fonts/ttf/cmex10.ttf') with score of 0.050000.
******

*** 2018-12-29 13:35:28,031 - matplotlib.font_manager - DEBUG ***
findfont: Matching :family=DejaVu Sans:style=normal:variant=normal:weight=normal:stretch=normal:size=10.0 to DejaVu Sans ('/home/ubuntu/.local/lib/python3.6/site-packages/matplotlib/mpl-data/fonts/ttf/DejaVuSans.ttf') with score of 0.050000.
******

*** 2018-12-29 13:35:28,036 - matplotlib.font_manager - DEBUG ***
findfont: Matching :family=DejaVu Sans:style=italic:variant=normal:weight=normal:stretch=normal:size=10.0 to DejaVu Sans ('/home/ubuntu/.local/lib/python3.6/site-packages/matplotlib/mpl-data/fonts/ttf/DejaVuSans-Oblique.ttf') with score of 0.150000.
******

*** 2018-12-29 13:35:28,037 - matplotlib.font_manager - DEBUG ***
findfont: Matching :family=DejaVu Sans:style=normal:variant=normal:weight=bold:stretch=normal:size=10.0 to DejaVu Sans ('/home/ubuntu/.local/lib/python3.6/site-packages/matplotlib/mpl-data/fonts/ttf/DejaVuSans-Bold.ttf') with score of 0.000000.
******

*** 2018-12-29 13:35:28,043 - matplotlib.font_manager - DEBUG ***
findfont: Matching :family=DejaVu Sans Mono:style=normal:variant=normal:weight=normal:stretch=normal:size=10.0 to DejaVu Sans Mono ('/home/ubuntu/.local/lib/python3.6/site-packages/matplotlib/mpl-data/fonts/ttf/DejaVuSansMono.ttf') with score of 0.050000.
******

*** 2018-12-29 13:35:28,048 - matplotlib.font_manager - DEBUG ***
findfont: Matching :family=DejaVu Sans Display:style=normal:variant=normal:weight=normal:stretch=normal:size=10.0 to DejaVu Sans Display ('/home/ubuntu/.local/lib/python3.6/site-packages/matplotlib/mpl-data/fonts/ttf/DejaVuSansDisplay.ttf') with score of 0.050000.
******

*** 2018-12-29 13:35:28,301 - matplotlib.ticker - DEBUG ***
vmin 0.009471074402518941 vmax 0.042468701550101975
******

*** 2018-12-29 13:35:28,301 - matplotlib.ticker - DEBUG ***
ticklocs array([1.e-04, 1.e-03, 1.e-02, 1.e-01, 1.e+00])
******

*** 2018-12-29 13:35:28,303 - matplotlib.ticker - DEBUG ***
vmin 0.009471074402518941 vmax 0.042468701550101975
******

*** 2018-12-29 13:35:28,303 - matplotlib.ticker - DEBUG ***
ticklocs [0.0002, 0.00030000000000000003, 0.0004, 0.0005, 0.0006000000000000001, 0.0007, 0.0008, 0.0009000000000000001, 0.002, 0.003, 0.004, 0.005, 0.006, 0.007, 0.008, 0.009000000000000001, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.2, 0.30000000000000004, 0.4, 0.5, 0.6000000000000001, 0.7000000000000001, 0.8, 0.9, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0]
******

*** 2018-12-29 13:35:28,399 - matplotlib.axes._base - DEBUG ***
update_title_pos
******

*** 2018-12-29 13:35:28,401 - matplotlib.ticker - DEBUG ***
vmin 0.009471074402518941 vmax 0.042468701550101975
******

*** 2018-12-29 13:35:28,402 - matplotlib.ticker - DEBUG ***
ticklocs array([1.e-04, 1.e-03, 1.e-02, 1.e-01, 1.e+00])
******

*** 2018-12-29 13:35:28,403 - matplotlib.ticker - DEBUG ***
vmin 0.009471074402518941 vmax 0.042468701550101975
******

*** 2018-12-29 13:35:28,403 - matplotlib.ticker - DEBUG ***
ticklocs [0.0002, 0.00030000000000000003, 0.0004, 0.0005, 0.0006000000000000001, 0.0007, 0.0008, 0.0009000000000000001, 0.002, 0.003, 0.004, 0.005, 0.006, 0.007, 0.008, 0.009000000000000001, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.2, 0.30000000000000004, 0.4, 0.5, 0.6000000000000001, 0.7000000000000001, 0.8, 0.9, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0]
******

*** 2018-12-29 13:35:28,412 - matplotlib.ticker - DEBUG ***
vmin 0.009471074402518941 vmax 0.042468701550101975
******

*** 2018-12-29 13:35:28,412 - matplotlib.ticker - DEBUG ***
ticklocs array([1.e-04, 1.e-03, 1.e-02, 1.e-01, 1.e+00])
******

*** 2018-12-29 13:35:28,414 - matplotlib.ticker - DEBUG ***
vmin 0.009471074402518941 vmax 0.042468701550101975
******

*** 2018-12-29 13:35:28,414 - matplotlib.ticker - DEBUG ***
ticklocs [0.0002, 0.00030000000000000003, 0.0004, 0.0005, 0.0006000000000000001, 0.0007, 0.0008, 0.0009000000000000001, 0.002, 0.003, 0.004, 0.005, 0.006, 0.007, 0.008, 0.009000000000000001, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.2, 0.30000000000000004, 0.4, 0.5, 0.6000000000000001, 0.7000000000000001, 0.8, 0.9, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0]
******

*** 2018-12-29 13:35:28,437 - code.resnet_fastai - INFO ***
Start model fitting: Stage 1
******

*** 2018-12-29 13:35:28,437 - code.resnet_fastai - DEBUG ***
LR: 0.012137125889205614
******

1         0.173901                
LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.
epoch     train_loss  valid_loss  fbeta   
1         0.111261    0.186789    0.623191  
2         0.103566    0.326356    0.580539  
*** 2018-12-29 14:54:30,697 - code.resnet_fastai - INFO ***
Complete model fitting: Stage 1
******

*** 2018-12-29 14:54:30,818 - code.resnet_fastai - INFO ***
Stage 1 model saved.
******

*** 2018-12-29 14:54:30,820 - code.resnet_fastai - DEBUG ***
Unfreezing model
******

*** 2018-12-29 14:54:30,820 - code.resnet_fastai - DEBUG ***
Start finding LR
******

3         0.096376    0.220208    0.682912  
epoch     train_loss  valid_loss  fbeta   
*** 2018-12-29 15:03:18,892 - code.resnet_fastai - DEBUG ***
[(1e-05, tensor(0.1144)), (1.0013872557113347e-05, tensor(0.1035)), (1.0027764359010778e-05, tensor(0.1012)), (1.0041675432389732e-05, tensor(0.0986)), (1.0055605803984682e-05, tensor(0.0978)), (1.006955550056719e-05, tensor(0.0950)), (1.0083524548945951e-05, tensor(0.0939)), (1.0097512975966858e-05, tensor(0.0928)), (1.0111520808513043e-05, tensor(0.0928)), (1.0125548073504929e-05, tensor(0.0920)), (1.0139594797900292e-05, tensor(0.0949)), (1.0153661008694298e-05, tensor(0.0950)), (1.0167746732919564e-05, tensor(0.0954)), (1.0181851997646207e-05, tensor(0.0947)), (1.0195976829981906e-05, tensor(0.0948)), (1.0210121257071934e-05, tensor(0.0944)), (1.0224285306099223e-05, tensor(0.0960)), (1.0238469004284424e-05, tensor(0.0952)), (1.025267237888594e-05, tensor(0.0955)), (1.026689545719999e-05, tensor(0.0948)), (1.0281138266560667e-05, tensor(0.0940)), (1.0295400834339972e-05, tensor(0.0926)), (1.0309683187947887e-05, tensor(0.0925)), (1.032398535483242e-05, tensor(0.0916)), (1.0338307362479645e-05, tensor(0.0926)), (1.0352649238413776e-05, tensor(0.0925)), (1.0367011010197207e-05, tensor(0.0937)), (1.0381392705430573e-05, tensor(0.0952)), (1.0395794351752788e-05, tensor(0.0939)), (1.0410215976841116e-05, tensor(0.0929)), (1.0424657608411216e-05, tensor(0.0931)), (1.0439119274217191e-05, tensor(0.0931)), (1.045360100205165e-05, tensor(0.0926)), (1.0468102819745758e-05, tensor(0.0925)), (1.0482624755169288e-05, tensor(0.0922)), (1.0497166836230674e-05, tensor(0.0923)), (1.0511729090877064e-05, tensor(0.0919)), (1.0526311547094386e-05, tensor(0.0921)), (1.054091423290738e-05, tensor(0.0927)), (1.0555537176379668e-05, tensor(0.0937)), (1.0570180405613803e-05, tensor(0.0935)), (1.0584843948751328e-05, tensor(0.0940)), (1.0599527833972819e-05, tensor(0.0945)), (1.0614232089497947e-05, tensor(0.0947)), (1.0628956743585532e-05, tensor(0.0949)), (1.06437018245336e-05, tensor(0.0945)), (1.0658467360679425e-05, tensor(0.0962)), (1.0673253380399601e-05, tensor(0.0971)), (1.068805991211008e-05, tensor(0.0974)), (1.0702886984266241e-05, tensor(0.0968)), (1.0717734625362933e-05, tensor(0.0965)), (1.0732602863934536e-05, tensor(0.0963)), (1.0747491728555012e-05, tensor(0.0965)), (1.0762401247837973e-05, tensor(0.0964)), (1.077733145043671e-05, tensor(0.0961)), (1.0792282365044274e-05, tensor(0.0960)), (1.0807254020393516e-05, tensor(0.0967)), (1.082224644525715e-05, tensor(0.0965)), (1.0837259668447802e-05, tensor(0.0967)), (1.0852293718818072e-05, tensor(0.0970)), (1.086734862526058e-05, tensor(0.0967)), (1.0882424416708038e-05, tensor(0.0966)), (1.0897521122133284e-05, tensor(0.0967)), (1.091263877054935e-05, tensor(0.0970)), (1.0927777391009527e-05, tensor(0.0972)), (1.0942937012607395e-05, tensor(0.0975)), (1.095811766447691e-05, tensor(0.0975)), (1.097331937579243e-05, tensor(0.0968)), (1.0988542175768798e-05, tensor(0.0973)), (1.1003786093661372e-05, tensor(0.0977)), (1.1019051158766108e-05, tensor(0.0974)), (1.1034337400419593e-05, tensor(0.0980)), (1.1049644847999117e-05, tensor(0.0980)), (1.1064973530922722e-05, tensor(0.0976)), (1.108032347864926e-05, tensor(0.0973)), (1.1095694720678452e-05, tensor(0.0975)), (1.1111087286550937e-05, tensor(0.0979)), (1.1126501205848341e-05, tensor(0.0980)), (1.1141936508193325e-05, tensor(0.0976)), (1.1157393223249642e-05, tensor(0.0973)), (1.11728713807222e-05, tensor(0.0971)), (1.1188371010357112e-05, tensor(0.0971)), (1.120389214194176e-05, tensor(0.0973)), (1.1219434805304846e-05, tensor(0.0968)), (1.1234999030316452e-05, tensor(0.0971)), (1.1250584846888094e-05, tensor(0.0963)), (1.1266192284972794e-05, tensor(0.0964)), (1.1281821374565117e-05, tensor(0.0962)), (1.1297472145701236e-05, tensor(0.0960)), (1.1313144628459003e-05, tensor(0.0961)), (1.1328838852957986e-05, tensor(0.0963)), (1.134455484935954e-05, tensor(0.0955)), (1.1360292647866863e-05, tensor(0.0960)), (1.137605227872505e-05, tensor(0.0963)), (1.1391833772221151e-05, tensor(0.0964)), (1.1407637158684237e-05, tensor(0.0964)), (1.1423462468485454e-05, tensor(0.0971)), (1.1439309732038076e-05, tensor(0.0965)), (1.145517897979757e-05, tensor(0.0965)), (1.1471070242261656e-05, tensor(0.0962)), (1.1486983549970351e-05, tensor(0.0966)), (1.1502918933506053e-05, tensor(0.0973)), (1.1518876423493577e-05, tensor(0.0976)), (1.1534856050600225e-05, tensor(0.0978)), (1.1550857845535844e-05, tensor(0.0978)), (1.1566881839052874e-05, tensor(0.0977)), (1.1582928061946432e-05, tensor(0.0983)), (1.1598996545054346e-05, tensor(0.0981)), (1.161508731925722e-05, tensor(0.0981)), (1.163120041547851e-05, tensor(0.0981)), (1.1647335864684558e-05, tensor(0.0981)), (1.1663493697884674e-05, tensor(0.0988)), (1.1679673946131181e-05, tensor(0.0984)), (1.1695876640519474e-05, tensor(0.0986)), (1.17121018121881e-05, tensor(0.0985)), (1.172834949231879e-05, tensor(0.0984)), (1.1744619712136536e-05, tensor(0.0983)), (1.176091250290965e-05, tensor(0.0987)), (1.1777227895949817e-05, tensor(0.0988)), (1.179356592261216e-05, tensor(0.0987)), (1.1809926614295303e-05, tensor(0.0987)), (1.1826310002441428e-05, tensor(0.0988)), (1.1842716118536328e-05, tensor(0.0988)), (1.185914499410948e-05, tensor(0.0988)), (1.1875596660734103e-05, tensor(0.0990)), (1.189207115002721e-05, tensor(0.0988)), (1.1908568493649684e-05, tensor(0.0990)), (1.1925088723306317e-05, tensor(0.0990)), (1.1941631870745896e-05, tensor(0.0991)), (1.1958197967761243e-05, tensor(0.0985)), (1.1974787046189286e-05, tensor(0.0985)), (1.1991399137911129e-05, tensor(0.0985)), (1.2008034274852087e-05, tensor(0.0985)), (1.2024692488981777e-05, tensor(0.0982)), (1.2041373812314158e-05, tensor(0.0983)), (1.2058078276907605e-05, tensor(0.0982)), (1.2074805914864964e-05, tensor(0.0982)), (1.2091556758333616e-05, tensor(0.0983)), (1.210833083950554e-05, tensor(0.0984)), (1.2125128190617372e-05, tensor(0.0979)), (1.214194884395047e-05, tensor(0.0982)), (1.2158792831830972e-05, tensor(0.0978)), (1.2175660186629863e-05, tensor(0.0981)), (1.2192550940763032e-05, tensor(0.0979)), (1.2209465126691345e-05, tensor(0.0979)), (1.2226402776920685e-05, tensor(0.0981)), (1.2243363924002046e-05, tensor(0.0979)), (1.2260348600531563e-05, tensor(0.0981)), (1.2277356839150604e-05, tensor(0.0983)), (1.2294388672545808e-05, tensor(0.0980)), (1.2311444133449164e-05, tensor(0.0977)), (1.2328523254638067e-05, tensor(0.0977)), (1.2345626068935384e-05, tensor(0.0978)), (1.2362752609209516e-05, tensor(0.0977)), (1.2379902908374457e-05, tensor(0.0981)), (1.2397076999389868e-05, tensor(0.0984)), (1.2414274915261124e-05, tensor(0.0982)), (1.2431496689039397e-05, tensor(0.0980)), (1.2448742353821703e-05, tensor(0.0977)), (1.2466011942750974e-05, tensor(0.0978)), (1.248330548901612e-05, tensor(0.0975)), (1.2500623025852092e-05, tensor(0.0975)), (1.2517964586539946e-05, tensor(0.0976)), (1.2535330204406906e-05, tensor(0.0981)), (1.2552719912826434e-05, tensor(0.0985)), (1.2570133745218285e-05, tensor(0.0984)), (1.2587571735048578e-05, tensor(0.0977)), (1.2605033915829857e-05, tensor(0.0979)), (1.2622520321121157e-05, tensor(0.0977)), (1.2640030984528068e-05, tensor(0.0972)), (1.26575659397028e-05, tensor(0.0967)), (1.2675125220344247e-05, tensor(0.0966)), (1.2692708860198048e-05, tensor(0.0963)), (1.2710316893056665e-05, tensor(0.0967)), (1.272794935275943e-05, tensor(0.0965)), (1.2745606273192624e-05, tensor(0.0973)), (1.276328768828953e-05, tensor(0.0971)), (1.2780993632030516e-05, tensor(0.0976)), (1.2798724138443079e-05, tensor(0.0978)), (1.281647924160193e-05, tensor(0.0975)), (1.2834258975629044e-05, tensor(0.0972)), (1.285206337469373e-05, tensor(0.0972)), (1.2869892473012708e-05, tensor(0.0972)), (1.2887746304850157e-05, tensor(0.0969)), (1.290562490451779e-05, tensor(0.0969)), (1.2923528306374925e-05, tensor(0.0973)), (1.2941456544828535e-05, tensor(0.0973)), (1.2959409654333338e-05, tensor(0.0972)), (1.2977387669391834e-05, tensor(0.0979)), (1.29953906245544e-05, tensor(0.0979)), (1.3013418554419336e-05, tensor(0.0978)), (1.303147149363294e-05, tensor(0.0978)), (1.3049549476889578e-05, tensor(0.0984)), (1.3067652538931734e-05, tensor(0.0978)), (1.3085780714550102e-05, tensor(0.0982)), (1.3103934038583635e-05, tensor(0.0977)), (1.312211254591961e-05, tensor(0.0978)), (1.3140316271493711e-05, tensor(0.0975)), (1.3158545250290082e-05, tensor(0.0975)), (1.3176799517341401e-05, tensor(0.0979)), (1.3195079107728943e-05, tensor(0.0981)), (1.3213384056582652e-05, tensor(0.0979)), (1.3231714399081203e-05, tensor(0.0979)), (1.3250070170452075e-05, tensor(0.0984)), (1.3268451405971619e-05, tensor(0.0984)), (1.3286858140965117e-05, tensor(0.0983)), (1.3305290410806862e-05, tensor(0.0979)), (1.332374825092022e-05, tensor(0.0981)), (1.3342231696777692e-05, tensor(0.0978)), (1.3360740783900993e-05, tensor(0.0980)), (1.3379275547861121e-05, tensor(0.0980)), (1.339783602427841e-05, tensor(0.0987)), (1.3416422248822614e-05, tensor(0.0988)), (1.3435034257212968e-05, tensor(0.0990)), (1.3453672085218263e-05, tensor(0.0986)), (1.3472335768656903e-05, tensor(0.0984)), (1.3491025343396988e-05, tensor(0.0983)), (1.3509740845356376e-05, tensor(0.0980)), (1.3528482310502746e-05, tensor(0.0981)), (1.3547249774853679e-05, tensor(0.0980)), (1.356604327447672e-05, tensor(0.0975)), (1.3584862845489449e-05, tensor(0.0981)), (1.360370852405955e-05, tensor(0.0981)), (1.3622580346404884e-05, tensor(0.0979)), (1.3641478348793546e-05, tensor(0.0978)), (1.3660402567543956e-05, tensor(0.0980)), (1.367935303902491e-05, tensor(0.0977)), (1.3698329799655659e-05, tensor(0.0976)), (1.3717332885905977e-05, tensor(0.0977)), (1.3736362334296225e-05, tensor(0.0983)), (1.375541818139744e-05, tensor(0.0985)), (1.3774500463831378e-05, tensor(0.0983)), (1.3793609218270606e-05, tensor(0.0982)), (1.381274448143857e-05, tensor(0.0983)), (1.3831906290109649e-05, tensor(0.0986)), (1.3851094681109248e-05, tensor(0.0987)), (1.3870309691313852e-05, tensor(0.0986)), (1.3889551357651106e-05, tensor(0.0984)), (1.3908819717099882e-05, tensor(0.0987)), (1.3928114806690352e-05, tensor(0.0992)), (1.3947436663504055e-05, tensor(0.0995)), (1.3966785324673977e-05, tensor(0.0995)), (1.3986160827384616e-05, tensor(0.0995)), (1.4005563208872049e-05, tensor(0.0992)), (1.4024992506424011e-05, tensor(0.0984)), (1.4044448757379973e-05, tensor(0.0983)), (1.4063931999131193e-05, tensor(0.0980)), (1.4083442269120809e-05, tensor(0.0983)), (1.4102979604843896e-05, tensor(0.0983)), (1.4122544043847551e-05, tensor(0.0982)), (1.4142135623730953e-05, tensor(0.0977)), (1.4161754382145441e-05, tensor(0.0979)), (1.4181400356794588e-05, tensor(0.0981)), (1.4201073585434273e-05, tensor(0.0980)), (1.4220774105872747e-05, tensor(0.0978)), (1.4240501955970718e-05, tensor(0.0977)), (1.4260257173641409e-05, tensor(0.0976)), (1.4280039796850642e-05, tensor(0.0977)), (1.4299849863616908e-05, tensor(0.0975)), (1.4319687412011437e-05, tensor(0.0970)), (1.4339552480158274e-05, tensor(0.0969)), (1.4359445106234355e-05, tensor(0.0967)), (1.4379365328469574e-05, tensor(0.0969)), (1.4399313185146859e-05, tensor(0.0975)), (1.441928871460225e-05, tensor(0.0974)), (1.4439291955224963e-05, tensor(0.0976)), (1.4459322945457475e-05, tensor(0.0980)), (1.4479381723795591e-05, tensor(0.0986)), (1.449946832878852e-05, tensor(0.0988)), (1.4519582799038946e-05, tensor(0.0986)), (1.4539725173203107e-05, tensor(0.0986)), (1.4559895489990868e-05, tensor(0.0990)), (1.4580093788165793e-05, tensor(0.0993)), (1.4600320106545218e-05, tensor(0.0996)), (1.4620574484000334e-05, tensor(0.0999)), (1.4640856959456256e-05, tensor(0.1001)), (1.4661167571892095e-05, tensor(0.1000)), (1.4681506360341034e-05, tensor(0.0997)), (1.4701873363890411e-05, tensor(0.0993)), (1.4722268621681785e-05, tensor(0.0994)), (1.4742692172911012e-05, tensor(0.0993)), (1.4763144056828331e-05, tensor(0.0993)), (1.478362431273842e-05, tensor(0.0988)), (1.480413298000049e-05, tensor(0.0985)), (1.4824670098028351e-05, tensor(0.0988)), (1.4845235706290494e-05, tensor(0.0983)), (1.4865829844310152e-05, tensor(0.0979)), (1.4886452551665399e-05, tensor(0.0980)), (1.4907103867989206e-05, tensor(0.0981)), (1.4927783832969532e-05, tensor(0.0984)), (1.4948492486349383e-05, tensor(0.0984)), (1.4969229867926916e-05, tensor(0.0983)), (1.4989996017555476e-05, tensor(0.0985)), (1.5010790975143712e-05, tensor(0.0985)), (1.5031614780655627e-05, tensor(0.0984)), (1.5052467474110673e-05, tensor(0.0982)), (1.5073349095583812e-05, tensor(0.0987)), (1.5094259685205598e-05, tensor(0.0987)), (1.5115199283162267e-05, tensor(0.0988)), (1.5136167929695793e-05, tensor(0.0989)), (1.515716566510398e-05, tensor(0.0989)), (1.517819252974054e-05, tensor(0.0986)), (1.5199248564015158e-05, tensor(0.0987)), (1.5220333808393582e-05, tensor(0.0987)), (1.5241448303397695e-05, tensor(0.0989)), (1.5262592089605593e-05, tensor(0.0987)), (1.5283765207651667e-05, tensor(0.0982)), (1.5304967698226677e-05, tensor(0.0982)), (1.5326199602077832e-05, tensor(0.0978)), (1.534746096000887e-05, tensor(0.0982)), (1.5368751812880125e-05, tensor(0.0981)), (1.5390072201608624e-05, tensor(0.0983)), (1.541142216716816e-05, tensor(0.0981)), (1.543280175058935e-05, tensor(0.0980)), (1.545421099295975e-05, tensor(0.0976)), (1.54756499354239e-05, tensor(0.0980)), (1.5497118619183433e-05, tensor(0.0980)), (1.5518617085497124e-05, tensor(0.0979)), (1.554014537568099e-05, tensor(0.0974)), (1.5561703531108373e-05, tensor(0.0970)), (1.558329159321e-05, tensor(0.0968)), (1.560490960347407e-05, tensor(0.0968)), (1.562655760344635e-05, tensor(0.0967)), (1.564823563473023e-05, tensor(0.0970)), (1.566994373898682e-05, tensor(0.0970)), (1.5691681957935017e-05, tensor(0.0977)), (1.5713450333351606e-05, tensor(0.0979)), (1.573524890707132e-05, tensor(0.0973)), (1.5757077720986925e-05, tensor(0.0973)), (1.5778936817049306e-05, tensor(0.0975)), (1.5800826237267546e-05, tensor(0.0978)), (1.5822746023709e-05, tensor(0.0981)), (1.5844696218499386e-05, tensor(0.0984)), (1.5866676863822857e-05, tensor(0.0981)), (1.5888688001922097e-05, tensor(0.0981)), (1.5910729675098374e-05, tensor(0.0984)), (1.5932801925711656e-05, tensor(0.0978)), (1.595490479618066e-05, tensor(0.0981)), (1.5977038328982962e-05, tensor(0.0983)), (1.5999202566655053e-05, tensor(0.0978)), (1.6021397551792445e-05, tensor(0.0980)), (1.6043623327049727e-05, tensor(0.0981)), (1.6065879935140678e-05, tensor(0.0980)), (1.6088167418838317e-05, tensor(0.0980)), (1.6110485820975005e-05, tensor(0.0975)), (1.6132835184442527e-05, tensor(0.0978)), (1.6155215552192164e-05, tensor(0.0976)), (1.6177626967234783e-05, tensor(0.0976)), (1.6200069472640918e-05, tensor(0.0976)), (1.6222543111540856e-05, tensor(0.0976)), (1.624504792712471e-05, tensor(0.0977)), (1.626758396264252e-05, tensor(0.0977)), (1.6290151261404306e-05, tensor(0.0982)), (1.6312749866780197e-05, tensor(0.0986)), (1.6335379822200457e-05, tensor(0.0994)), (1.635804117115562e-05, tensor(0.0991)), (1.6380733957196556e-05, tensor(0.0984)), (1.6403458223934527e-05, tensor(0.0988)), (1.6426214015041317e-05, tensor(0.0989)), (1.6449001374249287e-05, tensor(0.0987)), (1.6471820345351462e-05, tensor(0.0989)), (1.649467097220163e-05, tensor(0.0986)), (1.65175532987144e-05, tensor(0.0986)), (1.6540467368865313e-05, tensor(0.0985)), (1.6563413226690913e-05, tensor(0.0983)), (1.6586390916288835e-05, tensor(0.0986)), (1.6609400481817884e-05, tensor(0.0986)), (1.6632441967498128e-05, tensor(0.0983)), (1.665551541761098e-05, tensor(0.0987)), (1.667862087649928e-05, tensor(0.0984)), (1.6701758388567388e-05, tensor(0.0984)), (1.6724927998281258e-05, tensor(0.0985)), (1.6748129750168532e-05, tensor(0.0982)), (1.6771363688818626e-05, tensor(0.0977)), (1.679462985888281e-05, tensor(0.0981)), (1.681792830507429e-05, tensor(0.0976)), (1.684125907216832e-05, tensor(0.0974)), (1.686462220500225e-05, tensor(0.0970)), (1.688801774847564e-05, tensor(0.0974)), (1.6911445747550332e-05, tensor(0.0978)), (1.6934906247250543e-05, tensor(0.0977)), (1.6958399292662955e-05, tensor(0.0978)), (1.6981924928936795e-05, tensor(0.0978)), (1.7005483201283917e-05, tensor(0.0979)), (1.7029074154978903e-05, tensor(0.0977)), (1.7052697835359134e-05, tensor(0.0975)), (1.7076354287824898e-05, tensor(0.0978)), (1.7100043557839456e-05, tensor(0.0972)), (1.712376569092914e-05, tensor(0.0969)), (1.7147520732683434e-05, tensor(0.0967)), (1.7171308728755077e-05, tensor(0.0967)), (1.719512972486013e-05, tensor(0.0967)), (1.7218983766778082e-05, tensor(0.0972)), (1.724287090035192e-05, tensor(0.0973)), (1.7266791171488238e-05, tensor(0.0972)), (1.7290744626157306e-05, tensor(0.0974)), (1.731473131039317e-05, tensor(0.0969)), (1.7338751270293733e-05, tensor(0.0968)), (1.736280455202086e-05, tensor(0.0967)), (1.7386891201800436e-05, tensor(0.0966)), (1.7411011265922484e-05, tensor(0.0961)), (1.7435164790741245e-05, tensor(0.0960)), (1.7459351822675258e-05, tensor(0.0954)), (1.7483572408207464e-05, tensor(0.0955)), (1.7507826593885282e-05, tensor(0.0953)), (1.7532114426320704e-05, tensor(0.0956)), (1.7556435952190386e-05, tensor(0.0957)), (1.7580791218235743e-05, tensor(0.0957)), (1.7605180271263017e-05, tensor(0.0956)), (1.7629603158143402e-05, tensor(0.0959)), (1.76540599258131e-05, tensor(0.0961)), (1.7678550621273426e-05, tensor(0.0966)), (1.7703075291590904e-05, tensor(0.0965)), (1.7727633983897347e-05, tensor(0.0966)), (1.7752226745389956e-05, tensor(0.0964)), (1.7776853623331404e-05, tensor(0.0967)), (1.7801514665049928e-05, tensor(0.0966)), (1.7826209917939424e-05, tensor(0.0971)), (1.7850939429459533e-05, tensor(0.0967)), (1.7875703247135737e-05, tensor(0.0970)), (1.790050141855945e-05, tensor(0.0969)), (1.79253339913881e-05, tensor(0.0968)), (1.795020101334523e-05, tensor(0.0972)), (1.7975102532220595e-05, tensor(0.0969)), (1.8000038595870244e-05, tensor(0.0966)), (1.8025009252216605e-05, tensor(0.0965)), (1.80500145492486e-05, tensor(0.0970)), (1.807505453502172e-05, tensor(0.0970)), (1.810012925765811e-05, tensor(0.0967)), (1.812523876534669e-05, tensor(0.0965)), (1.815038310634322e-05, tensor(0.0963)), (1.81755623289704e-05, tensor(0.0959)), (1.820077648161798e-05, tensor(0.0956)), (1.8226025612742834e-05, tensor(0.0960)), (1.825130977086904e-05, tensor(0.0965)), (1.827662900458801e-05, tensor(0.0971)), (1.8301983362558567e-05, tensor(0.0972)), (1.8327372893507026e-05, tensor(0.0969)), (1.8352797646227304e-05, tensor(0.0971)), (1.8378257669580997e-05, tensor(0.0972)), (1.8403753012497503e-05, tensor(0.0971)), (1.842928372397408e-05, tensor(0.0973)), (1.845484985307597e-05, tensor(0.0971)), (1.848045144893647e-05, tensor(0.0968)), (1.8506088560757047e-05, tensor(0.0963)), (1.8531761237807418e-05, tensor(0.0963)), (1.8557469529425657e-05, tensor(0.0965)), (1.8583213485018266e-05, tensor(0.0963)), (1.860899315406031e-05, tensor(0.0961)), (1.8634808586095464e-05, tensor(0.0959)), (1.866065983073615e-05, tensor(0.0955)), (1.868654693766361e-05, tensor(0.0959)), (1.8712469956628002e-05, tensor(0.0960)), (1.873842893744851e-05, tensor(0.0962)), (1.8764423930013423e-05, tensor(0.0970)), (1.8790454984280237e-05, tensor(0.0965)), (1.8816522150275756e-05, tensor(0.0963)), (1.8842625478096176e-05, tensor(0.0964)), (1.8868765017907204e-05, tensor(0.0967)), (1.8894940819944126e-05, tensor(0.0963)), (1.8921152934511918e-05, tensor(0.0964)), (1.8947401411985357e-05, tensor(0.0966)), (1.897368630280908e-05, tensor(0.0966)), (1.9000007657497725e-05, tensor(0.0966)), (1.9026365526635985e-05, tensor(0.0966)), (1.9052759960878752e-05, tensor(0.0966)), (1.9079191010951164e-05, tensor(0.0968)), (1.910565872764875e-05, tensor(0.0969)), (1.9132163161837488e-05, tensor(0.0968)), (1.915870436445393e-05, tensor(0.0967)), (1.918528238650529e-05, tensor(0.0962)), (1.9211897279069533e-05, tensor(0.0957)), (1.9238549093295495e-05, tensor(0.0956)), (1.926523788040296e-05, tensor(0.0965)), (1.9291963691682768e-05, tensor(0.0963)), (1.9318726578496913e-05, tensor(0.0957)), (1.934552659227864e-05, tensor(0.0954)), (1.9372363784532554e-05, tensor(0.0955)), (1.9399238206834698e-05, tensor(0.0951)), (1.942614991083267e-05, tensor(0.0956)), (1.9453098948245713e-05, tensor(0.0954)), (1.948008537086482e-05, tensor(0.0953)), (1.9507109230552837e-05, tensor(0.0955)), (1.953417057924455e-05, tensor(0.0954)), (1.9561269468946788e-05, tensor(0.0951)), (1.958840595173854e-05, tensor(0.0952)), (1.961558007977103e-05, tensor(0.0951)), (1.9642791905267828e-05, tensor(0.0953)), (1.9670041480524968e-05, tensor(0.0950)), (1.9697328857911014e-05, tensor(0.0947)), (1.9724654089867186e-05, tensor(0.0946)), (1.9752017228907452e-05, tensor(0.0942)), (1.977941832761863e-05, tensor(0.0949)), (1.9806857438660495e-05, tensor(0.0949)), (1.9834334614765865e-05, tensor(0.0950)), (1.986184990874072e-05, tensor(0.0953)), (1.988940337346429e-05, tensor(0.0955)), (1.9916995061889162e-05, tensor(0.0954)), (1.9944625027041392e-05, tensor(0.0952)), (1.997229332202058e-05, tensor(0.0953)), (2e-05, tensor(0.0951)), (2.0027745114226694e-05, tensor(0.0958)), (2.0055528718021557e-05, tensor(0.0959)), (2.0083350864779465e-05, tensor(0.0955)), (2.0111211607969365e-05, tensor(0.0952)), (2.013911100113438e-05, tensor(0.0956)), (2.0167049097891902e-05, tensor(0.0956)), (2.0195025951933716e-05, tensor(0.0958)), (2.0223041617026085e-05, tensor(0.0958)), (2.0251096147009858e-05, tensor(0.0956)), (2.0279189595800584e-05, tensor(0.0957)), (2.0307322017388595e-05, tensor(0.0953)), (2.0335493465839128e-05, tensor(0.0952)), (2.0363703995292415e-05, tensor(0.0953)), (2.0391953659963812e-05, tensor(0.0955)), (2.042024251414387e-05, tensor(0.0950)), (2.0448570612198447e-05, tensor(0.0951)), (2.0476938008568847e-05, tensor(0.0950)), (2.050534475777188e-05, tensor(0.0952)), (2.053379091439998e-05, tensor(0.0950)), (2.0562276533121333e-05, tensor(0.0950)), (2.0590801668679944e-05, tensor(0.0950)), (2.0619366375895778e-05, tensor(0.0951)), (2.064797070966484e-05, tensor(0.0951)), (2.067661472495929e-05, tensor(0.0947)), (2.0705298476827553e-05, tensor(0.0947)), (2.073402202039442e-05, tensor(0.0949)), (2.0762785410861147e-05, tensor(0.0949)), (2.0791588703505576e-05, tensor(0.0945)), (2.082043195368223e-05, tensor(0.0945)), (2.084931521682243e-05, tensor(0.0945)), (2.0878238548434382e-05, tensor(0.0946)), (2.09072020041033e-05, tensor(0.0945)), (2.093620563949152e-05, tensor(0.0948)), (2.0965249510338575e-05, tensor(0.0947)), (2.0994333672461348e-05, tensor(0.0951)), (2.1023458181754136e-05, tensor(0.0950)), (2.1052623094188775e-05, tensor(0.0951)), (2.108182846581476e-05, tensor(0.0953)), (2.1111074352759337e-05, tensor(0.0953)), (2.114036081122761e-05, tensor(0.0950)), (2.1169687897502656e-05, tensor(0.0952)), (2.119905566794564e-05, tensor(0.0954)), (2.1228464178995893e-05, tensor(0.0955)), (2.1257913487171068e-05, tensor(0.0954)), (2.1287403649067202e-05, tensor(0.0954)), (2.131693472135885e-05, tensor(0.0953)), (2.1346506760799202e-05, tensor(0.0954)), (2.1376119824220163e-05, tensor(0.0954)), (2.1405773968532482e-05, tensor(0.0955)), (2.1435469250725866e-05, tensor(0.0957)), (2.1465205727869072e-05, tensor(0.0954)), (2.149498345711003e-05, tensor(0.0954)), (2.1524802495675945e-05, tensor(0.0956)), (2.155466290087342e-05, tensor(0.0956)), (2.1584564730088548e-05, tensor(0.0952)), (2.161450804078703e-05, tensor(0.0946)), (2.16444928905143e-05, tensor(0.0947)), (2.1674519336895607e-05, tensor(0.0947)), (2.1704587437636144e-05, tensor(0.0950)), (2.1734697250521167e-05, tensor(0.0950)), (2.176484883341608e-05, tensor(0.0955)), (2.1795042244266567e-05, tensor(0.0956)), (2.18252775410987e-05, tensor(0.0957)), (2.1855554782019047e-05, tensor(0.0955)), (2.188587402521479e-05, tensor(0.0960)), (2.1916235328953818e-05, tensor(0.0956)), (2.194663875158486e-05, tensor(0.0957)), (2.1977084351537595e-05, tensor(0.0954)), (2.2007572187322745e-05, tensor(0.0949)), (2.2038102317532215e-05, tensor(0.0950)), (2.2068674800839185e-05, tensor(0.0951)), (2.2099289695998235e-05, tensor(0.0948)), (2.2129947061845445e-05, tensor(0.0950)), (2.216064695729852e-05, tensor(0.0956)), (2.2191389441356897e-05, tensor(0.0956)), (2.2222174573101873e-05, tensor(0.0959)), (2.2253002411696682e-05, tensor(0.0958)), (2.2283873016386647e-05, tensor(0.0960)), (2.2314786446499283e-05, tensor(0.0960)), (2.2345742761444398e-05, tensor(0.0965)), (2.2376742020714225e-05, tensor(0.0963)), (2.240778428388352e-05, tensor(0.0965)), (2.2438869610609692e-05, tensor(0.0959)), (2.24699980606329e-05, tensor(0.0958)), (2.250116969377619e-05, tensor(0.0960)), (2.2532384569945588e-05, tensor(0.0961)), (2.2563642749130226e-05, tensor(0.0960)), (2.259494429140247e-05, tensor(0.0961)), (2.2626289256918006e-05, tensor(0.0962)), (2.2657677705915973e-05, tensor(0.0960)), (2.268910969871908e-05, tensor(0.0968)), (2.2720585295733727e-05, tensor(0.0967)), (2.2752104557450095e-05, tensor(0.0968)), (2.2783667544442302e-05, tensor(0.0969)), (2.2815274317368474e-05, tensor(0.0971)), (2.2846924936970908e-05, tensor(0.0969)), (2.2878619464076152e-05, tensor(0.0970)), (2.291035795959514e-05, tensor(0.0971)), (2.2942140484523305e-05, tensor(0.0976)), (2.29739670999407e-05, tensor(0.0976)), (2.3005837867012107e-05, tensor(0.0978)), (2.3037752846987154e-05, tensor(0.0980)), (2.306971210120045e-05, tensor(0.0982)), (2.310171569107168e-05, tensor(0.0979)), (2.313376367810575e-05, tensor(0.0977)), (2.3165856123892864e-05, tensor(0.0977)), (2.3197993090108688e-05, tensor(0.0976)), (2.323017463851444e-05, tensor(0.0972)), (2.326240083095702e-05, tensor(0.0972)), (2.3294671729369117e-05, tensor(0.0971)), (2.3326987395769348e-05, tensor(0.0968)), (2.3359347892262356e-05, tensor(0.0971)), (2.339175328103895e-05, tensor(0.0971)), (2.34242036243762e-05, tensor(0.0971)), (2.345669898463758e-05, tensor(0.0973)), (2.3489239424273073e-05, tensor(0.0972)), (2.35218250058193e-05, tensor(0.0976)), (2.3554455791899633e-05, tensor(0.0975)), (2.358713184522432e-05, tensor(0.0973)), (2.3619853228590606e-05, tensor(0.0975)), (2.3652620004882856e-05, tensor(0.0973)), (2.3685432237072656e-05, tensor(0.0973)), (2.371828998821896e-05, tensor(0.0974)), (2.3751193321468205e-05, tensor(0.0971)), (2.378414230005442e-05, tensor(0.0970)), (2.381713698729937e-05, tensor(0.0973)), (2.3850177446612634e-05, tensor(0.0977)), (2.3883263741491793e-05, tensor(0.0975)), (2.3916395935522486e-05, tensor(0.0981)), (2.3949574092378573e-05, tensor(0.0982)), (2.3982798275822257e-05, tensor(0.0979)), (2.4016068549704174e-05, tensor(0.0981)), (2.4049384977963553e-05, tensor(0.0980)), (2.4082747624628317e-05, tensor(0.0979)), (2.411615655381521e-05, tensor(0.0982)), (2.4149611829729928e-05, tensor(0.0978)), (2.4183113516667232e-05, tensor(0.0971)), (2.421666167901108e-05, tensor(0.0967)), (2.4250256381234744e-05, tensor(0.0969)), (2.428389768790094e-05, tensor(0.0964)), (2.4317585663661944e-05, tensor(0.0962)), (2.4351320373259725e-05, tensor(0.0966)), (2.4385101881526064e-05, tensor(0.0970)), (2.441893025338269e-05, tensor(0.0968)), (2.4452805553841374e-05, tensor(0.0969)), (2.4486727848004092e-05, tensor(0.0967)), (2.4520697201063127e-05, tensor(0.0969)), (2.455471367830121e-05, tensor(0.0966)), (2.458877734509162e-05, tensor(0.0964)), (2.4622888266898328e-05, tensor(0.0965)), (2.4657046509276134e-05, tensor(0.0968)), (2.469125213787077e-05, tensor(0.0963)), (2.4725505218419032e-05, tensor(0.0968)), (2.4759805816748914e-05, tensor(0.0966)), (2.4794153998779735e-05, tensor(0.0963)), (2.4828549830522248e-05, tensor(0.0962)), (2.4862993378078794e-05, tensor(0.0961)), (2.4897484707643406e-05, tensor(0.0961)), (2.493202388550195e-05, tensor(0.0964)), (2.496661097803224e-05, tensor(0.0964)), (2.5001246051704185e-05, tensor(0.0962)), (2.503592917307989e-05, tensor(0.0961)), (2.5070660408813812e-05, tensor(0.0962)), (2.510543982565287e-05, tensor(0.0965)), (2.514026749043657e-05, tensor(0.0969)), (2.5175143470097156e-05, tensor(0.0977)), (2.5210067831659717e-05, tensor(0.0976)), (2.5245040642242318e-05, tensor(0.0976)), (2.5280061969056136e-05, tensor(0.0978)), (2.53151318794056e-05, tensor(0.0976)), (2.5350250440688495e-05, tensor(0.0972)), (2.5385417720396103e-05, tensor(0.0974)), (2.5420633786113334e-05, tensor(0.0979)), (2.545589870551886e-05, tensor(0.0984)), (2.5491212546385247e-05, tensor(0.0985)), (2.5526575376579063e-05, tensor(0.0988)), (2.5561987264061032e-05, tensor(0.0988)), (2.5597448276886164e-05, tensor(0.0986)), (2.563295848320386e-05, tensor(0.0992)), (2.5668517951258088e-05, tensor(0.0992)), (2.570412674938746e-05, tensor(0.0990)), (2.5739784946025417e-05, tensor(0.0997)), (2.5775492609700315e-05, tensor(0.0996)), (2.581124980903558e-05, tensor(0.0998)), (2.584705661274985e-05, tensor(0.0996)), (2.5882913089657073e-05, tensor(0.0999)), (2.5918819308666675e-05, tensor(0.0997)), (2.5954775338783667e-05, tensor(0.1003)), (2.59907812491088e-05, tensor(0.1001)), (2.602683710883867e-05, tensor(0.1002)), (2.606294298726588e-05, tensor(0.1000)), (2.6099098953779153e-05, tensor(0.0999)), (2.6135305077863467e-05, tensor(0.1000)), (2.6171561429100205e-05, tensor(0.1006)), (2.6207868077167266e-05, tensor(0.1003)), (2.624422509183922e-05, tensor(0.1002)), (2.628063254298742e-05, tensor(0.1002)), (2.6317090500580163e-05, tensor(0.1001)), (2.6353599034682803e-05, tensor(0.1005)), (2.6390158215457886e-05, tensor(0.1008)), (2.64267681131653e-05, tensor(0.1007)), (2.6463428798162404e-05, tensor(0.1003)), (2.6500140340904148e-05, tensor(0.1000)), (2.6536902811943234e-05, tensor(0.1000)), (2.657371628193023e-05, tensor(0.1002)), (2.6610580821613723e-05, tensor(0.1003)), (2.664749650184044e-05, tensor(0.1002)), (2.668446339355538e-05, tensor(0.0997)), (2.6721481567801986e-05, tensor(0.1001)), (2.6758551095722242e-05, tensor(0.0996)), (2.679567204855682e-05, tensor(0.0997)), (2.6832844497645228e-05, tensor(0.0992)), (2.6870068514425936e-05, tensor(0.0994)), (2.6907344170436523e-05, tensor(0.0991)), (2.6944671537313805e-05, tensor(0.0993)), (2.6982050686793975e-05, tensor(0.0997)), (2.701948169071275e-05, tensor(0.0994)), (2.705696462100549e-05, tensor(0.0991)), (2.7094499549707358e-05, tensor(0.0991)), (2.713208654895344e-05, tensor(0.0989)), (2.7169725690978897e-05, tensor(0.0994)), (2.72074170481191e-05, tensor(0.1001)), (2.724516069280976e-05, tensor(0.0998)), (2.728295669758709e-05, tensor(0.0994)), (2.7320805135087913e-05, tensor(0.0996)), (2.735870607804982e-05, tensor(0.0997)), (2.7396659599311317e-05, tensor(0.0995)), (2.7434665771811947e-05, tensor(0.0997)), (2.747272466859245e-05, tensor(0.0995)), (2.7510836362794877e-05, tensor(0.0994)), (2.7549000927662756e-05, tensor(0.0991)), (2.7587218436541213e-05, tensor(0.0995)), (2.762548896287714e-05, tensor(0.0994)), (2.7663812580219297e-05, tensor(0.0999)), (2.7702189362218495e-05, tensor(0.0997)), (2.7740619382627705e-05, tensor(0.0994)), (2.7779102715302212e-05, tensor(0.0992)), (2.7817639434199764e-05, tensor(0.0993)), (2.7856229613380703e-05, tensor(0.0990)), (2.789487332700811e-05, tensor(0.0989)), (2.7933570649347954e-05, tensor(0.0984)), (2.7972321654769232e-05, tensor(0.0984)), (2.8011126417744098e-05, tensor(0.0984)), (2.8049985012848022e-05, tensor(0.0979)), (2.8088897514759947e-05, tensor(0.0981)), (2.8127863998262387e-05, tensor(0.0979)), (2.8166884538241617e-05, tensor(0.0975)), (2.8205959209687792e-05, tensor(0.0976)), (2.8245088087695102e-05, tensor(0.0974)), (2.8284271247461906e-05, tensor(0.0975)), (2.8323508764290882e-05, tensor(0.0974)), (2.8362800713589177e-05, tensor(0.0974)), (2.8402147170868547e-05, tensor(0.0974)), (2.8441548211745494e-05, tensor(0.0974)), (2.8481003911941436e-05, tensor(0.0973)), (2.8520514347282818e-05, tensor(0.0971)), (2.8560079593701283e-05, tensor(0.0971)), (2.8599699727233815e-05, tensor(0.0971)), (2.8639374824022875e-05, tensor(0.0975)), (2.867910496031655e-05, tensor(0.0979)), (2.871889021246871e-05, tensor(0.0981)), (2.875873065693915e-05, tensor(0.0978)), (2.8798626370293718e-05, tensor(0.0981)), (2.88385774292045e-05, tensor(0.0981)), (2.8878583910449926e-05, tensor(0.0981)), (2.891864589091495e-05, tensor(0.0982)), (2.8958763447591183e-05, tensor(0.0981)), (2.899893665757704e-05, tensor(0.0980)), (2.903916559807789e-05, tensor(0.0978)), (2.9079450346406214e-05, tensor(0.0978)), (2.9119790979981736e-05, tensor(0.0977)), (2.9160187576331585e-05, tensor(0.0981)), (2.9200640213090436e-05, tensor(0.0974)), (2.924114896800067e-05, tensor(0.0973)), (2.9281713918912512e-05, tensor(0.0978)), (2.932233514378419e-05, tensor(0.0977)), (2.936301272068207e-05, tensor(0.0977)), (2.9403746727780822e-05, tensor(0.0974)), (2.944453724336357e-05, tensor(0.0977)), (2.9485384345822024e-05, tensor(0.0972)), (2.9526288113656662e-05, tensor(0.0975)), (2.956724862547684e-05, tensor(0.0976)), (2.9608265960000983e-05, tensor(0.0973)), (2.9649340196056706e-05, tensor(0.0977)), (2.9690471412580987e-05, tensor(0.0976)), (2.9731659688620303e-05, tensor(0.0978)), (2.9772905103330797e-05, tensor(0.0983)), (2.9814207735978416e-05, tensor(0.0986)), (2.9855567665939064e-05, tensor(0.0988)), (2.9896984972698774e-05, tensor(0.0985)), (2.993845973585383e-05, tensor(0.0981)), (2.9979992035110955e-05, tensor(0.0983)), (3.0021581950287424e-05, tensor(0.0990)), (3.006322956131126e-05, tensor(0.0990)), (3.010493494822135e-05, tensor(0.0988)), (3.0146698191167623e-05, tensor(0.0990)), (3.0188519370411197e-05, tensor(0.0987)), (3.0230398566324533e-05, tensor(0.0983)), (3.0272335859391585e-05, tensor(0.0984)), (3.0314331330207968e-05, tensor(0.0980)), (3.0356385059481085e-05, tensor(0.0982)), (3.0398497128030316e-05, tensor(0.0983)), (3.0440667616787167e-05, tensor(0.0985)), (3.048289660679539e-05, tensor(0.0979)), (3.052518417921119e-05, tensor(0.0982)), (3.0567530415303334e-05, tensor(0.0982)), (3.060993539645335e-05, tensor(0.0978)), (3.0652399204155664e-05, tensor(0.0975)), (3.0694921920017745e-05, tensor(0.0974)), (3.073750362576025e-05, tensor(0.0978)), (3.078014440321725e-05, tensor(0.0975)), (3.082284433433632e-05, tensor(0.0973)), (3.0865603501178695e-05, tensor(0.0974)), (3.090842198591949e-05, tensor(0.0971)), (3.09512998708478e-05, tensor(0.0970)), (3.099423723836686e-05, tensor(0.0972)), (3.103723417099424e-05, tensor(0.0972)), (3.108029075136198e-05, tensor(0.0970)), (3.1123407062216745e-05, tensor(0.0969)), (3.116658318641999e-05, tensor(0.0971)), (3.120981920694814e-05, tensor(0.0969)), (3.12531152068927e-05, tensor(0.0970)), (3.1296471269460454e-05, tensor(0.0971)), (3.133988747797363e-05, tensor(0.0971)), (3.138336391587003e-05, tensor(0.0975)), (3.142690066670321e-05, tensor(0.0979)), (3.147049781414264e-05, tensor(0.0974)), (3.151415544197385e-05, tensor(0.0975)), (3.155787363409861e-05, tensor(0.0974)), (3.1601652474535085e-05, tensor(0.0978)), (3.1645492047417994e-05, tensor(0.0976)), (3.1689392436998765e-05, tensor(0.0976)), (3.1733353727645715e-05, tensor(0.0978)), (3.177737600384419e-05, tensor(0.0975)), (3.182145935019675e-05, tensor(0.0979)), (3.1865603851423306e-05, tensor(0.0981)), (3.190980959236132e-05, tensor(0.0979)), (3.195407665796592e-05, tensor(0.0977)), (3.1998405133310106e-05, tensor(0.0975)), (3.204279510358488e-05, tensor(0.0974)), (3.2087246654099454e-05, tensor(0.0977)), (3.2131759870281356e-05, tensor(0.0980)), (3.2176334837676633e-05, tensor(0.0980)), (3.222097164195001e-05, tensor(0.0983)), (3.2265670368885054e-05, tensor(0.0980)), (3.231043110438433e-05, tensor(0.0977)), (3.2355253934469566e-05, tensor(0.0976)), (3.2400138945281836e-05, tensor(0.0978)), (3.244508622308171e-05, tensor(0.0980)), (3.249009585424942e-05, tensor(0.0982)), (3.253516792528504e-05, tensor(0.0983)), (3.258030252280861e-05, tensor(0.0982)), (3.2625499733560394e-05, tensor(0.0986)), (3.2670759644400915e-05, tensor(0.0985)), (3.271608234231124e-05, tensor(0.0984)), (3.276146791439311e-05, tensor(0.0979)), (3.2806916447869053e-05, tensor(0.0983)), (3.285242803008263e-05, tensor(0.0977)), (3.2898002748498575e-05, tensor(0.0969)), (3.2943640690702924e-05, tensor(0.0971)), (3.298934194440326e-05, tensor(0.0969)), (3.30351065974288e-05, tensor(0.0966)), (3.3080934737730627e-05, tensor(0.0961)), (3.3126826453381826e-05, tensor(0.0965)), (3.317278183257767e-05, tensor(0.0968)), (3.321880096363577e-05, tensor(0.0967)), (3.3264883934996256e-05, tensor(0.0970)), (3.331103083522196e-05, tensor(0.0972)), (3.335724175299856e-05, tensor(0.0973)), (3.3403516777134775e-05, tensor(0.0971)), (3.3449855996562516e-05, tensor(0.0965)), (3.3496259500337064e-05, tensor(0.0963)), (3.354272737763725e-05, tensor(0.0958)), (3.358925971776562e-05, tensor(0.0955)), (3.363585661014858e-05, tensor(0.0959)), (3.368251814433664e-05, tensor(0.0963)), (3.37292444100045e-05, tensor(0.0961)), (3.377603549695128e-05, tensor(0.0963)), (3.3822891495100664e-05, tensor(0.0966)), (3.3869812494501086e-05, tensor(0.0960)), (3.391679858532591e-05, tensor(0.0958)), (3.396384985787359e-05, tensor(0.0962)), (3.4010966402567835e-05, tensor(0.0960)), (3.4058148309957805e-05, tensor(0.0957)), (3.410539567071827e-05, tensor(0.0956)), (3.4152708575649796e-05, tensor(0.0957)), (3.420008711567891e-05, tensor(0.0959)), (3.424753138185828e-05, tensor(0.0961)), (3.429504146536687e-05, tensor(0.0968)), (3.4342617457510154e-05, tensor(0.0969)), (3.439025944972026e-05, tensor(0.0964)), (3.4437967533556165e-05, tensor(0.0962)), (3.448574180070384e-05, tensor(0.0962)), (3.4533582342976476e-05, tensor(0.0958)), (3.458148925231461e-05, tensor(0.0962)), (3.462946262078634e-05, tensor(0.0961)), (3.4677502540587466e-05, tensor(0.0959)), (3.472560910404172e-05, tensor(0.0960)), (3.477378240360087e-05, tensor(0.0961)), (3.482202253184497e-05, tensor(0.0961)), (3.487032958148249e-05, tensor(0.0961)), (3.4918703645350516e-05, tensor(0.0960)), (3.496714481641493e-05, tensor(0.0964)), (3.5015653187770564e-05, tensor(0.0966)), (3.506422885264141e-05, tensor(0.0968)), (3.511287190438077e-05, tensor(0.0968)), (3.5161582436471486e-05, tensor(0.0967)), (3.521036054252604e-05, tensor(0.0964)), (3.525920631628681e-05, tensor(0.0969)), (3.53081198516262e-05, tensor(0.0969)), (3.535710124254685e-05, tensor(0.0964)), (3.540615058318181e-05, tensor(0.0962)), (3.5455267967794694e-05, tensor(0.0966)), (3.550445349077991e-05, tensor(0.0966)), (3.5553707246662815e-05, tensor(0.0966)), (3.560302933009986e-05, tensor(0.0970)), (3.565241983587885e-05, tensor(0.0971)), (3.5701878858919074e-05, tensor(0.0971)), (3.575140649427148e-05, tensor(0.0973)), (3.58010028371189e-05, tensor(0.0977)), (3.58506679827762e-05, tensor(0.0975)), (3.590040202669047e-05, tensor(0.0975)), (3.5950205064441196e-05, tensor(0.0969)), (3.600007719174049e-05, tensor(0.0969)), (3.605001850443322e-05, tensor(0.0975)), (3.61000290984972e-05, tensor(0.0975)), (3.615010907004344e-05, tensor(0.0976)), (3.620025851531622e-05, tensor(0.0972)), (3.625047753069338e-05, tensor(0.0967)), (3.630076621268644e-05, tensor(0.0965)), (3.635112465794081e-05, tensor(0.0964)), (3.640155296323597e-05, tensor(0.0962)), (3.645205122548567e-05, tensor(0.0964)), (3.650261954173808e-05, tensor(0.0965)), (3.655325800917603e-05, tensor(0.0965)), (3.660396672511714e-05, tensor(0.0964)), (3.665474578701406e-05, tensor(0.0967)), (3.670559529245461e-05, tensor(0.0965)), (3.6756515339161994e-05, tensor(0.0964)), (3.6807506024995006e-05, tensor(0.0967)), (3.685856744794816e-05, tensor(0.0968)), (3.6909699706151934e-05, tensor(0.0980)), (3.6960902897872935e-05, tensor(0.0977)), (3.701217712151409e-05, tensor(0.0977)), (3.7063522475614836e-05, tensor(0.0979)), (3.7114939058851306e-05, tensor(0.0980)), (3.716642697003653e-05, tensor(0.0985)), (3.721798630812061e-05, tensor(0.0982)), (3.726961717219093e-05, tensor(0.0982)), (3.73213196614723e-05, tensor(0.0986)), (3.737309387532721e-05, tensor(0.0985)), (3.7424939913256004e-05, tensor(0.0986)), (3.747685787489702e-05, tensor(0.0983)), (3.752884786002685e-05, tensor(0.0989)), (3.7580909968560473e-05, tensor(0.0986)), (3.7633044300551505e-05, tensor(0.0990)), (3.768525095619235e-05, tensor(0.0990)), (3.773753003581441e-05, tensor(0.0994)), (3.7789881639888245e-05, tensor(0.0989)), (3.7842305869023836e-05, tensor(0.0985)), (3.7894802823970714e-05, tensor(0.0988)), (3.794737260561816e-05, tensor(0.0990)), (3.800001531499544e-05, tensor(0.0987)), (3.805273105327197e-05, tensor(0.0987)), (3.81055199217575e-05, tensor(0.0986)), (3.815838202190233e-05, tensor(0.0984)), (3.821131745529749e-05, tensor(0.0980)), (3.8264326323674976e-05, tensor(0.0981)), (3.831740872890786e-05, tensor(0.0979)), (3.837056477301058e-05, tensor(0.0978)), (3.8423794558139065e-05, tensor(0.0983)), (3.847709818659099e-05, tensor(0.0981)), (3.853047576080592e-05, tensor(0.0985)), (3.8583927383365535e-05, tensor(0.0981)), (3.8637453156993825e-05, tensor(0.0981)), (3.869105318455728e-05, tensor(0.0978)), (3.874472756906511e-05, tensor(0.0972)), (3.8798476413669396e-05, tensor(0.0972)), (3.885229982166534e-05, tensor(0.0970)), (3.890619789649143e-05, tensor(0.0968)), (3.896017074172964e-05, tensor(0.0968)), (3.9014218461105674e-05, tensor(0.0962)), (3.90683411584891e-05, tensor(0.0962)), (3.9122538937893576e-05, tensor(0.0958)), (3.917681190347708e-05, tensor(0.0962)), (3.923116015954206e-05, tensor(0.0961)), (3.9285583810535655e-05, tensor(0.0956)), (3.9340082961049935e-05, tensor(0.0962)), (3.939465771582203e-05, tensor(0.0963)), (3.944930817973437e-05, tensor(0.0961)), (3.9504034457814904e-05, tensor(0.0963)), (3.955883665523726e-05, tensor(0.0967)), (3.961371487732099e-05, tensor(0.0966)), (3.966866922953173e-05, tensor(0.0962)), (3.972369981748144e-05, tensor(0.0960)), (3.977880674692858e-05, tensor(0.0961)), (3.9833990123778325e-05, tensor(0.0967)), (3.9889250054082784e-05, tensor(0.0962)), (3.994458664404116e-05, tensor(0.0965))]
******

*** 2018-12-29 15:03:19,257 - code.resnet_fastai - DEBUG ***
Best LR: 1.032398535483242e-05
******

*** 2018-12-29 15:03:19,290 - matplotlib.axes._base - DEBUG ***
update_title_pos
******

*** 2018-12-29 15:03:19,293 - matplotlib.ticker - DEBUG ***
vmin 9.471074402518961e-06 vmax 4.2468701550101954e-05
******

*** 2018-12-29 15:03:19,293 - matplotlib.ticker - DEBUG ***
ticklocs array([1.e-07, 1.e-06, 1.e-05, 1.e-04, 1.e-03])
******

*** 2018-12-29 15:03:19,299 - matplotlib.ticker - DEBUG ***
vmin 9.471074402518961e-06 vmax 4.2468701550101954e-05
******

*** 2018-12-29 15:03:19,299 - matplotlib.ticker - DEBUG ***
ticklocs [2e-07, 3e-07, 4e-07, 5e-07, 6e-07, 7e-07, 8e-07, 9e-07, 2e-06, 3e-06, 4e-06, 4.9999999999999996e-06, 6e-06, 7e-06, 8e-06, 9e-06, 2e-05, 3.0000000000000004e-05, 4e-05, 5e-05, 6.000000000000001e-05, 7.000000000000001e-05, 8e-05, 9e-05, 0.0002, 0.00030000000000000003, 0.0004, 0.0005, 0.0006000000000000001, 0.0007, 0.0008, 0.0009000000000000001, 0.002, 0.003, 0.004, 0.005, 0.006, 0.007, 0.008, 0.009000000000000001]
******

*** 2018-12-29 15:03:19,476 - matplotlib.ticker - DEBUG ***
vmin 9.471074402518961e-06 vmax 4.2468701550101954e-05
******

*** 2018-12-29 15:03:19,476 - matplotlib.ticker - DEBUG ***
ticklocs array([1.e-07, 1.e-06, 1.e-05, 1.e-04, 1.e-03])
******

*** 2018-12-29 15:03:19,478 - matplotlib.ticker - DEBUG ***
vmin 9.471074402518961e-06 vmax 4.2468701550101954e-05
******

*** 2018-12-29 15:03:19,478 - matplotlib.ticker - DEBUG ***
ticklocs [2e-07, 3e-07, 4e-07, 5e-07, 6e-07, 7e-07, 8e-07, 9e-07, 2e-06, 3e-06, 4e-06, 4.9999999999999996e-06, 6e-06, 7e-06, 8e-06, 9e-06, 2e-05, 3.0000000000000004e-05, 4e-05, 5e-05, 6.000000000000001e-05, 7.000000000000001e-05, 8e-05, 9e-05, 0.0002, 0.00030000000000000003, 0.0004, 0.0005, 0.0006000000000000001, 0.0007, 0.0008, 0.0009000000000000001, 0.002, 0.003, 0.004, 0.005, 0.006, 0.007, 0.008, 0.009000000000000001]
******

*** 2018-12-29 15:03:19,583 - matplotlib.axes._base - DEBUG ***
update_title_pos
******

*** 2018-12-29 15:03:19,585 - matplotlib.ticker - DEBUG ***
vmin 9.471074402518961e-06 vmax 4.2468701550101954e-05
******

*** 2018-12-29 15:03:19,585 - matplotlib.ticker - DEBUG ***
ticklocs array([1.e-07, 1.e-06, 1.e-05, 1.e-04, 1.e-03])
******

*** 2018-12-29 15:03:19,587 - matplotlib.ticker - DEBUG ***
vmin 9.471074402518961e-06 vmax 4.2468701550101954e-05
******

*** 2018-12-29 15:03:19,587 - matplotlib.ticker - DEBUG ***
ticklocs [2e-07, 3e-07, 4e-07, 5e-07, 6e-07, 7e-07, 8e-07, 9e-07, 2e-06, 3e-06, 4e-06, 4.9999999999999996e-06, 6e-06, 7e-06, 8e-06, 9e-06, 2e-05, 3.0000000000000004e-05, 4e-05, 5e-05, 6.000000000000001e-05, 7.000000000000001e-05, 8e-05, 9e-05, 0.0002, 0.00030000000000000003, 0.0004, 0.0005, 0.0006000000000000001, 0.0007, 0.0008, 0.0009000000000000001, 0.002, 0.003, 0.004, 0.005, 0.006, 0.007, 0.008, 0.009000000000000001]
******

*** 2018-12-29 15:03:19,596 - matplotlib.ticker - DEBUG ***
vmin 9.471074402518961e-06 vmax 4.2468701550101954e-05
******

*** 2018-12-29 15:03:19,596 - matplotlib.ticker - DEBUG ***
ticklocs array([1.e-07, 1.e-06, 1.e-05, 1.e-04, 1.e-03])
******

*** 2018-12-29 15:03:19,598 - matplotlib.ticker - DEBUG ***
vmin 9.471074402518961e-06 vmax 4.2468701550101954e-05
******

*** 2018-12-29 15:03:19,598 - matplotlib.ticker - DEBUG ***
ticklocs [2e-07, 3e-07, 4e-07, 5e-07, 6e-07, 7e-07, 8e-07, 9e-07, 2e-06, 3e-06, 4e-06, 4.9999999999999996e-06, 6e-06, 7e-06, 8e-06, 9e-06, 2e-05, 3.0000000000000004e-05, 4e-05, 5e-05, 6.000000000000001e-05, 7.000000000000001e-05, 8e-05, 9e-05, 0.0002, 0.00030000000000000003, 0.0004, 0.0005, 0.0006000000000000001, 0.0007, 0.0008, 0.0009000000000000001, 0.002, 0.003, 0.004, 0.005, 0.006, 0.007, 0.008, 0.009000000000000001]
******

*** 2018-12-29 15:03:19,622 - code.resnet_fastai - INFO ***
Start model fitting: Stage 2
******

*** 2018-12-29 15:03:19,623 - code.resnet_fastai - DEBUG ***
LR: 7.226789748382694e-06
******

1         0.096533                
LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.
epoch     train_loss  valid_loss  fbeta   
1         0.092830    1.038714    0.681179  
2         0.093802    0.696042    0.649610  
3         0.090888    0.290644    0.541737  
Exception ignored in: <bound method _DataLoaderIter.__del__ of <torch.utils.data.dataloader._DataLoaderIter object at 0x7fe0a89aca20>>
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 399, in __del__
    self._shutdown_workers()
  File "/home/ubuntu/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 378, in _shutdown_workers
    self.worker_result_queue.get()
  File "/usr/lib/python3.6/multiprocessing/queues.py", line 337, in get
    return _ForkingPickler.loads(res)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/torch/multiprocessing/reductions.py", line 151, in rebuild_storage_fd
    fd = df.detach()
  File "/usr/lib/python3.6/multiprocessing/resource_sharer.py", line 57, in detach
    with _resource_sharer.get_connection(self._id) as conn:
  File "/usr/lib/python3.6/multiprocessing/resource_sharer.py", line 87, in get_connection
    c = Client(address, authkey=process.current_process().authkey)
  File "/usr/lib/python3.6/multiprocessing/connection.py", line 487, in Client
    c = SocketClient(address)
  File "/usr/lib/python3.6/multiprocessing/connection.py", line 614, in SocketClient
    s.connect(address)
ConnectionRefusedError: [Errno 111] Connection refused
Traceback (most recent call last):
  File "/usr/lib/python3.6/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/usr/lib/python3.6/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/ubuntu/atlas/code/resnet_fastai.py", line 484, in <module>
    learn = fit_model(learn, stage=2, fold=index)
  File "/home/ubuntu/atlas/code/resnet_fastai.py", line 420, in fit_model
    learn.fit_one_cycle(cyc_len, max_lr)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/fastai/train.py", line 21, in fit_one_cycle
    learn.fit(cyc_len, max_lr, wd=wd, callbacks=callbacks)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/fastai/basic_train.py", line 166, in fit
    callbacks=self.callbacks+callbacks)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/fastai/basic_train.py", line 94, in fit
    raise e
  File "/home/ubuntu/.local/lib/python3.6/site-packages/fastai/basic_train.py", line 84, in fit
    loss = loss_batch(model, xb, yb, loss_func, opt, cb_handler)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/fastai/basic_train.py", line 28, in loss_batch
    opt.step()
  File "/home/ubuntu/.local/lib/python3.6/site-packages/fastai/callback.py", line 47, in step
    self.opt.step()
  File "/home/ubuntu/.local/lib/python3.6/site-packages/torch/optim/adam.py", line 93, in step
    exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 227, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 16183) is killed by signal: Terminated. Details are lost due to multiprocessing. Rerunning with num_workers=0 may give better error trace.
