*** 2018-12-28 07:00:34,603 - code.resnet_fastai - DEBUG ***
Start a new training task
******

*** 2018-12-28 07:00:34,603 - code.resnet_fastai - INFO ***
Device ID: 0
Image size: 512
Network architecture: resnet
Loss function: bce
Sampler: random
Encoder depth: 50
Dropout: 0.5
Threshold: 0.1
Stage 1 #epoch: 15
Stage 2 #epoch: 25
Learning rate #1: 0
Batch size: 32
Dataset: official
Dataset directory: data/official
Output directory: output
******

*** 2018-12-28 07:00:34,604 - code.resnet_fastai - INFO ***
Offical stats: ([0.07986162506177984, 0.05217604947235713, 0.054227752481757215, 0.08201468927464939], [0.1403192215484648, 0.1041239635111223, 0.1532386688507187, 0.14099509309392533])
******

*** 2018-12-28 07:00:34,663 - code.resnet_fastai - DEBUG ***
# Test ids: 11702
******

*** 2018-12-28 07:00:35,445 - code.resnet_fastai - DEBUG ***
Start of fold 0
******

*** 2018-12-28 07:00:35,446 - code.resnet_fastai - DEBUG ***
Size of valid set: 6209
******

*** 2018-12-28 07:00:35,633 - code.resnet_fastai - DEBUG ***
LabelList
y: MultiCategoryList (24863 items)
[MultiCategory 16;0, MultiCategory 1, MultiCategory 18, MultiCategory 0, MultiCategory 25;2]...
Path: data/official
x: ImageItemList (24863 items)
[Image (4, 512, 512), Image (4, 512, 512), Image (4, 512, 512), Image (4, 512, 512), Image (4, 512, 512)]...
Path: data/official
******

*** 2018-12-28 07:00:35,948 - code.resnet_fastai - DEBUG ***
LabelList
y: MultiCategoryList (6209 items)
[MultiCategory 7;1;2;0, MultiCategory 5, MultiCategory 21, MultiCategory 0, MultiCategory 25;4]...
Path: data/official
x: ImageItemList (6209 items)
[Image (4, 512, 512), Image (4, 512, 512), Image (4, 512, 512), Image (4, 512, 512), Image (4, 512, 512)]...
Path: data/official
******

*** 2018-12-28 07:00:38,921 - code.resnet_fastai - DEBUG ***
Databunch created
******

*** 2018-12-28 07:00:38,921 - code.resnet_fastai - INFO ***
Initialising model.
******

*** 2018-12-28 07:00:43,332 - code.resnet_fastai - INFO ***
Complete initialising model.
******

*** 2018-12-28 07:00:43,333 - code.resnet_fastai - INFO ***
No pretrained model.
******

*** 2018-12-28 07:00:43,333 - code.resnet_fastai - DEBUG ***
Start finding LR
******

epoch     train_loss  valid_loss  fbeta   
1         0.178007                
*** 2018-12-28 07:10:13,902 - code.resnet_fastai - DEBUG ***
[(0.01, tensor(0.8393)), (0.010016107337527294, tensor(0.8444)), (0.01003224061968681, tensor(0.8279)), (0.010048399888268441, tensor(0.8007)), (0.010064585185129395, tensor(0.7774)), (0.010080796552194305, tensor(0.7496)), (0.01009703403145532, tensor(0.7238)), (0.010113297664972242, tensor(0.6932)), (0.010129587494872612, tensor(0.6623)), (0.010145903563351828, tensor(0.6276)), (0.010162245912673255, tensor(0.5971)), (0.010178614585168335, tensor(0.5660)), (0.010195009623236688, tensor(0.5366)), (0.010211431069346237, tensor(0.5104)), (0.010227878966033301, tensor(0.4875)), (0.010244353355902722, tensor(0.4647)), (0.01026085428162796, tensor(0.4449)), (0.010277381785951216, tensor(0.4268)), (0.010293935911683535, tensor(0.4109)), (0.010310516701704915, tensor(0.3983)), (0.01032712419896443, tensor(0.3884)), (0.010343758446480332, tensor(0.3769)), (0.010360419487340155, tensor(0.3687)), (0.010377107364700848, tensor(0.3612)), (0.01039382212178887, tensor(0.3544)), (0.010410563801900299, tensor(0.3476)), (0.010427332448400963, tensor(0.3407)), (0.010444128104726532, tensor(0.3358)), (0.010460950814382643, tensor(0.3297)), (0.010477800620945013, tensor(0.3273)), (0.010494677568059536, tensor(0.3225)), (0.010511581699442421, tensor(0.3152)), (0.010528513058880284, tensor(0.3100)), (0.010545471690230275, tensor(0.3067)), (0.010562457637420182, tensor(0.3041)), (0.010579470944448548, tensor(0.3000)), (0.010596511655384791, tensor(0.2960)), (0.010613579814369308, tensor(0.2923)), (0.010630675465613599, tensor(0.2898)), (0.010647798653400375, tensor(0.2863)), (0.010664949422083674, tensor(0.2833)), (0.010682127816088976, tensor(0.2800)), (0.010699333879913318, tensor(0.2763)), (0.010716567658125415, tensor(0.2734)), (0.010733829195365766, tensor(0.2697)), (0.010751118536346773, tensor(0.2669)), (0.01076843572585286, tensor(0.2633)), (0.010785780808740588, tensor(0.2607)), (0.010803153829938768, tensor(0.2574)), (0.010820554834448578, tensor(0.2551)), (0.010837983867343682, tensor(0.2531)), (0.010855440973770348, tensor(0.2508)), (0.010872926198947561, tensor(0.2477)), (0.01089043958816714, tensor(0.2451)), (0.010907981186793862, tensor(0.2431)), (0.010925551040265567, tensor(0.2412)), (0.01094314919409329, tensor(0.2398)), (0.010960775693861368, tensor(0.2374)), (0.010978430585227569, tensor(0.2361)), (0.010996113913923189, tensor(0.2337)), (0.011013825725753202, tensor(0.2319)), (0.011031566066596353, tensor(0.2305)), (0.011049334982405281, tensor(0.2293)), (0.011067132519206655, tensor(0.2276)), (0.01108495872310127, tensor(0.2261)), (0.01110281364026418, tensor(0.2251)), (0.011120697316944818, tensor(0.2237)), (0.011138609799467108, tensor(0.2229)), (0.011156551134229592, tensor(0.2214)), (0.011174521367705546, tensor(0.2197)), (0.011192520546443104, tensor(0.2190)), (0.011210548717065377, tensor(0.2178)), (0.01122860592627057, tensor(0.2168)), (0.01124669222083211, tensor(0.2154)), (0.011264807647598763, tensor(0.2142)), (0.011282952253494754, tensor(0.2131)), (0.011301126085519893, tensor(0.2123)), (0.01131932919074969, tensor(0.2113)), (0.011337561616335484, tensor(0.2101)), (0.011355823409504565, tensor(0.2089)), (0.011374114617560287, tensor(0.2080)), (0.011392435287882204, tensor(0.2067)), (0.011410785467926182, tensor(0.2061)), (0.011429165205224523, tensor(0.2047)), (0.011447574547386097, tensor(0.2044)), (0.011466013542096459, tensor(0.2040)), (0.011484482237117963, tensor(0.2029)), (0.011502980680289912, tensor(0.2030)), (0.011521508919528647, tensor(0.2023)), (0.011540067002827703, tensor(0.2018)), (0.011558654978257917, tensor(0.2010)), (0.01157727289396755, tensor(0.2002)), (0.011595920798182422, tensor(0.1992)), (0.01161459873920603, tensor(0.1990)), (0.011633306765419679, tensor(0.1986)), (0.011652044925282594, tensor(0.1976)), (0.011670813267332065, tensor(0.1969)), (0.011689611840183559, tensor(0.1961)), (0.011708440692530846, tensor(0.1959)), (0.011727299873146135, tensor(0.1950)), (0.01174618943088019, tensor(0.1942)), (0.011765109414662463, tensor(0.1932)), (0.011784059873501212, tensor(0.1933)), (0.011803040856483645, tensor(0.1930)), (0.011822052412776025, tensor(0.1924)), (0.01184109459162382, tensor(0.1916)), (0.011860167442351808, tensor(0.1908)), (0.011879271014364225, tensor(0.1914)), (0.011898405357144882, tensor(0.1907)), (0.011917570520257291, tensor(0.1901)), (0.011936766553344802, tensor(0.1901)), (0.011955993506130724, tensor(0.1897)), (0.011975251428418463, tensor(0.1892)), (0.011994540370091636, tensor(0.1890)), (0.012013860381114216, tensor(0.1889)), (0.012033211511530655, tensor(0.1887)), (0.012052593811466011, tensor(0.1884)), (0.012072007331126075, tensor(0.1882)), (0.012091452120797515, tensor(0.1874)), (0.012110928230847995, tensor(0.1873)), (0.012130435711726305, tensor(0.1867)), (0.012149974613962496, tensor(0.1866)), (0.012169544988168008, tensor(0.1869)), (0.012189146885035808, tensor(0.1864)), (0.012208780355340511, tensor(0.1864)), (0.01222844544993852, tensor(0.1857)), (0.012248142219768143, tensor(0.1851)), (0.012267870715849755, tensor(0.1849)), (0.012287630989285894, tensor(0.1839)), (0.012307423091261418, tensor(0.1834)), (0.012327247073043635, tensor(0.1833)), (0.01234710298598242, tensor(0.1830)), (0.012366990881510368, tensor(0.1826)), (0.01238691081114291, tensor(0.1817)), (0.012406862826478467, tensor(0.1819)), (0.012426846979198559, tensor(0.1815)), (0.012446863321067956, tensor(0.1812)), (0.01246691190393481, tensor(0.1808)), (0.012486992779730782, tensor(0.1803)), (0.01250710600047118, tensor(0.1794)), (0.012527251618255105, tensor(0.1789)), (0.012547429685265558, tensor(0.1783)), (0.012567640253769614, tensor(0.1783)), (0.012587883376118523, tensor(0.1784)), (0.012608159104747857, tensor(0.1783)), (0.012628467492177655, tensor(0.1784)), (0.01264880859101255, tensor(0.1782)), (0.012669182453941908, tensor(0.1775)), (0.012689589133739958, tensor(0.1769)), (0.012710028683265942, tensor(0.1764)), (0.012730501155464235, tensor(0.1759)), (0.012751006603364501, tensor(0.1763)), (0.012771545080081817, tensor(0.1760)), (0.01279211663881681, tensor(0.1755)), (0.012812721332855801, tensor(0.1752)), (0.012833359215570947, tensor(0.1746)), (0.012854030340420368, tensor(0.1738)), (0.01287473476094829, tensor(0.1736)), (0.012895472530785186, tensor(0.1738)), (0.012916243703647917, tensor(0.1734)), (0.01293704833333986, tensor(0.1736)), (0.012957886473751061, tensor(0.1732)), (0.012978758178858367, tensor(0.1739)), (0.012999663502725568, tensor(0.1742)), (0.013020602499503531, tensor(0.1738)), (0.013041575223430354, tensor(0.1737)), (0.01306258172883149, tensor(0.1728)), (0.013083622070119904, tensor(0.1728)), (0.013104696301796202, tensor(0.1723)), (0.013125804478448773, tensor(0.1719)), (0.013146946654753935, tensor(0.1721)), (0.013168122885476079, tensor(0.1715)), (0.013189333225467803, tensor(0.1717)), (0.01321057772967006, tensor(0.1716)), (0.013231856453112294, tensor(0.1716)), (0.01325316945091259, tensor(0.1716)), (0.013274516778277817, tensor(0.1715)), (0.013295898490503761, tensor(0.1713)), (0.013317314642975277, tensor(0.1712)), (0.013338765291166435, tensor(0.1709)), (0.013360250490640654, tensor(0.1705)), (0.013381770297050845, tensor(0.1706)), (0.013403324766139577, tensor(0.1706)), (0.01342491395373919, tensor(0.1705)), (0.013446537915771967, tensor(0.1699)), (0.013468196708250255, tensor(0.1696)), (0.013489890387276632, tensor(0.1691)), (0.013511619009044035, tensor(0.1685)), (0.013533382629835925, tensor(0.1686)), (0.0135551813060264, tensor(0.1692)), (0.013577015094080382, tensor(0.1687)), (0.013598884050553735, tensor(0.1683)), (0.013620788232093416, tensor(0.1683)), (0.013642727695437626, tensor(0.1682)), (0.013664702497415962, tensor(0.1683)), (0.013686712694949555, tensor(0.1681)), (0.01370875834505122, tensor(0.1678)), (0.013730839504825603, tensor(0.1676)), (0.013752956231469335, tensor(0.1673)), (0.013775108582271173, tensor(0.1673)), (0.01379729661461215, tensor(0.1675)), (0.013819520385965723, tensor(0.1671)), (0.013841779953897928, tensor(0.1667)), (0.013864075376067524, tensor(0.1665)), (0.01388640671022614, tensor(0.1666)), (0.013908774014218429, tensor(0.1670)), (0.013931177345982216, tensor(0.1668)), (0.013953616763548646, tensor(0.1664)), (0.013976092325042344, tensor(0.1660)), (0.013998604088681553, tensor(0.1658)), (0.014021152112778285, tensor(0.1660)), (0.014043736455738491, tensor(0.1662)), (0.014066357176062184, tensor(0.1660)), (0.014089014332343615, tensor(0.1658)), (0.014111707983271406, tensor(0.1658)), (0.014134438187628722, tensor(0.1651)), (0.014157205004293403, tensor(0.1649)), (0.014180008492238128, tensor(0.1646)), (0.014202848710530565, tensor(0.1638)), (0.014225725718333524, tensor(0.1642)), (0.014248639574905114, tensor(0.1640)), (0.014271590339598888, tensor(0.1641)), (0.014294578071864007, tensor(0.1641)), (0.014317602831245382, tensor(0.1635)), (0.014340664677383843, tensor(0.1638)), (0.014363763670016278, tensor(0.1635)), (0.014386899868975801, tensor(0.1633)), (0.014410073334191897, tensor(0.1632)), (0.014433284125690584, tensor(0.1631)), (0.014456532303594567, tensor(0.1630)), (0.01447981792812339, tensor(0.1626)), (0.014503141059593595, tensor(0.1625)), (0.014526501758418875, tensor(0.1627)), (0.014549900085110243, tensor(0.1627)), (0.014573336100276169, tensor(0.1627)), (0.014596809864622752, tensor(0.1623)), (0.014620321438953874, tensor(0.1622)), (0.014643870884171349, tensor(0.1620)), (0.014667458261275095, tensor(0.1619)), (0.014691083631363278, tensor(0.1617)), (0.014714747055632485, tensor(0.1618)), (0.014738448595377866, tensor(0.1615)), (0.014762188311993307, tensor(0.1613)), (0.014785966266971581, tensor(0.1612)), (0.014809782521904511, tensor(0.1607)), (0.014833637138483124, tensor(0.1603)), (0.014857530178497818, tensor(0.1601)), (0.014881461703838519, tensor(0.1603)), (0.014905431776494843, tensor(0.1603)), (0.014929440458556247, tensor(0.1601)), (0.014953487812212205, tensor(0.1606)), (0.014977573899752364, tensor(0.1609)), (0.015001698783566693, tensor(0.1612)), (0.015025862526145661, tensor(0.1613)), (0.015050065190080395, tensor(0.1616)), (0.015074306838062835, tensor(0.1617)), (0.0150985875328859, tensor(0.1619)), (0.01512290733744366, tensor(0.1618)), (0.015147266314731478, tensor(0.1619)), (0.015171664527846197, tensor(0.1616)), (0.015196102039986283, tensor(0.1617)), (0.015220578914452011, tensor(0.1612)), (0.015245095214645598, tensor(0.1617)), (0.015269651004071402, tensor(0.1614)), (0.015294246346336056, tensor(0.1612)), (0.015318881305148658, tensor(0.1608)), (0.015343555944320915, tensor(0.1611)), (0.015368270327767322, tensor(0.1612)), (0.015393024519505326, tensor(0.1608)), (0.015417818583655483, tensor(0.1610)), (0.015442652584441635, tensor(0.1610)), (0.015467526586191067, tensor(0.1611)), (0.015492440653334685, tensor(0.1612)), (0.015517394850407167, tensor(0.1617)), (0.015542389242047146, tensor(0.1615)), (0.015567423892997369, tensor(0.1617)), (0.015592498868104866, tensor(0.1620)), (0.015617614232321116, tensor(0.1617)), (0.01564277005070222, tensor(0.1614)), (0.01566796638840907, tensor(0.1612)), (0.01569320331070751, tensor(0.1609)), (0.01571848088296851, tensor(0.1607)), (0.015743799170668336, tensor(0.1608)), (0.015769158239388726, tensor(0.1607)), (0.01579455815481704, tensor(0.1606)), (0.015819998982746452, tensor(0.1604)), (0.0158454807890761, tensor(0.1598)), (0.015871003639811292, tensor(0.1592)), (0.015896567601063626, tensor(0.1591)), (0.015922172739051204, tensor(0.1590)), (0.01594781912009878, tensor(0.1587)), (0.015973506810637944, tensor(0.1583)), (0.015999235877207293, tensor(0.1582)), (0.01602500638645259, tensor(0.1580)), (0.01605081840512695, tensor(0.1581)), (0.016076672000091018, tensor(0.1580)), (0.016102567238313124, tensor(0.1579)), (0.01612850418686947, tensor(0.1579)), (0.016154482912944294, tensor(0.1582)), (0.016180503483830065, tensor(0.1583)), (0.016206565966927624, tensor(0.1587)), (0.01623267042974639, tensor(0.1585)), (0.016258816939904514, tensor(0.1584)), (0.016285005565129067, tensor(0.1579)), (0.016311236373256204, tensor(0.1577)), (0.016337509432231355, tensor(0.1577)), (0.01636382481010938, tensor(0.1575)), (0.016390182575054778, tensor(0.1572)), (0.016416582795341812, tensor(0.1571)), (0.016443025539354745, tensor(0.1571)), (0.016469510875587977, tensor(0.1568)), (0.016496038872646227, tensor(0.1565)), (0.016522609599244734, tensor(0.1572)), (0.01654922312420941, tensor(0.1579)), (0.016575879516477023, tensor(0.1583)), (0.016602578845095386, tensor(0.1583)), (0.01662932117922353, tensor(0.1583)), (0.016656106588131884, tensor(0.1581)), (0.016682935141202445, tensor(0.1582)), (0.016709806907928972, tensor(0.1578)), (0.016736721957917165, tensor(0.1582)), (0.01676368036088483, tensor(0.1583)), (0.016790682186662072, tensor(0.1580)), (0.01681772750519148, tensor(0.1580)), (0.016844816386528298, tensor(0.1582)), (0.016871948900840605, tensor(0.1580)), (0.016899125118409516, tensor(0.1580)), (0.016926345109629332, tensor(0.1581)), (0.016953608945007758, tensor(0.1582)), (0.016980916695166055, tensor(0.1579)), (0.017008268430839243, tensor(0.1578)), (0.01703566422287628, tensor(0.1574)), (0.01706310414224023, tensor(0.1572)), (0.01709058826000847, tensor(0.1574)), (0.017118116647372866, tensor(0.1575)), (0.01714568937563995, tensor(0.1574)), (0.017173306516231106, tensor(0.1580)), (0.017200968140682765, tensor(0.1579)), (0.017228674320646585, tensor(0.1575)), (0.017256425127889632, tensor(0.1568)), (0.01728422063429457, tensor(0.1564)), (0.017312060911859852, tensor(0.1560)), (0.01733994603269989, tensor(0.1561)), (0.017367876069045263, tensor(0.1559)), (0.017395851093242893, tensor(0.1562)), (0.017423871177756234, tensor(0.1560)), (0.017451936395165454, tensor(0.1561)), (0.017480046818167633, tensor(0.1563)), (0.017508202519576944, tensor(0.1560)), (0.017536403572324846, tensor(0.1558)), (0.017564650049460274, tensor(0.1559)), (0.017592942024149818, tensor(0.1553)), (0.01762127956967793, tensor(0.1547)), (0.01764966275944709, tensor(0.1549)), (0.01767809166697802, tensor(0.1543)), (0.017706566365909865, tensor(0.1544)), (0.017735086930000376, tensor(0.1541)), (0.017763653433126116, tensor(0.1543)), (0.01779226594928264, tensor(0.1545)), (0.017820924552584688, tensor(0.1544)), (0.01784962931726638, tensor(0.1539)), (0.017878380317681405, tensor(0.1539)), (0.01790717762830323, tensor(0.1536)), (0.017936021323725255, tensor(0.1538)), (0.017964911478661052, tensor(0.1540)), (0.017993848167944526, tensor(0.1538)), (0.018022831466530123, tensor(0.1541)), (0.018051861449493015, tensor(0.1540)), (0.018080938192029307, tensor(0.1537)), (0.01811006176945622, tensor(0.1536)), (0.018139232257212298, tensor(0.1536)), (0.01816844973085759, tensor(0.1531)), (0.018197714266073847, tensor(0.1535)), (0.018227025938664734, tensor(0.1533)), (0.018256384824556018, tensor(0.1532)), (0.018285790999795742, tensor(0.1530)), (0.01831524454055447, tensor(0.1534)), (0.01834474552312543, tensor(0.1538)), (0.01837429402392476, tensor(0.1534)), (0.01840389011949167, tensor(0.1537)), (0.018433533886488655, tensor(0.1534)), (0.018463225401701703, tensor(0.1534)), (0.018492964742040475, tensor(0.1544)), (0.018522751984538514, tensor(0.1544)), (0.018552587206353444, tensor(0.1542)), (0.01858247048476717, tensor(0.1539)), (0.018612401897186083, tensor(0.1543)), (0.018642381521141244, tensor(0.1544)), (0.018672409434288603, tensor(0.1540)), (0.018702485714409193, tensor(0.1536)), (0.018732610439409332, tensor(0.1537)), (0.01876278368732082, tensor(0.1534)), (0.018793005536301147, tensor(0.1534)), (0.018823276064633698, tensor(0.1535)), (0.018853595350727944, tensor(0.1534)), (0.01888396347311966, tensor(0.1535)), (0.01891438051047112, tensor(0.1536)), (0.018944846541571306, tensor(0.1537)), (0.018975361645336093, tensor(0.1540)), (0.01900592590080848, tensor(0.1545)), (0.019036539387158786, tensor(0.1540)), (0.019067202183684843, tensor(0.1542)), (0.01909791436981222, tensor(0.1539)), (0.01912867602509441, tensor(0.1537)), (0.019159487229213054, tensor(0.1534)), (0.019190348061978137, tensor(0.1534)), (0.019221258603328188, tensor(0.1539)), (0.019252218933330505, tensor(0.1546)), (0.01928322913218136, tensor(0.1546)), (0.019314289280206173, tensor(0.1548)), (0.019345399457859784, tensor(0.1548)), (0.019376559745726586, tensor(0.1548)), (0.019407770224520806, tensor(0.1555)), (0.019439030975086657, tensor(0.1554)), (0.01947034207839858, tensor(0.1552)), (0.019501703615561444, tensor(0.1554)), (0.01953311566781075, tensor(0.1553)), (0.019564578316512864, tensor(0.1556)), (0.019596091643165187, tensor(0.1560)), (0.01962765572939641, tensor(0.1560)), (0.0196592706569667, tensor(0.1558)), (0.019690936507767917, tensor(0.1556)), (0.019722653363823832, tensor(0.1556)), (0.019754421307290323, tensor(0.1551)), (0.01978624042045561, tensor(0.1546)), (0.019818110785740455, tensor(0.1544)), (0.019850032485698377, tensor(0.1551)), (0.019882005603015864, tensor(0.1551)), (0.019914030220512597, tensor(0.1556)), (0.019946106421141648, tensor(0.1554)), (0.019978234287989713, tensor(0.1555)), (0.020010413904277314, tensor(0.1552)), (0.020042645353359013, tensor(0.1555)), (0.020074928718723656, tensor(0.1553)), (0.02010726408399454, tensor(0.1552)), (0.02013965153292967, tensor(0.1547)), (0.02017209114942197, tensor(0.1549)), (0.02020458301749948, tensor(0.1546)), (0.020237127221325583, tensor(0.1549)), (0.02026972384519925, tensor(0.1552)), (0.020302372973555218, tensor(0.1555)), (0.020335074690964225, tensor(0.1554)), (0.020367829082133228, tensor(0.1551)), (0.020400636231905644, tensor(0.1549)), (0.02043349622526153, tensor(0.1554)), (0.020466409147317823, tensor(0.1552)), (0.02049937508332858, tensor(0.1555)), (0.020532394118685154, tensor(0.1553)), (0.020565466338916462, tensor(0.1557)), (0.020598591829689174, tensor(0.1558)), (0.020631770676807947, tensor(0.1558)), (0.020665002966215654, tensor(0.1558)), (0.020698288783993588, tensor(0.1559)), (0.020731628216361705, tensor(0.1558)), (0.020765021349678836, tensor(0.1562)), (0.02079846827044291, tensor(0.1569)), (0.020831969065291184, tensor(0.1573)), (0.02086552382100046, tensor(0.1575)), (0.020899132624487327, tensor(0.1574)), (0.020932795562808356, tensor(0.1571)), (0.02096651272316035, tensor(0.1574)), (0.021000284192880577, tensor(0.1573)), (0.02103411005944696, tensor(0.1574)), (0.021067990410478334, tensor(0.1571)), (0.02110192533373467, tensor(0.1569)), (0.021135914917117287, tensor(0.1570)), (0.021169959248669105, tensor(0.1570)), (0.021204058416574842, tensor(0.1570)), (0.02123821250916126, tensor(0.1571)), (0.021272421614897407, tensor(0.1570)), (0.02130668582239481, tensor(0.1574)), (0.021341005220407744, tensor(0.1574)), (0.021375379897833426, tensor(0.1575)), (0.02140980994371228, tensor(0.1575)), (0.02144429544722814, tensor(0.1575)), (0.021478836497708483, tensor(0.1577)), (0.0215134331846247, tensor(0.1581)), (0.021548085597592262, tensor(0.1583)), (0.021582793826371006, tensor(0.1584)), (0.021617557960865338, tensor(0.1585)), (0.02165237809112449, tensor(0.1582)), (0.02168725430734272, tensor(0.1585)), (0.021722186699859584, tensor(0.1585)), (0.021757175359160136, tensor(0.1586)), (0.021792220375875183, tensor(0.1587)), (0.021827321840781524, tensor(0.1588)), (0.021862479844802156, tensor(0.1591)), (0.021897694479006545, tensor(0.1587)), (0.021932965834610837, tensor(0.1588)), (0.021968294002978103, tensor(0.1590)), (0.022003679075618583, tensor(0.1592)), (0.0220391211441899, tensor(0.1588)), (0.02207462030049734, tensor(0.1582)), (0.022110176636494033, tensor(0.1585)), (0.022145790244281247, tensor(0.1582)), (0.022181461216108575, tensor(0.1582)), (0.022217189644374215, tensor(0.1580)), (0.0222529756216252, tensor(0.1585)), (0.022288819240557613, tensor(0.1582)), (0.022324720594016863, tensor(0.1585)), (0.022360679774997897, tensor(0.1585)), (0.02239669687664546, tensor(0.1580)), (0.02243277199225432, tensor(0.1579)), (0.022468905215269525, tensor(0.1580)), (0.022505096639286636, tensor(0.1580)), (0.02254134635805197, tensor(0.1577)), (0.022577654465462848, tensor(0.1583)), (0.02261402105556783, tensor(0.1582)), (0.022650446222566966, tensor(0.1581)), (0.022686930060812033, tensor(0.1578)), (0.02272347266480679, tensor(0.1581)), (0.02276007412920722, tensor(0.1584)), (0.022796734548821758, tensor(0.1586)), (0.022833454018611556, tensor(0.1586)), (0.022870232633690726, tensor(0.1590)), (0.02290707048932658, tensor(0.1589)), (0.022943967680939894, tensor(0.1590)), (0.022980924304105112, tensor(0.1589)), (0.02301794045455065, tensor(0.1590)), (0.023055016228159114, tensor(0.1595)), (0.02309215172096753, tensor(0.1592)), (0.023129347029167642, tensor(0.1595)), (0.023166602249106112, tensor(0.1595)), (0.0232039174772848, tensor(0.1599)), (0.02324129281036101, tensor(0.1593)), (0.023278728345147725, tensor(0.1594)), (0.02331622417861387, tensor(0.1592)), (0.02335378040788457, tensor(0.1595)), (0.023391397130241378, tensor(0.1595)), (0.023429074443122554, tensor(0.1603)), (0.023466812444123297, tensor(0.1603)), (0.02350461123099602, tensor(0.1603)), (0.023542470901650563, tensor(0.1601)), (0.023580391554154503, tensor(0.1601)), (0.023618373286733354, tensor(0.1601)), (0.023656416197770855, tensor(0.1601)), (0.023694520385809218, tensor(0.1603)), (0.023732685949549373, tensor(0.1607)), (0.02377091298785124, tensor(0.1605)), (0.02380920159973396, tensor(0.1606)), (0.023847551884376192, tensor(0.1607)), (0.02388596394111632, tensor(0.1605)), (0.023924437869452753, tensor(0.1605)), (0.02396297376904416, tensor(0.1597)), (0.024001571739709723, tensor(0.1596)), (0.02404023188142943, tensor(0.1592)), (0.024078954294344288, tensor(0.1594)), (0.024117739078756615, tensor(0.1597)), (0.02415658633513029, tensor(0.1592)), (0.024195496164091, tensor(0.1589)), (0.024234468666426538, tensor(0.1596)), (0.02427350394308701, tensor(0.1596)), (0.02431260209518515, tensor(0.1594)), (0.024351763223996542, tensor(0.1593)), (0.02439098743095991, tensor(0.1596)), (0.024430274817677355, tensor(0.1600)), (0.02446962548591464, tensor(0.1599)), (0.02450903953760145, tensor(0.1591)), (0.024548517074831643, tensor(0.1588)), (0.024588058199863524, tensor(0.1584)), (0.024627663015120118, tensor(0.1586)), (0.024667331623189414, tensor(0.1586)), (0.024707064126824656, tensor(0.1586)), (0.024746860628944573, tensor(0.1585)), (0.024786721232633705, tensor(0.1585)), (0.0248266460411426, tensor(0.1586)), (0.024866635157888135, tensor(0.1582)), (0.02490668868645375, tensor(0.1580)), (0.024946806730589747, tensor(0.1574)), (0.02498698939421352, tensor(0.1574)), (0.02502723678140987, tensor(0.1574)), (0.025067548996431235, tensor(0.1572)), (0.025107926143697987, tensor(0.1571)), (0.025148368327798678, tensor(0.1570)), (0.025188875653490327, tensor(0.1574)), (0.025229448225698708, tensor(0.1578)), (0.02527008614951858, tensor(0.1578)), (0.025310789530213988, tensor(0.1587)), (0.025351558473218534, tensor(0.1582)), (0.025392393084135635, tensor(0.1580)), (0.025433293468738825, tensor(0.1589)), (0.025474259732971995, tensor(0.1591)), (0.025515291982949686, tensor(0.1595)), (0.025556390324957365, tensor(0.1602)), (0.0255975548654517, tensor(0.1604)), (0.025638785711060822, tensor(0.1605)), (0.025680082968584626, tensor(0.1606)), (0.025721446744995016, tensor(0.1607)), (0.02576287714743621, tensor(0.1610)), (0.025804374283225004, tensor(0.1608)), (0.025845938259851056, tensor(0.1609)), (0.025887569184977154, tensor(0.1607)), (0.025929267166439517, tensor(0.1612)), (0.025971032312248035, tensor(0.1608)), (0.0260128647305866, tensor(0.1609)), (0.026054764529813337, tensor(0.1613)), (0.02609673181846092, tensor(0.1613)), (0.026138766705236842, tensor(0.1613)), (0.026180869299023683, tensor(0.1615)), (0.026223039708879417, tensor(0.1617)), (0.02626527804403767, tensor(0.1615)), (0.026307584413908024, tensor(0.1613)), (0.026349958928076282, tensor(0.1611)), (0.02639240169630477, tensor(0.1609)), (0.026434912828532595, tensor(0.1610)), (0.02647749243487597, tensor(0.1616)), (0.026520140625628463, tensor(0.1610)), (0.02656285751126129, tensor(0.1617)), (0.026605643202423618, tensor(0.1616)), (0.02664849780994284, tensor(0.1625)), (0.026691421444824847, tensor(0.1625)), (0.02673441421825435, tensor(0.1626)), (0.02677747624159514, tensor(0.1632)), (0.026820607626390384, tensor(0.1643)), (0.02686380848436292, tensor(0.1652)), (0.02690707892741554, tensor(0.1652)), (0.02695041906763128, tensor(0.1657)), (0.026993829017273714, tensor(0.1666)), (0.02703730888878724, tensor(0.1668)), (0.02708085879479738, tensor(0.1668)), (0.027124478848111058, tensor(0.1669)), (0.027168169161716906, tensor(0.1673)), (0.02721192984878554, tensor(0.1675)), (0.027255761022669883, tensor(0.1676)), (0.027299662796905426, tensor(0.1676)), (0.027343635285210527, tensor(0.1680)), (0.027387678601486738, tensor(0.1685)), (0.02743179285981906, tensor(0.1692)), (0.027475978174476248, tensor(0.1692)), (0.02752023465991113, tensor(0.1691)), (0.027564562430760883, tensor(0.1695)), (0.027608961601847323, tensor(0.1697)), (0.027653432288177227, tensor(0.1706)), (0.02769797460494261, tensor(0.1706)), (0.02774258866752103, tensor(0.1709)), (0.027787274591475893, tensor(0.1713)), (0.02783203249255674, tensor(0.1711)), (0.02787686248669956, tensor(0.1721)), (0.027921764690027082, tensor(0.1721)), (0.02796673921884908, tensor(0.1730)), (0.028011786189662657, tensor(0.1735)), (0.028056905719152585, tensor(0.1735)), (0.02810209792419157, tensor(0.1735)), (0.028147362921840568, tensor(0.1739)), (0.0281927008293491, tensor(0.1748)), (0.028238111764155534, tensor(0.1746)), (0.028283595843887403, tensor(0.1747)), (0.028329153186361707, tensor(0.1751)), (0.02837478390958522, tensor(0.1751)), (0.028420488131754792, tensor(0.1749)), (0.028466265971257648, tensor(0.1755)), (0.028512117546671725, tensor(0.1757)), (0.028558042976765938, tensor(0.1755)), (0.02860404238050051, tensor(0.1759)), (0.028650115877027284, tensor(0.1757)), (0.02869626358569002, tensor(0.1752)), (0.028742485626024706, tensor(0.1755)), (0.02878878211775988, tensor(0.1757)), (0.028835153180816926, tensor(0.1757)), (0.02888159893531039, tensor(0.1756)), (0.028928119501548288, tensor(0.1761)), (0.02897471500003242, tensor(0.1763)), (0.029021385551458685, tensor(0.1765)), (0.029068131276717393, tensor(0.1762)), (0.029114952296893568, tensor(0.1762)), (0.029161848733267277, tensor(0.1764)), (0.02920882070731394, tensor(0.1759)), (0.02925586834070463, tensor(0.1762)), (0.029302991755306408, tensor(0.1765)), (0.029350191073182628, tensor(0.1762)), (0.02939746641659326, tensor(0.1762)), (0.029444817907995195, tensor(0.1760)), (0.02949224567004257, tensor(0.1760)), (0.029539749825587096, tensor(0.1761)), (0.02958733049767835, tensor(0.1765)), (0.02963498780956412, tensor(0.1763)), (0.029682721884690705, tensor(0.1756)), (0.029730532846703253, tensor(0.1758)), (0.029778420819446066, tensor(0.1757)), (0.029826385926962927, tensor(0.1754)), (0.02987442829349742, tensor(0.1752)), (0.02992254804349325, tensor(0.1753)), (0.02997074530159457, tensor(0.1753)), (0.0300190201926463, tensor(0.1753)), (0.03006737284169446, tensor(0.1752)), (0.030115803373986474, tensor(0.1751)), (0.030164311914971514, tensor(0.1749)), (0.030212898590300817, tensor(0.1755)), (0.030261563525827997, tensor(0.1754)), (0.03031030684760941, tensor(0.1755)), (0.03035912868190444, tensor(0.1751)), (0.030408029155175834, tensor(0.1750)), (0.030457008394090056, tensor(0.1748)), (0.030506066525517576, tensor(0.1748)), (0.030555203676533235, tensor(0.1749)), (0.030604419974416545, tensor(0.1749)), (0.030653715546652045, tensor(0.1751)), (0.030703090520929597, tensor(0.1751)), (0.030752545025144764, tensor(0.1749)), (0.030802079187399092, tensor(0.1750)), (0.030851693136000478, tensor(0.1750)), (0.030901386999463487, tensor(0.1752)), (0.03095116090650967, tensor(0.1752)), (0.031001014986067946, tensor(0.1755)), (0.03105094936727487, tensor(0.1759)), (0.031100964179475034, tensor(0.1757)), (0.031151059552221336, tensor(0.1763)), (0.031201235615275384, tensor(0.1760)), (0.03125149249860777, tensor(0.1761)), (0.031301830332398445, tensor(0.1760)), (0.03135224924703704, tensor(0.1759)), (0.031402749373123234, tensor(0.1760)), (0.03145333084146702, tensor(0.1760)), (0.031503993783089136, tensor(0.1760)), (0.03155473832922133, tensor(0.1755)), (0.03160556461130675, tensor(0.1758)), (0.03165647276100025, tensor(0.1759)), (0.03170746291016875, tensor(0.1757)), (0.03175853519089157, tensor(0.1757)), (0.031809689735460785, tensor(0.1757)), (0.03186092667638154, tensor(0.1756)), (0.03191224614637242, tensor(0.1759)), (0.031963648278365786, tensor(0.1763)), (0.032015133205508126, tensor(0.1768)), (0.03206670106116036, tensor(0.1764)), (0.03211835197889825, tensor(0.1766)), (0.032170086092512706, tensor(0.1763)), (0.03222190353601012, tensor(0.1764)), (0.03227380444361276, tensor(0.1761)), (0.03232578894975908, tensor(0.1765)), (0.03237785718910406, tensor(0.1764)), (0.0324300092965196, tensor(0.1769)), (0.03248224540709483, tensor(0.1768)), (0.03253456565613648, tensor(0.1770)), (0.03258697017916921, tensor(0.1767)), (0.03263945911193598, tensor(0.1769)), (0.03269203259039841, tensor(0.1767)), (0.032744690750737084, tensor(0.1768)), (0.03279743372935198, tensor(0.1768)), (0.03285026166286275, tensor(0.1768)), (0.03290317468810911, tensor(0.1769)), (0.0329561729421512, tensor(0.1769)), (0.03300925656226991, tensor(0.1763)), (0.03306242568596726, tensor(0.1764)), (0.03311568045096676, tensor(0.1762)), (0.033169020995213724, tensor(0.1763)), (0.033222447456875706, tensor(0.1763)), (0.033275959974342774, tensor(0.1767)), (0.03332955868622792, tensor(0.1768)), (0.0333832437313674, tensor(0.1769)), (0.0334370152488211, tensor(0.1771)), (0.033490873377872904, tensor(0.1778)), (0.033544818258031024, tensor(0.1785)), (0.03359885002902841, tensor(0.1785)), (0.03365296883082306, tensor(0.1787)), (0.03370717480359841, tensor(0.1789)), (0.033761468087763716, tensor(0.1788)), (0.03381584882395437, tensor(0.1789)), (0.033870317153032306, tensor(0.1788)), (0.033924873216086344, tensor(0.1784)), (0.03397951715443256, tensor(0.1786)), (0.034034249109614646, tensor(0.1791)), (0.0340890692234043, tensor(0.1793)), (0.034143977637801574, tensor(0.1785)), (0.034198974495035214, tensor(0.1786)), (0.03425405993756309, tensor(0.1787)), (0.03430923410807254, tensor(0.1789)), (0.034364497149480706, tensor(0.1790)), (0.034419849204934946, tensor(0.1783)), (0.034475290417813186, tensor(0.1786)), (0.03453082093172431, tensor(0.1784)), (0.03458644089050849, tensor(0.1784)), (0.03464215043823761, tensor(0.1780)), (0.0346979497192156, tensor(0.1783)), (0.03475383887797885, tensor(0.1778)), (0.03480981805929653, tensor(0.1780)), (0.03486588740817101, tensor(0.1778)), (0.03492204706983821, tensor(0.1776)), (0.034978297189768, tensor(0.1781)), (0.035034637913664554, tensor(0.1782)), (0.03509106938746675, tensor(0.1781)), (0.03514759175734851, tensor(0.1781)), (0.03520420516971922, tensor(0.1779)), (0.0352609097712241, tensor(0.1777)), (0.035317705708744554, tensor(0.1777)), (0.035374593129398585, tensor(0.1781)), (0.03543157218054118, tensor(0.1777)), (0.03548864300976464, tensor(0.1781)), (0.03554580576489903, tensor(0.1786)), (0.03560306059401252, tensor(0.1787)), (0.03566040764541176, tensor(0.1790)), (0.03571784706764231, tensor(0.1791)), (0.03577537900948899, tensor(0.1795)), (0.03583300361997626, tensor(0.1792)), (0.03589072104836863, tensor(0.1796)), (0.035948531444171025, tensor(0.1793)), (0.036006434957129206, tensor(0.1798)), (0.03606443173723011, tensor(0.1795)), (0.036122521934702265, tensor(0.1793)), (0.0361807057000162, tensor(0.1794)), (0.03623898318388478, tensor(0.1792)), (0.036297354537263654, tensor(0.1792)), (0.03635581991135161, tensor(0.1795)), (0.03641437945759097, tensor(0.1793)), (0.036473033327668006, tensor(0.1796)), (0.03653178167351331, tensor(0.1799)), (0.03659062464730217, tensor(0.1797)), (0.03664956240145503, tensor(0.1799)), (0.036708595088637816, tensor(0.1801)), (0.03676772286176236, tensor(0.1802)), (0.0368269458739868, tensor(0.1802)), (0.03688626427871597, tensor(0.1804)), (0.03694567822960179, tensor(0.1801)), (0.03700518788054368, tensor(0.1800)), (0.03706479338568896, tensor(0.1801)), (0.03712449489943323, tensor(0.1805)), (0.03718429257642077, tensor(0.1809)), (0.037244186571544984, tensor(0.1810)), (0.03730417703994871, tensor(0.1808)), (0.037364264137024754, tensor(0.1809)), (0.03742444801841615, tensor(0.1814)), (0.03748472884001668, tensor(0.1815)), (0.0375451067579712, tensor(0.1814)), (0.03760558192867609, tensor(0.1813)), (0.03766615450877964, tensor(0.1814)), (0.03772682465518245, tensor(0.1810)), (0.037787592525037855, tensor(0.1809)), (0.03784845827575231, tensor(0.1814)), (0.03790942206498584, tensor(0.1812)), (0.03797048405065237, tensor(0.1817)), (0.03803164439092023, tensor(0.1820)), (0.03809290324421249, tensor(0.1820)), (0.038154260769207395, tensor(0.1813)), (0.03821571712483879, tensor(0.1807)), (0.038277272470296525, tensor(0.1812)), (0.03833892696502685, tensor(0.1810)), (0.03840068076873284, tensor(0.1808)), (0.03846253404137483, tensor(0.1805)), (0.03852448694317077, tensor(0.1805)), (0.03858653963459672, tensor(0.1805)), (0.03864869227638719, tensor(0.1805)), (0.038710945029535614, tensor(0.1813)), (0.03877329805529474, tensor(0.1810)), (0.038835751515177035, tensor(0.1819)), (0.03889830557095514, tensor(0.1823)), (0.038960960384662256, tensor(0.1825)), (0.03902371611859259, tensor(0.1823)), (0.03908657293530173, tensor(0.1822)), (0.03914953099760714, tensor(0.1827)), (0.03921259046858851, tensor(0.1832)), (0.039275751511588214, tensor(0.1837)), (0.03933901429021174, tensor(0.1843)), (0.039402378968328086, tensor(0.1844)), (0.0394658457100702, tensor(0.1840)), (0.03952941467983543, tensor(0.1840)), (0.039593086042285876, tensor(0.1841)), (0.0396568599623489, tensor(0.1838)), (0.039720736605217515, tensor(0.1837)), (0.03978471613635081, tensor(0.1838)), (0.039848798721474385, tensor(0.1840)), (0.039912984526580784, tensor(0.1841)), (0.03997727371792991, tensor(0.1847)), (0.04004166646204948, tensor(0.1854)), (0.04010616292573543, tensor(0.1858)), (0.040170763276052374, tensor(0.1857)), (0.04023546768033402, tensor(0.1855)), (0.04030027630618358, tensor(0.1852)), (0.04036518932147427, tensor(0.1854)), (0.04043020689434968, tensor(0.1860)), (0.04049532919322424, tensor(0.1858)), (0.04056055638678365, tensor(0.1861)), (0.04062588864398532, tensor(0.1863)), (0.04069132613405881, tensor(0.1864)), (0.04075686902650626, tensor(0.1862)), (0.04082251749110283, tensor(0.1863)), (0.040888271697897126, tensor(0.1868)), (0.04095413181721169, tensor(0.1865)), (0.04102009801964341, tensor(0.1864)), (0.041086170476063916, tensor(0.1866)), (0.041152349357620105, tensor(0.1869)), (0.04121863483573453, tensor(0.1867)), (0.04128502708210587, tensor(0.1867)), (0.04135152626870936, tensor(0.1862)), (0.04141813256779724, tensor(0.1863)), (0.04148484615189922, tensor(0.1865)), (0.04155166719382287, tensor(0.1872)), (0.04161859586665413, tensor(0.1875)), (0.04168563234375776, tensor(0.1876)), (0.041752776798777716, tensor(0.1881)), (0.04182002940563768, tensor(0.1884)), (0.04188739033854147, tensor(0.1887)), (0.04195485977197351, tensor(0.1883)), (0.04202243788069925, tensor(0.1886)), (0.042090124839765666, tensor(0.1888)), (0.04215792082450167, tensor(0.1894)), (0.042225826010518594, tensor(0.1898)), (0.042293840573710606, tensor(0.1897)), (0.04236196469025524, tensor(0.1896)), (0.042430198536613756, tensor(0.1898)), (0.042498542289531684, tensor(0.1898)), (0.04256699612603924, tensor(0.1896)), (0.04263556022345174, tensor(0.1897)), (0.04270423475937018, tensor(0.1892)), (0.04277301991168158, tensor(0.1891)), (0.04284191585855949, tensor(0.1893)), (0.042910922778464464, tensor(0.1887)), (0.042980040850144494, tensor(0.1887)), (0.0430492702526355, tensor(0.1887)), (0.04311861116526179, tensor(0.1888)), (0.043188063767636495, tensor(0.1887)), (0.04325762823966205, tensor(0.1888)), (0.0433273047615307, tensor(0.1886)), (0.04339709351372489, tensor(0.1888)), (0.04346699467701779, tensor(0.1888)), (0.04353700843247377, tensor(0.1888)), (0.04360713496144882, tensor(0.1886)), (0.04367737444559105, tensor(0.1890)), (0.04374772706684116, tensor(0.1890)), (0.04381819300743291, tensor(0.1889)), (0.043888772449893594, tensor(0.1883)), (0.04395946557704449, tensor(0.1892)), (0.044030272572001386, tensor(0.1896)), (0.04410119361817498, tensor(0.1892)), (0.04417222889927142, tensor(0.1892)), (0.04424337859929277, tensor(0.1894)), (0.04431464290253743, tensor(0.1893)), (0.04438602199360069, tensor(0.1892)), (0.04445751605737518, tensor(0.1888)), (0.0445291252790513, tensor(0.1888)), (0.04460084984411778, tensor(0.1889)), (0.04467268993836211, tensor(0.1898)), (0.04474464574787104, tensor(0.1899)), (0.04481671745903106, tensor(0.1898)), (0.04488890525852885, tensor(0.1901)), (0.04496120933335184, tensor(0.1900)), (0.0450336298707886, tensor(0.1897)), (0.045106167058429396, tensor(0.1897)), (0.045178821084166654, tensor(0.1897)), (0.04525159213619544, tensor(0.1897)), (0.04532448040301395, tensor(0.1898)), (0.045397486073424, tensor(0.1901)), (0.04547060933653152, tensor(0.1901)), (0.04554385038174704, tensor(0.1903)), (0.04561720939878618, tensor(0.1901)), (0.04569068657767013, tensor(0.1904)), (0.04576428210872616, tensor(0.1904)), (0.04583799618258811, tensor(0.1905)), (0.04591182899019689, tensor(0.1908)), (0.04598578072280093, tensor(0.1907)), (0.04605985157195676, tensor(0.1905)), (0.04613404172952941, tensor(0.1905)), (0.04620835138769299, tensor(0.1902)), (0.04628278073893113, tensor(0.1905)), (0.04635732997603749, tensor(0.1903)), (0.046431999292116305, tensor(0.1897)), (0.04650678888058282, tensor(0.1893)), (0.04658169893516383, tensor(0.1892)), (0.046656729649898185, tensor(0.1894)), (0.04673188121913724, tensor(0.1890)), (0.04680715383754545, tensor(0.1888)), (0.04688254770010078, tensor(0.1888)), (0.04695806300209527, tensor(0.1887)), (0.04703369993913554, tensor(0.1889)), (0.04710945870714325, tensor(0.1886)), (0.04718533950235565, tensor(0.1884)), (0.047261342521326084, tensor(0.1882)), (0.04733746796092449, tensor(0.1883)), (0.04741371601833789, tensor(0.1880)), (0.04749008689107096, tensor(0.1881)), (0.047566580776946456, tensor(0.1885)), (0.04764319787410581, tensor(0.1880)), (0.047719938381009595, tensor(0.1885)), (0.04779680249643805, tensor(0.1886)), (0.0478737904194916, tensor(0.1881)), (0.047950902349591366, tensor(0.1883)), (0.04802813848647969, tensor(0.1885)), (0.048105499030220614, tensor(0.1885)), (0.048182984181200476, tensor(0.1881)), (0.048260594140128364, tensor(0.1882)), (0.04833832910803664, tensor(0.1885)), (0.04841618928628149, tensor(0.1887)), (0.048494174876543436, tensor(0.1881)), (0.048572286080827844, tensor(0.1879)), (0.04865052310146546, tensor(0.1884)), (0.04872888614111293, tensor(0.1881)), (0.04880737540275333, tensor(0.1884)), (0.04888599108969667, tensor(0.1887)), (0.04896473340558047, tensor(0.1887)), (0.049043602554370236, tensor(0.1887)), (0.04912259874036, tensor(0.1892)), (0.049201722168172875, tensor(0.1890)), (0.049280973042761564, tensor(0.1891)), (0.04936035156940889, tensor(0.1892)), (0.04943985795372832, tensor(0.1890)), (0.04951949240166454, tensor(0.1898)), (0.049599255119493924, tensor(0.1905)), (0.04967914631382512, tensor(0.1902)), (0.04975916619159958, tensor(0.1904)), (0.049839314960092064, tensor(0.1905)), (0.04991959282691119, tensor(0.1909))]
******

*** 2018-12-28 07:10:14,328 - code.resnet_fastai - DEBUG ***
0.018285790999795742
******

*** 2018-12-28 07:10:14,364 - matplotlib.axes._base - DEBUG ***
update_title_pos
******

*** 2018-12-28 07:10:14,366 - matplotlib.ticker - DEBUG ***
vmin 0.009388590043431337 vmax 0.05360008871710735
******

*** 2018-12-28 07:10:14,366 - matplotlib.ticker - DEBUG ***
ticklocs array([1.e-04, 1.e-03, 1.e-02, 1.e-01, 1.e+00])
******

*** 2018-12-28 07:10:14,373 - matplotlib.ticker - DEBUG ***
vmin 0.009388590043431337 vmax 0.05360008871710735
******

*** 2018-12-28 07:10:14,373 - matplotlib.ticker - DEBUG ***
ticklocs [0.0002, 0.00030000000000000003, 0.0004, 0.0005, 0.0006000000000000001, 0.0007, 0.0008, 0.0009000000000000001, 0.002, 0.003, 0.004, 0.005, 0.006, 0.007, 0.008, 0.009000000000000001, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.2, 0.30000000000000004, 0.4, 0.5, 0.6000000000000001, 0.7000000000000001, 0.8, 0.9, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0]
******

*** 2018-12-28 07:10:14,452 - matplotlib.font_manager - DEBUG ***
findfont: Matching :family=sans-serif:style=normal:variant=normal:weight=normal:stretch=normal:size=10.0 to DejaVu Sans ('/home/ubuntu/.local/lib/python3.6/site-packages/matplotlib/mpl-data/fonts/ttf/DejaVuSans.ttf') with score of 0.050000.
******

*** 2018-12-28 07:10:14,471 - matplotlib.font_manager - DEBUG ***
findfont: Matching :family=STIXGeneral:style=normal:variant=normal:weight=normal:stretch=normal:size=10.0 to STIXGeneral ('/home/ubuntu/.local/lib/python3.6/site-packages/matplotlib/mpl-data/fonts/ttf/STIXGeneral.ttf') with score of 0.050000.
******

*** 2018-12-28 07:10:14,477 - matplotlib.font_manager - DEBUG ***
findfont: Matching :family=STIXGeneral:style=italic:variant=normal:weight=normal:stretch=normal:size=10.0 to STIXGeneral ('/home/ubuntu/.local/lib/python3.6/site-packages/matplotlib/mpl-data/fonts/ttf/STIXGeneralItalic.ttf') with score of 0.050000.
******

*** 2018-12-28 07:10:14,491 - matplotlib.font_manager - DEBUG ***
findfont: Matching :family=STIXGeneral:style=normal:variant=normal:weight=bold:stretch=normal:size=10.0 to STIXGeneral ('/home/ubuntu/.local/lib/python3.6/site-packages/matplotlib/mpl-data/fonts/ttf/STIXGeneralBol.ttf') with score of 0.000000.
******

*** 2018-12-28 07:10:14,496 - matplotlib.font_manager - DEBUG ***
findfont: Matching :family=STIXNonUnicode:style=normal:variant=normal:weight=normal:stretch=normal:size=10.0 to STIXNonUnicode ('/home/ubuntu/.local/lib/python3.6/site-packages/matplotlib/mpl-data/fonts/ttf/STIXNonUni.ttf') with score of 0.050000.
******

*** 2018-12-28 07:10:14,509 - matplotlib.font_manager - DEBUG ***
findfont: Matching :family=STIXNonUnicode:style=italic:variant=normal:weight=normal:stretch=normal:size=10.0 to STIXNonUnicode ('/home/ubuntu/.local/lib/python3.6/site-packages/matplotlib/mpl-data/fonts/ttf/STIXNonUniIta.ttf') with score of 0.050000.
******

*** 2018-12-28 07:10:14,510 - matplotlib.font_manager - DEBUG ***
findfont: Matching :family=STIXNonUnicode:style=normal:variant=normal:weight=bold:stretch=normal:size=10.0 to STIXNonUnicode ('/home/ubuntu/.local/lib/python3.6/site-packages/matplotlib/mpl-data/fonts/ttf/STIXNonUniBol.ttf') with score of 0.000000.
******

*** 2018-12-28 07:10:14,515 - matplotlib.font_manager - DEBUG ***
findfont: Matching :family=STIXSizeOneSym:style=normal:variant=normal:weight=normal:stretch=normal:size=10.0 to STIXSizeOneSym ('/home/ubuntu/.local/lib/python3.6/site-packages/matplotlib/mpl-data/fonts/ttf/STIXSizOneSymReg.ttf') with score of 0.050000.
******

*** 2018-12-28 07:10:14,532 - matplotlib.font_manager - DEBUG ***
findfont: Matching :family=STIXSizeTwoSym:style=normal:variant=normal:weight=normal:stretch=normal:size=10.0 to STIXSizeTwoSym ('/home/ubuntu/.local/lib/python3.6/site-packages/matplotlib/mpl-data/fonts/ttf/STIXSizTwoSymReg.ttf') with score of 0.050000.
******

*** 2018-12-28 07:10:14,537 - matplotlib.font_manager - DEBUG ***
findfont: Matching :family=STIXSizeThreeSym:style=normal:variant=normal:weight=normal:stretch=normal:size=10.0 to STIXSizeThreeSym ('/home/ubuntu/.local/lib/python3.6/site-packages/matplotlib/mpl-data/fonts/ttf/STIXSizThreeSymReg.ttf') with score of 0.050000.
******

*** 2018-12-28 07:10:14,550 - matplotlib.font_manager - DEBUG ***
findfont: Matching :family=STIXSizeFourSym:style=normal:variant=normal:weight=normal:stretch=normal:size=10.0 to STIXSizeFourSym ('/home/ubuntu/.local/lib/python3.6/site-packages/matplotlib/mpl-data/fonts/ttf/STIXSizFourSymReg.ttf') with score of 0.050000.
******

*** 2018-12-28 07:10:14,555 - matplotlib.font_manager - DEBUG ***
findfont: Matching :family=STIXSizeFiveSym:style=normal:variant=normal:weight=normal:stretch=normal:size=10.0 to STIXSizeFiveSym ('/home/ubuntu/.local/lib/python3.6/site-packages/matplotlib/mpl-data/fonts/ttf/STIXSizFiveSymReg.ttf') with score of 0.050000.
******

*** 2018-12-28 07:10:14,560 - matplotlib.font_manager - DEBUG ***
findfont: Matching :family=cmsy10:style=normal:variant=normal:weight=normal:stretch=normal:size=10.0 to cmsy10 ('/home/ubuntu/.local/lib/python3.6/site-packages/matplotlib/mpl-data/fonts/ttf/cmsy10.ttf') with score of 0.050000.
******

*** 2018-12-28 07:10:14,565 - matplotlib.font_manager - DEBUG ***
findfont: Matching :family=cmr10:style=normal:variant=normal:weight=normal:stretch=normal:size=10.0 to cmr10 ('/home/ubuntu/.local/lib/python3.6/site-packages/matplotlib/mpl-data/fonts/ttf/cmr10.ttf') with score of 0.050000.
******

*** 2018-12-28 07:10:14,570 - matplotlib.font_manager - DEBUG ***
findfont: Matching :family=cmtt10:style=normal:variant=normal:weight=normal:stretch=normal:size=10.0 to cmtt10 ('/home/ubuntu/.local/lib/python3.6/site-packages/matplotlib/mpl-data/fonts/ttf/cmtt10.ttf') with score of 0.050000.
******

*** 2018-12-28 07:10:14,575 - matplotlib.font_manager - DEBUG ***
findfont: Matching :family=cmmi10:style=normal:variant=normal:weight=normal:stretch=normal:size=10.0 to cmmi10 ('/home/ubuntu/.local/lib/python3.6/site-packages/matplotlib/mpl-data/fonts/ttf/cmmi10.ttf') with score of 0.050000.
******

*** 2018-12-28 07:10:14,580 - matplotlib.font_manager - DEBUG ***
findfont: Matching :family=cmb10:style=normal:variant=normal:weight=normal:stretch=normal:size=10.0 to cmb10 ('/home/ubuntu/.local/lib/python3.6/site-packages/matplotlib/mpl-data/fonts/ttf/cmb10.ttf') with score of 0.050000.
******

*** 2018-12-28 07:10:14,585 - matplotlib.font_manager - DEBUG ***
findfont: Matching :family=cmss10:style=normal:variant=normal:weight=normal:stretch=normal:size=10.0 to cmss10 ('/home/ubuntu/.local/lib/python3.6/site-packages/matplotlib/mpl-data/fonts/ttf/cmss10.ttf') with score of 0.050000.
******

*** 2018-12-28 07:10:14,591 - matplotlib.font_manager - DEBUG ***
findfont: Matching :family=cmex10:style=normal:variant=normal:weight=normal:stretch=normal:size=10.0 to cmex10 ('/home/ubuntu/.local/lib/python3.6/site-packages/matplotlib/mpl-data/fonts/ttf/cmex10.ttf') with score of 0.050000.
******

*** 2018-12-28 07:10:14,597 - matplotlib.font_manager - DEBUG ***
findfont: Matching :family=DejaVu Sans:style=normal:variant=normal:weight=normal:stretch=normal:size=10.0 to DejaVu Sans ('/home/ubuntu/.local/lib/python3.6/site-packages/matplotlib/mpl-data/fonts/ttf/DejaVuSans.ttf') with score of 0.050000.
******

*** 2018-12-28 07:10:14,603 - matplotlib.font_manager - DEBUG ***
findfont: Matching :family=DejaVu Sans:style=italic:variant=normal:weight=normal:stretch=normal:size=10.0 to DejaVu Sans ('/home/ubuntu/.local/lib/python3.6/site-packages/matplotlib/mpl-data/fonts/ttf/DejaVuSans-Oblique.ttf') with score of 0.150000.
******

*** 2018-12-28 07:10:14,604 - matplotlib.font_manager - DEBUG ***
findfont: Matching :family=DejaVu Sans:style=normal:variant=normal:weight=bold:stretch=normal:size=10.0 to DejaVu Sans ('/home/ubuntu/.local/lib/python3.6/site-packages/matplotlib/mpl-data/fonts/ttf/DejaVuSans-Bold.ttf') with score of 0.000000.
******

*** 2018-12-28 07:10:14,612 - matplotlib.font_manager - DEBUG ***
findfont: Matching :family=DejaVu Sans Mono:style=normal:variant=normal:weight=normal:stretch=normal:size=10.0 to DejaVu Sans Mono ('/home/ubuntu/.local/lib/python3.6/site-packages/matplotlib/mpl-data/fonts/ttf/DejaVuSansMono.ttf') with score of 0.050000.
******

*** 2018-12-28 07:10:14,619 - matplotlib.font_manager - DEBUG ***
findfont: Matching :family=DejaVu Sans Display:style=normal:variant=normal:weight=normal:stretch=normal:size=10.0 to DejaVu Sans Display ('/home/ubuntu/.local/lib/python3.6/site-packages/matplotlib/mpl-data/fonts/ttf/DejaVuSansDisplay.ttf') with score of 0.050000.
******

*** 2018-12-28 07:10:14,888 - matplotlib.ticker - DEBUG ***
vmin 0.009388590043431337 vmax 0.05360008871710735
******

*** 2018-12-28 07:10:14,888 - matplotlib.ticker - DEBUG ***
ticklocs array([1.e-04, 1.e-03, 1.e-02, 1.e-01, 1.e+00])
******

*** 2018-12-28 07:10:14,889 - matplotlib.ticker - DEBUG ***
vmin 0.009388590043431337 vmax 0.05360008871710735
******

*** 2018-12-28 07:10:14,890 - matplotlib.ticker - DEBUG ***
ticklocs [0.0002, 0.00030000000000000003, 0.0004, 0.0005, 0.0006000000000000001, 0.0007, 0.0008, 0.0009000000000000001, 0.002, 0.003, 0.004, 0.005, 0.006, 0.007, 0.008, 0.009000000000000001, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.2, 0.30000000000000004, 0.4, 0.5, 0.6000000000000001, 0.7000000000000001, 0.8, 0.9, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0]
******

*** 2018-12-28 07:10:15,017 - matplotlib.axes._base - DEBUG ***
update_title_pos
******

*** 2018-12-28 07:10:15,019 - matplotlib.ticker - DEBUG ***
vmin 0.009388590043431337 vmax 0.05360008871710735
******

*** 2018-12-28 07:10:15,020 - matplotlib.ticker - DEBUG ***
ticklocs array([1.e-04, 1.e-03, 1.e-02, 1.e-01, 1.e+00])
******

*** 2018-12-28 07:10:15,021 - matplotlib.ticker - DEBUG ***
vmin 0.009388590043431337 vmax 0.05360008871710735
******

*** 2018-12-28 07:10:15,021 - matplotlib.ticker - DEBUG ***
ticklocs [0.0002, 0.00030000000000000003, 0.0004, 0.0005, 0.0006000000000000001, 0.0007, 0.0008, 0.0009000000000000001, 0.002, 0.003, 0.004, 0.005, 0.006, 0.007, 0.008, 0.009000000000000001, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.2, 0.30000000000000004, 0.4, 0.5, 0.6000000000000001, 0.7000000000000001, 0.8, 0.9, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0]
******

*** 2018-12-28 07:10:15,039 - matplotlib.ticker - DEBUG ***
vmin 0.009388590043431337 vmax 0.05360008871710735
******

*** 2018-12-28 07:10:15,039 - matplotlib.ticker - DEBUG ***
ticklocs array([1.e-04, 1.e-03, 1.e-02, 1.e-01, 1.e+00])
******

*** 2018-12-28 07:10:15,041 - matplotlib.ticker - DEBUG ***
vmin 0.009388590043431337 vmax 0.05360008871710735
******

*** 2018-12-28 07:10:15,041 - matplotlib.ticker - DEBUG ***
ticklocs [0.0002, 0.00030000000000000003, 0.0004, 0.0005, 0.0006000000000000001, 0.0007, 0.0008, 0.0009000000000000001, 0.002, 0.003, 0.004, 0.005, 0.006, 0.007, 0.008, 0.009000000000000001, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.2, 0.30000000000000004, 0.4, 0.5, 0.6000000000000001, 0.7000000000000001, 0.8, 0.9, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0]
******

*** 2018-12-28 07:10:15,097 - code.resnet_fastai - INFO ***
Start model fitting: Stage 1
******

*** 2018-12-28 07:10:15,097 - code.resnet_fastai - DEBUG ***
Use best LR: 0.018285790999795742
******

2         0.190921                
LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.
epoch     train_loss  valid_loss  fbeta   
1         0.141760    0.159406    0.382719  
2         0.131830    0.129934    0.527954  
3         0.130162    0.126398    0.551696  
4         0.132332    2.088376    0.536606  
5         0.133149    0.288185    0.499992  
Epoch 5: reducing lr to 0.0036367345639252494
6         0.129511    0.435402    0.427149  
